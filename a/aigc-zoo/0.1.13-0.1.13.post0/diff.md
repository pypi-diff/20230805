# Comparing `tmp/aigc_zoo-0.1.13-py3-none-any.whl.zip` & `tmp/aigc_zoo-0.1.13.post0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,50 +1,54 @@
-Zip file size: 73580 bytes, number of entries: 48
--rw-rw-rw-  2.0 fat        7 b- defN 23-Jun-26 02:52 aigc_zoo/.gitignore
--rw-rw-rw-  2.0 fat       80 b- defN 23-Jun-26 02:52 aigc_zoo/__init__.py
--rw-rw-rw-  2.0 fat      105 b- defN 23-Jul-31 06:56 aigc_zoo/requirements.txt
--rw-rw-rw-  2.0 fat       77 b- defN 23-Jun-26 02:52 aigc_zoo/model_zoo/__init__.py
--rw-rw-rw-  2.0 fat       66 b- defN 23-Jun-26 02:52 aigc_zoo/model_zoo/baichuan/__init__.py
--rw-rw-rw-  2.0 fat     5578 b- defN 23-Jul-10 04:30 aigc_zoo/model_zoo/baichuan/llm_model.py
--rw-rw-rw-  2.0 fat     9574 b- defN 23-Jun-26 02:52 aigc_zoo/model_zoo/baichuan/tokenization_baichuan.py
--rw-rw-rw-  2.0 fat       66 b- defN 23-Jun-26 02:52 aigc_zoo/model_zoo/baichuan2/__init__.py
--rw-rw-rw-  2.0 fat     5575 b- defN 23-Jul-11 04:15 aigc_zoo/model_zoo/baichuan2/llm_model.py
--rw-rw-rw-  2.0 fat     8720 b- defN 23-Jul-11 03:13 aigc_zoo/model_zoo/baichuan2/tokenization_baichuan.py
--rw-rw-rw-  2.0 fat       77 b- defN 23-Jun-26 02:52 aigc_zoo/model_zoo/chatglm/__init__.py
--rw-rw-rw-  2.0 fat    17057 b- defN 23-Jul-13 09:23 aigc_zoo/model_zoo/chatglm/llm_model.py
--rw-rw-rw-  2.0 fat     6435 b- defN 23-Jul-10 04:30 aigc_zoo/model_zoo/chatglm/ppo_model.py
--rw-rw-rw-  2.0 fat     9700 b- defN 23-Jul-10 04:30 aigc_zoo/model_zoo/chatglm/reward_model.py
--rw-rw-rw-  2.0 fat    17037 b- defN 23-Jun-26 02:52 aigc_zoo/model_zoo/chatglm/tokenization_chatglm.py
--rw-rw-rw-  2.0 fat       77 b- defN 23-Jun-26 02:52 aigc_zoo/model_zoo/chatglm2/__init__.py
--rw-rw-rw-  2.0 fat      114 b- defN 23-Jul-14 00:27 aigc_zoo/model_zoo/chatglm2/chatglm_model.py
--rw-rw-rw-  2.0 fat     9402 b- defN 23-Jul-17 03:15 aigc_zoo/model_zoo/chatglm2/llm_model.py
--rw-rw-rw-  2.0 fat    10002 b- defN 23-Jul-17 03:15 aigc_zoo/model_zoo/chatglm2/tokenization_chatglm.py
--rw-rw-rw-  2.0 fat       76 b- defN 23-Jul-18 01:57 aigc_zoo/model_zoo/internlm/__init__.py
--rw-rw-rw-  2.0 fat     5571 b- defN 23-Jul-18 02:42 aigc_zoo/model_zoo/internlm/llm_model.py
--rw-rw-rw-  2.0 fat     8954 b- defN 23-Jul-18 01:27 aigc_zoo/model_zoo/internlm/tokenization_internlm.py
--rw-rw-rw-  2.0 fat       77 b- defN 23-Jun-26 02:52 aigc_zoo/model_zoo/llm/__init__.py
--rw-rw-rw-  2.0 fat     6344 b- defN 23-Jul-10 04:30 aigc_zoo/model_zoo/llm/ilql_model.py
--rw-rw-rw-  2.0 fat     5443 b- defN 23-Jul-10 04:30 aigc_zoo/model_zoo/llm/llm_model.py
--rw-rw-rw-  2.0 fat     6298 b- defN 23-Jul-10 04:30 aigc_zoo/model_zoo/llm/ppo_model.py
--rw-rw-rw-  2.0 fat     9732 b- defN 23-Jul-10 04:30 aigc_zoo/model_zoo/llm/reward_model.py
--rw-rw-rw-  2.0 fat     6704 b- defN 23-Jul-10 04:30 aigc_zoo/model_zoo/llm/rrhf_model.py
--rw-rw-rw-  2.0 fat       77 b- defN 23-Jun-26 02:52 aigc_zoo/model_zoo/moss/__init__.py
--rw-rw-rw-  2.0 fat     4554 b- defN 23-Jul-10 04:30 aigc_zoo/model_zoo/moss/llm_model.py
--rw-rw-rw-  2.0 fat     2050 b- defN 23-Jun-26 02:52 aigc_zoo/model_zoo/moss/moss_model.py
--rw-rw-rw-  2.0 fat       77 b- defN 23-Jun-26 02:52 aigc_zoo/model_zoo/rwkv4/__init__.py
--rw-rw-rw-  2.0 fat     5734 b- defN 23-Jul-17 03:15 aigc_zoo/model_zoo/rwkv4/llm_model.py
--rw-rw-rw-  2.0 fat     6519 b- defN 23-Jul-17 03:15 aigc_zoo/model_zoo/rwkv4/rwkv4_tokenizer.py
--rw-rw-rw-  2.0 fat       77 b- defN 23-Jun-26 02:52 aigc_zoo/model_zoo/t5/__init__.py
--rw-rw-rw-  2.0 fat     4950 b- defN 23-Jul-10 04:30 aigc_zoo/model_zoo/t5/llm_model.py
--rw-rw-rw-  2.0 fat     5848 b- defN 23-Jul-10 04:30 aigc_zoo/model_zoo/t5/ppo_model.py
--rw-rw-rw-  2.0 fat     9098 b- defN 23-Jul-10 04:30 aigc_zoo/model_zoo/t5/reward_model.py
--rw-rw-rw-  2.0 fat       77 b- defN 23-Jun-26 02:52 aigc_zoo/utils/__init__.py
--rw-rw-rw-  2.0 fat     2664 b- defN 23-Jul-12 07:02 aigc_zoo/utils/llm_generate.py
--rw-rw-rw-  2.0 fat    13198 b- defN 23-Jul-14 00:27 aigc_zoo/utils/moss_generate.py
--rw-rw-rw-  2.0 fat     3429 b- defN 23-Jul-17 03:15 aigc_zoo/utils/rwkv4_generate.py
--rw-rw-rw-  2.0 fat      748 b- defN 23-Jul-26 05:46 aigc_zoo/utils/streamgenerator.py
--rw-rw-rw-  2.0 fat    11357 b- defN 23-Jul-31 07:00 aigc_zoo-0.1.13.dist-info/LICENSE
--rw-rw-rw-  2.0 fat      351 b- defN 23-Jul-31 07:00 aigc_zoo-0.1.13.dist-info/METADATA
--rw-rw-rw-  2.0 fat       92 b- defN 23-Jul-31 07:00 aigc_zoo-0.1.13.dist-info/WHEEL
--rw-rw-rw-  2.0 fat        9 b- defN 23-Jul-31 07:00 aigc_zoo-0.1.13.dist-info/top_level.txt
-?rw-rw-r--  2.0 fat     4420 b- defN 23-Jul-31 07:00 aigc_zoo-0.1.13.dist-info/RECORD
-48 files, 224277 bytes uncompressed, 66372 bytes compressed:  70.4%
+Zip file size: 84902 bytes, number of entries: 52
+-rw-rw-rw-  2.0 fat       80 b- defN 23-Aug-01 16:34 aigc_zoo/__init__.py
+-rw-rw-rw-  2.0 fat       77 b- defN 23-Aug-01 16:34 aigc_zoo/model_zoo/__init__.py
+-rw-rw-rw-  2.0 fat       66 b- defN 23-Aug-01 16:34 aigc_zoo/model_zoo/baichuan/__init__.py
+-rw-rw-rw-  2.0 fat     5583 b- defN 23-Aug-04 13:55 aigc_zoo/model_zoo/baichuan/llm_model.py
+-rw-rw-rw-  2.0 fat     9574 b- defN 23-Aug-01 16:34 aigc_zoo/model_zoo/baichuan/tokenization_baichuan.py
+-rw-rw-rw-  2.0 fat       66 b- defN 23-Aug-01 16:34 aigc_zoo/model_zoo/baichuan2/__init__.py
+-rw-rw-rw-  2.0 fat     5580 b- defN 23-Aug-04 13:55 aigc_zoo/model_zoo/baichuan2/llm_model.py
+-rw-rw-rw-  2.0 fat     8720 b- defN 23-Aug-01 16:34 aigc_zoo/model_zoo/baichuan2/tokenization_baichuan.py
+-rw-rw-rw-  2.0 fat       77 b- defN 23-Aug-01 16:34 aigc_zoo/model_zoo/chatglm/__init__.py
+-rw-rw-rw-  2.0 fat    17033 b- defN 23-Aug-01 16:34 aigc_zoo/model_zoo/chatglm/llm_model.py
+-rw-rw-rw-  2.0 fat     6419 b- defN 23-Aug-01 16:34 aigc_zoo/model_zoo/chatglm/ppo_model.py
+-rw-rw-rw-  2.0 fat     9684 b- defN 23-Aug-01 16:34 aigc_zoo/model_zoo/chatglm/reward_model.py
+-rw-rw-rw-  2.0 fat    17037 b- defN 23-Aug-01 16:34 aigc_zoo/model_zoo/chatglm/tokenization_chatglm.py
+-rw-rw-rw-  2.0 fat       77 b- defN 23-Aug-01 16:34 aigc_zoo/model_zoo/chatglm2/__init__.py
+-rw-rw-rw-  2.0 fat      114 b- defN 23-Aug-01 16:34 aigc_zoo/model_zoo/chatglm2/chatglm_model.py
+-rw-rw-rw-  2.0 fat     9378 b- defN 23-Aug-01 16:34 aigc_zoo/model_zoo/chatglm2/llm_model.py
+-rw-rw-rw-  2.0 fat    10002 b- defN 23-Aug-01 16:34 aigc_zoo/model_zoo/chatglm2/tokenization_chatglm.py
+-rw-rw-rw-  2.0 fat       76 b- defN 23-Aug-01 16:34 aigc_zoo/model_zoo/internlm/__init__.py
+-rw-rw-rw-  2.0 fat     5576 b- defN 23-Aug-04 13:55 aigc_zoo/model_zoo/internlm/llm_model.py
+-rw-rw-rw-  2.0 fat     8954 b- defN 23-Aug-01 16:34 aigc_zoo/model_zoo/internlm/tokenization_internlm.py
+-rw-rw-rw-  2.0 fat       77 b- defN 23-Aug-01 16:34 aigc_zoo/model_zoo/llm/__init__.py
+-rw-rw-rw-  2.0 fat     6328 b- defN 23-Aug-01 16:34 aigc_zoo/model_zoo/llm/ilql_model.py
+-rw-rw-rw-  2.0 fat     5427 b- defN 23-Aug-01 16:34 aigc_zoo/model_zoo/llm/llm_model.py
+-rw-rw-rw-  2.0 fat     6282 b- defN 23-Aug-01 16:34 aigc_zoo/model_zoo/llm/ppo_model.py
+-rw-rw-rw-  2.0 fat     9716 b- defN 23-Aug-01 16:34 aigc_zoo/model_zoo/llm/reward_model.py
+-rw-rw-rw-  2.0 fat     6688 b- defN 23-Aug-01 16:34 aigc_zoo/model_zoo/llm/rrhf_model.py
+-rw-rw-rw-  2.0 fat       77 b- defN 23-Aug-01 16:34 aigc_zoo/model_zoo/moss/__init__.py
+-rw-rw-rw-  2.0 fat     4538 b- defN 23-Aug-01 16:34 aigc_zoo/model_zoo/moss/llm_model.py
+-rw-rw-rw-  2.0 fat     2050 b- defN 23-Aug-01 16:34 aigc_zoo/model_zoo/moss/moss_model.py
+-rw-rw-rw-  2.0 fat       76 b- defN 23-Aug-03 10:55 aigc_zoo/model_zoo/qwen/__init__.py
+-rw-rw-rw-  2.0 fat     7256 b- defN 23-Aug-03 13:26 aigc_zoo/model_zoo/qwen/llm_model.py
+-rw-rw-rw-  2.0 fat    14379 b- defN 23-Aug-04 10:49 aigc_zoo/model_zoo/qwen/qwen_generation_utils.py
+-rw-rw-rw-  2.0 fat     9422 b- defN 23-Aug-04 10:49 aigc_zoo/model_zoo/qwen/tokenization_qwen.py
+-rw-rw-rw-  2.0 fat       77 b- defN 23-Aug-01 16:34 aigc_zoo/model_zoo/rwkv4/__init__.py
+-rw-rw-rw-  2.0 fat     5718 b- defN 23-Aug-01 16:34 aigc_zoo/model_zoo/rwkv4/llm_model.py
+-rw-rw-rw-  2.0 fat     6519 b- defN 23-Aug-01 16:34 aigc_zoo/model_zoo/rwkv4/rwkv4_tokenizer.py
+-rw-rw-rw-  2.0 fat       77 b- defN 23-Aug-01 16:34 aigc_zoo/model_zoo/t5/__init__.py
+-rw-rw-rw-  2.0 fat     4934 b- defN 23-Aug-01 16:34 aigc_zoo/model_zoo/t5/llm_model.py
+-rw-rw-rw-  2.0 fat     5832 b- defN 23-Aug-01 16:34 aigc_zoo/model_zoo/t5/ppo_model.py
+-rw-rw-rw-  2.0 fat     9082 b- defN 23-Aug-01 16:34 aigc_zoo/model_zoo/t5/reward_model.py
+-rw-rw-rw-  2.0 fat       77 b- defN 23-Aug-01 16:34 aigc_zoo/utils/__init__.py
+-rw-rw-rw-  2.0 fat     2664 b- defN 23-Aug-01 16:34 aigc_zoo/utils/llm_generate.py
+-rw-rw-rw-  2.0 fat    13198 b- defN 23-Aug-01 16:34 aigc_zoo/utils/moss_generate.py
+-rw-rw-rw-  2.0 fat     3429 b- defN 23-Aug-01 16:34 aigc_zoo/utils/rwkv4_generate.py
+-rw-rw-rw-  2.0 fat      748 b- defN 23-Aug-01 16:34 aigc_zoo/utils/streamgenerator.py
+-rw-rw-rw-  2.0 fat       79 b- defN 23-Aug-01 16:14 aigc_zoo/weight/__init__.py
+-rw-rw-rw-  2.0 fat     5250 b- defN 23-Aug-02 10:46 aigc_zoo/weight/modelweighter.py
+-rw-rw-rw-  2.0 fat    11357 b- defN 23-Aug-04 20:58 aigc_zoo-0.1.13.post0.dist-info/LICENSE
+-rw-rw-rw-  2.0 fat      369 b- defN 23-Aug-04 20:58 aigc_zoo-0.1.13.post0.dist-info/METADATA
+-rw-rw-rw-  2.0 fat       92 b- defN 23-Aug-04 20:58 aigc_zoo-0.1.13.post0.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat        9 b- defN 23-Aug-04 20:58 aigc_zoo-0.1.13.post0.dist-info/top_level.txt
+-rw-rw-r--  2.0 fat     4857 b- defN 23-Aug-04 20:58 aigc_zoo-0.1.13.post0.dist-info/RECORD
+52 files, 260857 bytes uncompressed, 76974 bytes compressed:  70.5%
```

## zipnote {}

```diff
@@ -1,16 +1,10 @@
-Filename: aigc_zoo/.gitignore
-Comment: 
-
 Filename: aigc_zoo/__init__.py
 Comment: 
 
-Filename: aigc_zoo/requirements.txt
-Comment: 
-
 Filename: aigc_zoo/model_zoo/__init__.py
 Comment: 
 
 Filename: aigc_zoo/model_zoo/baichuan/__init__.py
 Comment: 
 
 Filename: aigc_zoo/model_zoo/baichuan/llm_model.py
@@ -87,14 +81,26 @@
 
 Filename: aigc_zoo/model_zoo/moss/llm_model.py
 Comment: 
 
 Filename: aigc_zoo/model_zoo/moss/moss_model.py
 Comment: 
 
+Filename: aigc_zoo/model_zoo/qwen/__init__.py
+Comment: 
+
+Filename: aigc_zoo/model_zoo/qwen/llm_model.py
+Comment: 
+
+Filename: aigc_zoo/model_zoo/qwen/qwen_generation_utils.py
+Comment: 
+
+Filename: aigc_zoo/model_zoo/qwen/tokenization_qwen.py
+Comment: 
+
 Filename: aigc_zoo/model_zoo/rwkv4/__init__.py
 Comment: 
 
 Filename: aigc_zoo/model_zoo/rwkv4/llm_model.py
 Comment: 
 
 Filename: aigc_zoo/model_zoo/rwkv4/rwkv4_tokenizer.py
@@ -123,23 +129,29 @@
 
 Filename: aigc_zoo/utils/rwkv4_generate.py
 Comment: 
 
 Filename: aigc_zoo/utils/streamgenerator.py
 Comment: 
 
-Filename: aigc_zoo-0.1.13.dist-info/LICENSE
+Filename: aigc_zoo/weight/__init__.py
+Comment: 
+
+Filename: aigc_zoo/weight/modelweighter.py
+Comment: 
+
+Filename: aigc_zoo-0.1.13.post0.dist-info/LICENSE
 Comment: 
 
-Filename: aigc_zoo-0.1.13.dist-info/METADATA
+Filename: aigc_zoo-0.1.13.post0.dist-info/METADATA
 Comment: 
 
-Filename: aigc_zoo-0.1.13.dist-info/WHEEL
+Filename: aigc_zoo-0.1.13.post0.dist-info/WHEEL
 Comment: 
 
-Filename: aigc_zoo-0.1.13.dist-info/top_level.txt
+Filename: aigc_zoo-0.1.13.post0.dist-info/top_level.txt
 Comment: 
 
-Filename: aigc_zoo-0.1.13.dist-info/RECORD
+Filename: aigc_zoo-0.1.13.post0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## aigc_zoo/model_zoo/baichuan/llm_model.py

```diff
@@ -1,13 +1,13 @@
 # coding=utf8
 # @Time    : 2023/5/12 20:41
 # @Author  : tk
 # @FileName: llm_model
-from deep_training.nlp.models.baichuan.modeling_baichuan import BaiChuanForCausalLM,TransformerBaiChuanLMHeadModel,BaiChuanConfig
-from deep_training.trainer.pl.modelweighter import *
+from deep_training.nlp.models.baichuan.modeling_baichuan import BaiChuanForCausalLM,TransformerBaiChuanLMHeadModel,BaiChuanConfig,setup_model_profile
+from ...weight.modelweighter import *
 from .tokenization_baichuan import BaiChuanTokenizer
 import logging
 logger = logging.getLogger(__name__)
 
 
 
 class TransformerForLM(TransformerBaiChuanLMHeadModel):
```

## aigc_zoo/model_zoo/baichuan2/llm_model.py

```diff
@@ -1,13 +1,13 @@
 # coding=utf8
 # @Time    : 2023/5/12 20:41
 # @Author  : tk
 # @FileName: llm_model
-from deep_training.nlp.models.baichuan2.modeling_baichuan import BaichuanForCausalLM,TransformerBaichuanLMHeadModel,BaichuanConfig
-from deep_training.trainer.pl.modelweighter import *
+from deep_training.nlp.models.baichuan2.modeling_baichuan import BaichuanForCausalLM,TransformerBaichuanLMHeadModel,BaichuanConfig,setup_model_profile
+from ...weight.modelweighter import *
 from .tokenization_baichuan import BaichuanTokenizer
 import logging
 logger = logging.getLogger(__name__)
 
 
 
 class TransformerForLM(TransformerBaichuanLMHeadModel):
```

## aigc_zoo/model_zoo/chatglm/llm_model.py

```diff
@@ -5,20 +5,20 @@
 
 import copy
 import os
 import re
 import warnings
 from typing import List, Tuple, Optional, Callable
 import torch
-from deep_training.nlp.models.chatglm import ChatGLMForConditionalGeneration,ChatGLMConfig, logger,setup_model_profile
+from deep_training.nlp.models.chatglm import ChatGLMForConditionalGeneration,ChatGLMConfig,setup_model_profile
 from deep_training.nlp.models.transformer import TransformerBase
 from torch import nn
 from transformers import LogitsProcessorList, LogitsProcessor, GenerationConfig, StoppingCriteriaList
 from .tokenization_chatglm import ChatGLMTokenizer
-from deep_training.trainer.pl.modelweighter import *
+from ...weight.modelweighter import *
 import logging
 logger = logging.getLogger(__name__)
 
 
 def build_masks_and_position_ids_glm(batch_input_ids, ctxlens, max_len = None):
     if max_len is None:
         max_len = batch_input_ids.size(1)
@@ -278,15 +278,15 @@
     def enable_input_require_grads(self):
         setattr(self.model, 'model_parallel', True)
         setattr(self.model, 'is_parallelizable', True)
         self.model.enable_input_require_grads()
 
 
 
-class MyTransformer(MyTransformerChatGlmLMHeadModel,ModelWeightMinMax, with_pl=True):
+class MyTransformer(MyTransformerChatGlmLMHeadModel,ModelWeightMixin, with_pl=True):
     def __init__(self, *args,new_num_tokens=None, **kwargs):
         lora_args: LoraArguments = kwargs.pop('lora_args',None)
         num_layers_freeze = kwargs.pop('num_layers_freeze',-1)
         super(MyTransformer, self).__init__(*args, **kwargs)
         self.lora_args = lora_args
 
         #可能添加新词
```

## aigc_zoo/model_zoo/chatglm/ppo_model.py

```diff
@@ -4,15 +4,15 @@
 import torch
 from deep_training.nlp.models.rl.modeling_ppo import ChatglmModelForCausalPrefixLMWithValueHead
 from deep_training.nlp.rl.ppo.configuration import PPOConfig,PPOArguments
 from deep_training.nlp.rl.ppo.ppo_module import PPOModelLoss
 from transformers import AdamW
 from deep_training.nlp.optimizer.lion import Lion
 from .llm_model import MyChatGLMForConditionalGeneration
-from deep_training.trainer.pl.modelweighter import *
+from ...weight.modelweighter import *
 import logging
 logger = logging.getLogger(__name__)
 
 class MyChatglmModelForCausalPrefixLMWithValueHead(ChatglmModelForCausalPrefixLMWithValueHead):
     def __init__(self, *args,up_sampling_score=False, **kwargs):
         # 如果显卡支持int8 可以开启
         load_in_8bit = kwargs.get('load_in_8bit', False)
@@ -33,15 +33,15 @@
         setattr(self.model, 'is_parallelizable', True)
         # self.model.gradient_checkpointing_enable()
         self.model.enable_input_require_grads()
 
 
 
 
-class MyPPOTransformer(MyChatglmModelForCausalPrefixLMWithValueHead,PPOModelLoss,ModelWeightMinMax, with_pl=True):
+class MyPPOTransformer(MyChatglmModelForCausalPrefixLMWithValueHead,PPOModelLoss,ModelWeightMixin, with_pl=True):
     def __init__(self, *args,new_num_tokens=None, **kwargs):
         lora_args: LoraConfig = kwargs.pop('lora_args', None)
         ppo_args: PPOConfig = kwargs.pop('ppo_args', None)
         super(MyPPOTransformer, self).__init__(*args, **kwargs)
 
         self.lora_args = lora_args
         self.ppo_config = ppo_args
```

## aigc_zoo/model_zoo/chatglm/reward_model.py

```diff
@@ -1,12 +1,12 @@
 # -*- coding: utf-8 -*-
 # @Author  : ssbuild
 # @Time    : 2023/5/29 13:34
 import torch
-from deep_training.trainer.pl.modelweighter import *
+from ...weight.modelweighter import *
 from torch import nn
 from deep_training.nlp.models.transformer_base import TransformerBase
 from .llm_model import MyChatGLMForConditionalGeneration
 import logging
 logger = logging.getLogger(__name__)
 
 
@@ -137,15 +137,15 @@
         return (value_a,scores)
 
 
 
 
 # 训练
 
-class MyRewardTransformer(MyRewardChatGlmLMHeadModel, ModelWeightMinMax, with_pl=True):
+class MyRewardTransformer(MyRewardChatGlmLMHeadModel, ModelWeightMixin, with_pl=True):
     def __init__(self, *args,new_num_tokens=None, **kwargs):
         lora_args: LoraConfig = kwargs.pop('lora_args', None)
         super(MyRewardTransformer, self).__init__(*args, **kwargs)
         self.lora_args = lora_args
         self.prompt_args = None
 
         self.resize_token_embs(new_num_tokens)
```

## aigc_zoo/model_zoo/chatglm2/llm_model.py

```diff
@@ -5,20 +5,20 @@
 
 import copy
 import os
 import re
 import warnings
 from typing import List, Tuple, Optional, Callable
 import torch
-from deep_training.nlp.models.chatglm2.modeling_chatglm import ChatGLMForConditionalGeneration,ChatGLMConfig, logger,setup_model_profile
+from deep_training.nlp.models.chatglm2.modeling_chatglm import ChatGLMForConditionalGeneration,ChatGLMConfig,setup_model_profile
 from deep_training.nlp.models.transformer import TransformerBase
 from torch import nn
 from transformers import LogitsProcessorList, LogitsProcessor, GenerationConfig, StoppingCriteriaList
 from .tokenization_chatglm import ChatGLMTokenizer
-from deep_training.trainer.pl.modelweighter import *
+from ...weight.modelweighter import *
 import logging
 logger = logging.getLogger(__name__)
 
 
 
 
 def build_masks_and_position_ids_glm(batch_input_ids, ctxlens):
@@ -138,15 +138,15 @@
 
 
 
 
 
 
 
-class MyTransformer(MyTransformerChatGlmLMHeadModel,ModelWeightMinMax, with_pl=True):
+class MyTransformer(MyTransformerChatGlmLMHeadModel,ModelWeightMixin, with_pl=True):
     def __init__(self, *args,new_num_tokens=None, **kwargs):
         lora_args: LoraArguments = kwargs.pop('lora_args',None)
         num_layers_freeze = kwargs.pop('num_layers_freeze',-1)
         super(MyTransformer, self).__init__(*args, **kwargs)
         self.lora_args = lora_args
 
         #可能添加新词
```

## aigc_zoo/model_zoo/internlm/llm_model.py

```diff
@@ -1,13 +1,13 @@
 # coding=utf8
 # @Time    : 2023/07/18 10:41
 # @Author  : tk
 # @FileName: llm_model
-from deep_training.nlp.models.internlm.modeling_internlm import InternLMForCausalLM,TransformerInternLMHeadModel,InternLMConfig
-from deep_training.trainer.pl.modelweighter import *
+from deep_training.nlp.models.internlm.modeling_internlm import InternLMForCausalLM,TransformerInternLMHeadModel,InternLMConfig,setup_model_profile
+from ...weight.modelweighter import *
 from .tokenization_internlm import InternLMTokenizer
 import logging
 logger = logging.getLogger(__name__)
 
 
 
 class TransformerForLM(TransformerInternLMHeadModel):
```

## aigc_zoo/model_zoo/llm/ilql_model.py

```diff
@@ -2,15 +2,15 @@
 # @Author  : ssbuild
 # @Time    : 2023/5/29 9:52
 import torch
 from deep_training.nlp.rl.ilql.configuration import ILQLArguments, ILQLConfig
 from deep_training.nlp.rl.ilql.ilql_module import ILQLModelLoss
 from deep_training.nlp.models.rl.modeling_ilql import AutoModelForCausalLMWithILQLHeads
 from transformers import AdamW
-from deep_training.trainer.pl.modelweighter import *
+from ...weight.modelweighter import *
 import logging
 logger = logging.getLogger(__name__)
 
 class ILQLModelForCausalLMWithILQLHeads(AutoModelForCausalLMWithILQLHeads):
     def __init__(self, *args,hidden_size=None, up_sampling_score=False,**kwargs):
         config = kwargs.get('config')
         if hidden_size is None:
@@ -31,15 +31,15 @@
     def enable_input_require_grads(self):
         setattr(self.model, 'model_parallel', True)
         setattr(self.model, 'is_parallelizable', True)
         # self.model.gradient_checkpointing_enable()
         self.model.enable_input_require_grads()
 
 
-class MyILQLTransformer(ILQLModelForCausalLMWithILQLHeads, ILQLModelLoss,ModelWeightMinMax, with_pl=True):
+class MyILQLTransformer(ILQLModelForCausalLMWithILQLHeads, ILQLModelLoss,ModelWeightMixin, with_pl=True):
     def __init__(self, *args, new_num_tokens = None, **kwargs):
         lora_args: LoraConfig = kwargs.pop('lora_args', None)
         prompt_args: PromptLearningConfig = kwargs.pop('prompt_args', None)
         ilql_args: ILQLConfig = kwargs.pop('ilql_args', None)
         if ilql_args is not None:
             kwargs.update({
                 "two_qs": ilql_args.two_qs,
```

## aigc_zoo/model_zoo/llm/llm_model.py

```diff
@@ -1,13 +1,13 @@
 # coding=utf8
 # @Time    : 2023/5/12 20:41
 # @Author  : tk
 # @FileName: llm_model
 from deep_training.nlp.models.transformer import TransformerForCausalLM
-from deep_training.trainer.pl.modelweighter import *
+from ...weight.modelweighter import *
 import logging
 logger = logging.getLogger(__name__)
 class TransformerForLM(TransformerForCausalLM):
     def __init__(self, *args, **kwargs):
         # 如果显卡支持int8 可以开启
         load_in_8bit = kwargs.get('load_in_8bit', False)
         load_in_4bit = kwargs.get('load_in_4bit', False)
@@ -40,15 +40,15 @@
         setattr(self.model, 'model_parallel', True)
         setattr(self.model, 'is_parallelizable', True)
         # self.model.gradient_checkpointing_enable()
         self.model.enable_input_require_grads()
 
 
 
-class MyTransformer(TransformerForLM, ModelWeightMinMax, with_pl=True):
+class MyTransformer(TransformerForLM, ModelWeightMixin, with_pl=True):
     def __init__(self, *args,new_num_tokens=None, **kwargs):
         lora_args: LoraConfig = kwargs.pop('lora_args', None)
         prompt_args: PromptLearningConfig = kwargs.pop('prompt_args', None)
         super(MyTransformer, self).__init__(*args, **kwargs)
         self.lora_args = lora_args
         self.prompt_args = prompt_args
```

## aigc_zoo/model_zoo/llm/ppo_model.py

```diff
@@ -5,15 +5,15 @@
 
 from typing import List, Tuple, Optional, Union
 import torch
 from deep_training.nlp.models.rl.modeling_ppo import AutoModelForCausalLMWithValueHead
 from deep_training.nlp.rl.ppo.configuration import PPOConfig,PPOArguments
 from deep_training.nlp.rl.ppo.ppo_module import PPOModelLoss
 from transformers import AdamW
-from deep_training.trainer.pl.modelweighter import *
+from ...weight.modelweighter import *
 import logging
 logger = logging.getLogger(__name__)
 
 class PPOModelForCausalLMWithValueHead(AutoModelForCausalLMWithValueHead):
     def __init__(self,*args,hidden_size=None, up_sampling_score=False,**kwargs):
         config = kwargs.get('config')
         if hidden_size is None:
@@ -44,15 +44,15 @@
 
 
 
 
 
 
 
-class MyPPOTransformer(PPOModelForCausalLMWithValueHead, PPOModelLoss,ModelWeightMinMax, with_pl=True):
+class MyPPOTransformer(PPOModelForCausalLMWithValueHead, PPOModelLoss,ModelWeightMixin, with_pl=True):
     def __init__(self, *args,new_num_tokens=None, **kwargs):
         lora_args: LoraConfig = kwargs.pop('lora_args', None)
         prompt_args: PromptLearningConfig = kwargs.pop('prompt_args', None)
         ppo_args: PPOConfig = kwargs.pop('ppo_args', None)
         super(MyPPOTransformer, self).__init__(*args, **kwargs)
 
         self.lora_args = lora_args
```

## aigc_zoo/model_zoo/llm/reward_model.py

```diff
@@ -1,14 +1,14 @@
 # -*- coding: utf-8 -*-
 # @Author  : ssbuild
 # @Time    : 2023/5/29 9:46
 import torch
 from deep_training.nlp.models.transformer import TransformerForCausalLM
 from torch import nn
-from deep_training.trainer.pl.modelweighter import *
+from ...weight.modelweighter import *
 
 import logging
 logger = logging.getLogger(__name__)
 
 class RewardModel(TransformerForCausalLM):
     def __init__(self, *args, **kwargs):
         load_in_8bit = kwargs.get('load_in_8bit', False)
@@ -133,15 +133,15 @@
             return (value_a,)
         scores = self.forward_score(batch["input_ids"], value_a)
         return (value_a, scores)
 
 
 
 
-class MyRewardTransformer(RewardModel, ModelWeightMinMax, with_pl=True):
+class MyRewardTransformer(RewardModel, ModelWeightMixin, with_pl=True):
     def __init__(self, *args,new_num_tokens=None,**kwargs):
         lora_args: LoraConfig = kwargs.pop('lora_args', None)
         prompt_args: PromptLearningConfig = kwargs.pop('prompt_args', None)
         super(MyRewardTransformer, self).__init__(*args, **kwargs)
         self.lora_args = lora_args
         self.prompt_args = prompt_args
```

## aigc_zoo/model_zoo/llm/rrhf_model.py

```diff
@@ -1,13 +1,13 @@
 # -*- coding: utf-8 -*-
 # @Time    : 2023/5/18 16:41
 import torch
 from deep_training.nlp.models.transformer import TransformerForCausalLM
 from torch.nn import functional as F
-from deep_training.trainer.pl.modelweighter import *
+from ...weight.modelweighter import *
 import logging
 logger = logging.getLogger(__name__)
 
 __all__ = [
     'RRHFModelForCausalLM',
     'MyRRHFTransformer',
 ]
@@ -82,15 +82,15 @@
 
 
 
 
 
 
 
-class MyRRHFTransformer(RRHFModelForCausalLM,ModelWeightMinMax,with_pl=True):
+class MyRRHFTransformer(RRHFModelForCausalLM,ModelWeightMixin,with_pl=True):
     def __init__(self, *args,new_num_tokens=None, **kwargs):
         lora_args: LoraConfig = kwargs.pop('lora_args', None)
         prompt_args: PromptLearningConfig = kwargs.pop('prompt_args', None)
         super(MyRRHFTransformer, self).__init__(*args, **kwargs)
 
         self.lora_args = lora_args
         self.prompt_args=prompt_args
```

## aigc_zoo/model_zoo/moss/llm_model.py

```diff
@@ -1,19 +1,19 @@
 # -*- coding: utf-8 -*-
 # @Author  : ssbuild
 # @Time    : 2023/5/29 16:14
 import re
-from deep_training.trainer.pl.modelweighter import *
+from ...weight.modelweighter import *
 from torch import nn
 from .moss_model import MyTransformerMossForCausalLM,MossConfig,MossTokenizer,MyMossForCausalLM
 import logging
 logger = logging.getLogger(__name__)
 
 
-class MyTransformer(MyTransformerMossForCausalLM,ModelWeightMinMax, with_pl=True):
+class MyTransformer(MyTransformerMossForCausalLM,ModelWeightMixin, with_pl=True):
     def __init__(self, *args,new_num_tokens=None, **kwargs):
         lora_args: LoraConfig = kwargs.pop('lora_args',None)
         prompt_args: PromptLearningConfig = kwargs.pop('prompt_args', None)
         num_layers_freeze = kwargs.pop('num_layers_freeze', -1)
         super(MyTransformer, self).__init__(*args, **kwargs)
         self.lora_args = lora_args
         self.prompt_args = prompt_args
```

## aigc_zoo/model_zoo/rwkv4/llm_model.py

```diff
@@ -1,15 +1,15 @@
 # coding=utf8
 # @Time    : 2023/5/12 20:41
 # @Author  : tk
 # @FileName: llm_model
 import torch
 from deep_training.nlp.models.rwkv4.modeling_rwkv import TransformerRWKV4LMHeadModel, RwkvConfig, set_model_profile, \
     RwkvForCausalLM
-from deep_training.trainer.pl.modelweighter import *
+from ...weight.modelweighter import *
 import logging
 
 from torch import nn
 
 logger = logging.getLogger(__name__)
 
 
@@ -53,15 +53,15 @@
             m.disable_input_require_grads()
         m.enable_input_require_grads()
 
 
 
 
 
-class MyTransformer(TransformerRWKV4ForLM, ModelWeightMinMax, with_pl=True):
+class MyTransformer(TransformerRWKV4ForLM, ModelWeightMixin, with_pl=True):
     def __init__(self, *args,new_num_tokens=None, **kwargs):
         lora_args: LoraConfig = kwargs.pop('lora_args', None)
         prompt_args: PromptLearningConfig = kwargs.pop('prompt_args', None)
         super(MyTransformer, self).__init__(*args, **kwargs)
         self.lora_args = lora_args
         self.prompt_args = prompt_args
```

## aigc_zoo/model_zoo/t5/llm_model.py

```diff
@@ -1,13 +1,13 @@
 # @Time    : 2023/4/2 23:14
 # @Author  : tk
 # @FileName: models
 
 from deep_training.nlp.models.transformer import TransformerForSeq2SeqLM
-from deep_training.trainer.pl.modelweighter import *
+from ...weight.modelweighter import *
 from transformers import T5ForConditionalGeneration
 import logging
 logger = logging.getLogger(__name__)
 
 class TransformerForLM(TransformerForSeq2SeqLM):
     def __init__(self, *args, **kwargs):
         # 如果显卡支持int8 可以开启 ， 需安装依赖 pip install bitsandbytes
@@ -29,15 +29,15 @@
         # setattr(self.model, 'is_parallelizable', True)
         # self.model.gradient_checkpointing_enable()
         self.model.enable_input_require_grads()
 
 
 
 
-class MyTransformer(TransformerForLM,ModelWeightMinMax, with_pl=True):
+class MyTransformer(TransformerForLM,ModelWeightMixin, with_pl=True):
     def __init__(self, *args,new_num_tokens=None, **kwargs):
         lora_args: LoraConfig = kwargs.pop('lora_args', None)
         prompt_args: PromptLearningConfig = kwargs.pop('prompt_args', None)
         super(MyTransformer, self).__init__(*args, **kwargs)
         self.lora_args = lora_args
         self.prompt_args = prompt_args
```

## aigc_zoo/model_zoo/t5/ppo_model.py

```diff
@@ -1,15 +1,15 @@
 # -*- coding: utf-8 -*-
 # @Author  : ssbuild
 # @Time    : 2023/5/29 15:32
 import torch
 from deep_training.nlp.models.rl.modeling_ppo import AutoModelForSeq2SeqLMWithValueHead
 from deep_training.nlp.rl.ppo.configuration import PPOConfig,PPOArguments
 from deep_training.nlp.rl.ppo.ppo_module import PPOModelLoss
-from deep_training.trainer.pl.modelweighter import *
+from ...weight.modelweighter import *
 from transformers import AdamW
 import logging
 logger = logging.getLogger(__name__)
 
 class MyModelForSeq2SeqLMWithValueHead(AutoModelForSeq2SeqLMWithValueHead):
     def __init__(self, *args,up_sampling_score=False,**kwargs):
         # 如果显卡支持int8 可以开启 ， 需安装依赖 pip install bitsandbytes
@@ -29,15 +29,15 @@
         # setattr(self.model, 'model_parallel', True)
         # setattr(self.model, 'is_parallelizable', True)
         # self.model.gradient_checkpointing_enable()
         self.model.enable_input_require_grads()
 
 
 
-class MyPPOTransformer(MyModelForSeq2SeqLMWithValueHead,PPOModelLoss,ModelWeightMinMax, with_pl=True):
+class MyPPOTransformer(MyModelForSeq2SeqLMWithValueHead,PPOModelLoss,ModelWeightMixin, with_pl=True):
     def __init__(self, *args,new_num_tokens=None, **kwargs):
         lora_args: LoraConfig = kwargs.pop('lora_args', None)
         ppo_args: PPOConfig = kwargs.pop('ppo_args', None)
         super(MyPPOTransformer, self).__init__(*args, **kwargs)
 
         self.lora_args = lora_args
         self.ppo_config = ppo_args
```

## aigc_zoo/model_zoo/t5/reward_model.py

```diff
@@ -1,14 +1,14 @@
 # -*- coding: utf-8 -*-
 # @Author  : ssbuild
 # @Time    : 2023/5/29 15:32
 import torch
 from torch import nn
 from deep_training.nlp.models.transformer import TransformerForSeq2SeqLM
-from deep_training.trainer.pl.modelweighter import *
+from ...weight.modelweighter import *
 import logging
 logger = logging.getLogger(__name__)
 
 __all__ = [
     'RewardT5Model',
     'RewardTransformer'
 ]
@@ -128,15 +128,15 @@
             return (value_a,)
         scores = self.forward_score(batch["decoder_input_ids"], value_a)
         return (value_a, scores)
 
 
 
 
-class RewardTransformer(RewardT5Model,ModelWeightMinMax, with_pl=True):
+class RewardTransformer(RewardT5Model,ModelWeightMixin, with_pl=True):
     def __init__(self, *args,new_num_tokens=None, **kwargs):
         lora_args: LoraConfig = kwargs.pop('lora_args', None)
         super(RewardTransformer, self).__init__(*args, **kwargs)
         self.lora_args = lora_args
 
         self.resize_token_embs(new_num_tokens)
```

## Comparing `aigc_zoo-0.1.13.dist-info/LICENSE` & `aigc_zoo-0.1.13.post0.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `aigc_zoo-0.1.13.dist-info/RECORD` & `aigc_zoo-0.1.13.post0.dist-info/RECORD`

 * *Files 22% similar despite different names*

```diff
@@ -1,48 +1,52 @@
-aigc_zoo/.gitignore,sha256=WMWRJLHvEkjvi4h1Yem_XEBDutffhQXAiaW1vlgFlv8,7
 aigc_zoo/__init__.py,sha256=mmJF0EECVZ_Bj8ul8zq9hou9n4ktDZLo7dUa8vX32g8,80
-aigc_zoo/requirements.txt,sha256=jjyqI-79blp4Xp2VambtFTc7GajPfyXEmUF6ELnx9oI,105
 aigc_zoo/model_zoo/__init__.py,sha256=hab96oKzPI-gzccPWfe8027s99Z7zELBE5D0DVKSTnU,77
 aigc_zoo/model_zoo/baichuan/__init__.py,sha256=2WFhbzYihC48B1ZSY2a42c1l8F8gIxxWh8slLoxGEt0,66
-aigc_zoo/model_zoo/baichuan/llm_model.py,sha256=9vqXeEyvqgjdQmviJnPylFqNtqNR3hyMLnmb1t1VNUI,5578
+aigc_zoo/model_zoo/baichuan/llm_model.py,sha256=fDP4zxnYfvV2j9bFMI5ykAIIVOcfVLXYAeVUMqh-s9g,5583
 aigc_zoo/model_zoo/baichuan/tokenization_baichuan.py,sha256=ZUGfjVTCcNiKfG_JV8TaZsdw7acnUXXxkOpH-wGwht4,9574
 aigc_zoo/model_zoo/baichuan2/__init__.py,sha256=2WFhbzYihC48B1ZSY2a42c1l8F8gIxxWh8slLoxGEt0,66
-aigc_zoo/model_zoo/baichuan2/llm_model.py,sha256=-MYekAWzGJO9FX9e79LJTcisgY-kOA7aSwKNvhjT94o,5575
+aigc_zoo/model_zoo/baichuan2/llm_model.py,sha256=2Q6NhtgR8FQYkDKPR81MDiSf2bSeCF28USStRwvGAXM,5580
 aigc_zoo/model_zoo/baichuan2/tokenization_baichuan.py,sha256=15HCHJatue1VgvRZ86HbnaYMgodHt0MHZsi2pIUBYTY,8720
 aigc_zoo/model_zoo/chatglm/__init__.py,sha256=RFimplzKNFznknZ0MzZDOhcvck-tAJQ53rTCMX-shS8,77
-aigc_zoo/model_zoo/chatglm/llm_model.py,sha256=YLHqq5MaQMZLnie0ea6BTgj5aYV2-abatZIVyL_IJHU,17057
-aigc_zoo/model_zoo/chatglm/ppo_model.py,sha256=MjbAJm22oitIMJrJ-RxbHJv0df4m80G4BQsBXorwqac,6435
-aigc_zoo/model_zoo/chatglm/reward_model.py,sha256=Xa5yONvwlIknxMEK2Y59gE_our5iICtsUyaPmi86by0,9700
+aigc_zoo/model_zoo/chatglm/llm_model.py,sha256=jFtr-dj2geg3hodJu2R826ic1C8ddaH7jffmvC_qJ3o,17033
+aigc_zoo/model_zoo/chatglm/ppo_model.py,sha256=mBjr_675_x3fhhUxCiBjx6fmyi4UfHWzAZIr4Lnxp7I,6419
+aigc_zoo/model_zoo/chatglm/reward_model.py,sha256=6f_yYWjVFpx-FfrDBdk7fP8BMjwADPrGtonuKr9_HYk,9684
 aigc_zoo/model_zoo/chatglm/tokenization_chatglm.py,sha256=IMyHa8uPOgE0ia1DYp8Lx-IZ0N4TKSh1HVlA5sDdw-s,17037
 aigc_zoo/model_zoo/chatglm2/__init__.py,sha256=3cMgJqib4OqHJHv37rn3xHzyBbGl9s1zpTqd549QDmw,77
 aigc_zoo/model_zoo/chatglm2/chatglm_model.py,sha256=5bqiInqxxSAi0DB4zDZmzEQ_v4DluJQbnhTlx8R31Ik,114
-aigc_zoo/model_zoo/chatglm2/llm_model.py,sha256=8OJsp7qGhJkXLzmj8JXuGQ8HXk1cHUQj14p8Rex6aoc,9402
+aigc_zoo/model_zoo/chatglm2/llm_model.py,sha256=LiwganG066IcEBatGCIDSsjfSVGsnlHA13Cnb0e70EY,9378
 aigc_zoo/model_zoo/chatglm2/tokenization_chatglm.py,sha256=LFOlpiWgpz6oRjNimO5T85Gm6q-nbunyrpm--nQOcJo,10002
 aigc_zoo/model_zoo/internlm/__init__.py,sha256=-3UmkJC8t2B6lZWMqdpTgCuoBcevA-7PdlZj4h5AYNI,76
-aigc_zoo/model_zoo/internlm/llm_model.py,sha256=XCJu5L1aScJq1oY7gRHlRuyBPqMmKvjtDR0351I5PEg,5571
+aigc_zoo/model_zoo/internlm/llm_model.py,sha256=HLqCzAA1bZaw4tNHQOMBSFVqlt9jy609f08eQo-AbWI,5576
 aigc_zoo/model_zoo/internlm/tokenization_internlm.py,sha256=vFW7WSDm8craOxPXQn83BPm0WSc66KM6ccVYQW9mcPM,8954
 aigc_zoo/model_zoo/llm/__init__.py,sha256=RFimplzKNFznknZ0MzZDOhcvck-tAJQ53rTCMX-shS8,77
-aigc_zoo/model_zoo/llm/ilql_model.py,sha256=ZMpmamGRakLJx3V3ynS8XUN8s0IXLpNAD8WdRfDbdcw,6344
-aigc_zoo/model_zoo/llm/llm_model.py,sha256=x176u-JFBww6RtAKzOUevUL7iQQ6t50xrlg2vom-uaM,5443
-aigc_zoo/model_zoo/llm/ppo_model.py,sha256=tH0Rk1Oy4AOFrvs7tYtrmAjJY5Xoxugk3XILVvLfic4,6298
-aigc_zoo/model_zoo/llm/reward_model.py,sha256=Wa7fkI0z83Lg9uy8GZBN_JQ7kkhpnf6YLzZSjqm1YCQ,9732
-aigc_zoo/model_zoo/llm/rrhf_model.py,sha256=bTN95boS_ndI9_-ux0xHeL1B7vgQXvEKdJZezyTO0zg,6704
+aigc_zoo/model_zoo/llm/ilql_model.py,sha256=tpRekQEcFRhu4O53EJ0gSkHyZ5JM98EnXVMqK5jMEk0,6328
+aigc_zoo/model_zoo/llm/llm_model.py,sha256=Za9xpv4dltbTy-W1vIXym-WE4146hYmyGpTocNJISAQ,5427
+aigc_zoo/model_zoo/llm/ppo_model.py,sha256=EGoFpH4ixLFpgDeVto5uqJ3f31YkRgdJotXtAYg_7gY,6282
+aigc_zoo/model_zoo/llm/reward_model.py,sha256=Ae-Mgt2i9DvhN6t-Tib5rziaFsFKTaElV69SRzdGIAk,9716
+aigc_zoo/model_zoo/llm/rrhf_model.py,sha256=42dZOcD5T7nHsEjODTTPw57k1LTINASghkpUzOH297s,6688
 aigc_zoo/model_zoo/moss/__init__.py,sha256=RFimplzKNFznknZ0MzZDOhcvck-tAJQ53rTCMX-shS8,77
-aigc_zoo/model_zoo/moss/llm_model.py,sha256=_Z8uPLQBlvTMoY_tABvjkY07sFOHcObKxAynOg9A6kY,4554
+aigc_zoo/model_zoo/moss/llm_model.py,sha256=sfyJUgKCKRZzTrm_zEwYefXXM48clwF7RtheBN362JM,4538
 aigc_zoo/model_zoo/moss/moss_model.py,sha256=lwvkju3ZdQeir0gO15uvVLCcKbpgmO9iq1Kbu8a3cAw,2050
+aigc_zoo/model_zoo/qwen/__init__.py,sha256=P87H1JDOoEhefhB9CXY1Tjw6U35LvA7BVYzGlDn6A8U,76
+aigc_zoo/model_zoo/qwen/llm_model.py,sha256=e7dc2c8kxa33vM-dqH0DjNP1Htq_ic8lorQbNT0bLcc,7256
+aigc_zoo/model_zoo/qwen/qwen_generation_utils.py,sha256=XoILgNfierIYbwnZSMGZPrlGqpiiyCiNPSb0bbn0yoA,14379
+aigc_zoo/model_zoo/qwen/tokenization_qwen.py,sha256=wJIEGZtRVlSuAnknYi5_VfbX-XD6G3tiPYrtKdV_Rho,9422
 aigc_zoo/model_zoo/rwkv4/__init__.py,sha256=RFimplzKNFznknZ0MzZDOhcvck-tAJQ53rTCMX-shS8,77
-aigc_zoo/model_zoo/rwkv4/llm_model.py,sha256=9LwhDnf2CMMyN9R5W0XqX4EM_AXMdywB85_QObuzRwI,5734
+aigc_zoo/model_zoo/rwkv4/llm_model.py,sha256=gmkZ5wEzoeGh-5oWgRJ3KZqeBsdKrAa_chi_-4X_5Kc,5718
 aigc_zoo/model_zoo/rwkv4/rwkv4_tokenizer.py,sha256=p32IYzEyE6Snxkl8Ds7GuhGHXTiDezUUKbWOsN62vxw,6519
 aigc_zoo/model_zoo/t5/__init__.py,sha256=RFimplzKNFznknZ0MzZDOhcvck-tAJQ53rTCMX-shS8,77
-aigc_zoo/model_zoo/t5/llm_model.py,sha256=CJmPXz4Q04geCgMFiN8AyPYLwVfthW0UvdZhipkUdWw,4950
-aigc_zoo/model_zoo/t5/ppo_model.py,sha256=eXm5sKi4WgISU95r25OCJhrP_ziYy3wRwALn1bf6_24,5848
-aigc_zoo/model_zoo/t5/reward_model.py,sha256=YhWq_Q6zNgQYlyrR3R79NDUkf4BZZiSB8gMzjS2n9kY,9098
+aigc_zoo/model_zoo/t5/llm_model.py,sha256=FlyHjgEpYPA6Wny4S0mISJnVsYSnAdw-PKBtLkPR5ws,4934
+aigc_zoo/model_zoo/t5/ppo_model.py,sha256=Wu8OQ7w6R94mZzWFQGHHvMZpULYjcxHSeDFLSSjMl-0,5832
+aigc_zoo/model_zoo/t5/reward_model.py,sha256=09Ij6YFFEGaM8R-phWrxiCUrvY9aH1E5Y3by5pi6L6E,9082
 aigc_zoo/utils/__init__.py,sha256=Oc9cllKC2z1rKpYMLkfRj01Jud2WLQaZ_Dd89t4sreY,77
 aigc_zoo/utils/llm_generate.py,sha256=92TvTXISdU7GSrBgMYJoFyicWxkXrm_-02O4jcDnMp0,2664
 aigc_zoo/utils/moss_generate.py,sha256=NYcvqjm3NbutslYlF8_7_BiHNBMPPI0mZzVui4nmkIU,13198
 aigc_zoo/utils/rwkv4_generate.py,sha256=y0HzEYG5Wu9r4NIoFE8rhealv_KJriV8rlhEV-tJpnU,3429
 aigc_zoo/utils/streamgenerator.py,sha256=C1HLeanS0JUokFJIQpY90BKEOlvh8TZvPPCHut-q2EU,748
-aigc_zoo-0.1.13.dist-info/LICENSE,sha256=xx0jnfkXJvxRnG63LTGOxlggYnIysveWIZ6H3PNdCrQ,11357
-aigc_zoo-0.1.13.dist-info/METADATA,sha256=luprLn-vqsVtC9k5-ZQHPDTfd5QkWOpotxo9PcLkLWE,351
-aigc_zoo-0.1.13.dist-info/WHEEL,sha256=OqRkF0eY5GHssMorFjlbTIq072vpHpF60fIQA6lS9xA,92
-aigc_zoo-0.1.13.dist-info/top_level.txt,sha256=dl7_T4oT72ShHHM2khyOXJL4Kyvq0u_TdVolu-lKbnY,9
-aigc_zoo-0.1.13.dist-info/RECORD,,
+aigc_zoo/weight/__init__.py,sha256=wnFJZRkSekOPLWe7Xhy8zDktVerx_xPpP7zPhcB8GsU,79
+aigc_zoo/weight/modelweighter.py,sha256=HGbKl6JZ8CwCil-MlbJREwcG54ONymooBzOzjca4XLI,5250
+aigc_zoo-0.1.13.post0.dist-info/LICENSE,sha256=xx0jnfkXJvxRnG63LTGOxlggYnIysveWIZ6H3PNdCrQ,11357
+aigc_zoo-0.1.13.post0.dist-info/METADATA,sha256=T1llTViW0DETups3aBzuUYUO2p_SEImYQh1WcmJhiy0,369
+aigc_zoo-0.1.13.post0.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
+aigc_zoo-0.1.13.post0.dist-info/top_level.txt,sha256=dl7_T4oT72ShHHM2khyOXJL4Kyvq0u_TdVolu-lKbnY,9
+aigc_zoo-0.1.13.post0.dist-info/RECORD,,
```

