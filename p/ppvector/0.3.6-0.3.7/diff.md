# Comparing `tmp/ppvector-0.3.6-py3-none-any.whl.zip` & `tmp/ppvector-0.3.7-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,40 +1,40 @@
-Zip file size: 58323 bytes, number of entries: 38
--rw-rw-rw-  2.0 fat      124 b- defN 23-Apr-02 15:01 ppvector/__init__.py
--rw-rw-rw-  2.0 fat    15091 b- defN 23-Apr-02 15:00 ppvector/predict.py
--rw-rw-rw-  2.0 fat    24585 b- defN 23-Apr-02 10:24 ppvector/trainer.py
+Zip file size: 58824 bytes, number of entries: 38
+-rw-rw-rw-  2.0 fat      124 b- defN 23-Aug-05 09:02 ppvector/__init__.py
+-rw-rw-rw-  2.0 fat    16208 b- defN 23-Aug-05 09:02 ppvector/predict.py
+-rw-rw-rw-  2.0 fat    24577 b- defN 23-Aug-05 09:02 ppvector/trainer.py
 -rw-rw-rw-  2.0 fat        0 b- defN 22-Nov-04 14:44 ppvector/data_utils/__init__.py
--rw-rw-rw-  2.0 fat    22703 b- defN 23-Mar-28 11:42 ppvector/data_utils/audio.py
+-rw-rw-rw-  2.0 fat    22239 b- defN 23-Aug-05 09:02 ppvector/data_utils/audio.py
 -rw-rw-rw-  2.0 fat      909 b- defN 23-Feb-15 13:35 ppvector/data_utils/collate_fn.py
--rw-rw-rw-  2.0 fat     2918 b- defN 23-Mar-26 02:04 ppvector/data_utils/featurizer.py
--rw-rw-rw-  2.0 fat     3121 b- defN 23-Mar-28 11:26 ppvector/data_utils/reader.py
+-rw-rw-rw-  2.0 fat     2918 b- defN 23-Aug-05 09:02 ppvector/data_utils/featurizer.py
+-rw-rw-rw-  2.0 fat     3121 b- defN 23-Aug-05 09:02 ppvector/data_utils/reader.py
 -rw-rw-rw-  2.0 fat     4653 b- defN 23-Mar-28 11:51 ppvector/data_utils/utils.py
--rw-rw-rw-  2.0 fat        0 b- defN 22-Nov-04 14:44 ppvector/data_utils/augmentor/__init__.py
--rw-rw-rw-  2.0 fat     4274 b- defN 23-Feb-15 13:35 ppvector/data_utils/augmentor/augmentation.py
--rw-rw-rw-  2.0 fat      965 b- defN 22-Nov-04 14:44 ppvector/data_utils/augmentor/base.py
--rw-rw-rw-  2.0 fat     1990 b- defN 23-Mar-28 11:42 ppvector/data_utils/augmentor/noise_perturb.py
--rw-rw-rw-  2.0 fat      983 b- defN 22-Nov-04 14:44 ppvector/data_utils/augmentor/resample.py
--rw-rw-rw-  2.0 fat     1013 b- defN 22-Nov-04 14:44 ppvector/data_utils/augmentor/shift_perturb.py
--rw-rw-rw-  2.0 fat     1984 b- defN 22-Nov-04 14:44 ppvector/data_utils/augmentor/speed_perturb.py
--rw-rw-rw-  2.0 fat     1144 b- defN 22-Nov-04 14:44 ppvector/data_utils/augmentor/volume_perturb.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-Aug-05 09:02 ppvector/data_utils/augmentor/__init__.py
+-rw-rw-rw-  2.0 fat     4274 b- defN 23-Aug-05 09:02 ppvector/data_utils/augmentor/augmentation.py
+-rw-rw-rw-  2.0 fat      965 b- defN 23-Aug-05 09:02 ppvector/data_utils/augmentor/base.py
+-rw-rw-rw-  2.0 fat     2575 b- defN 23-Aug-05 09:02 ppvector/data_utils/augmentor/noise_perturb.py
+-rw-rw-rw-  2.0 fat      983 b- defN 23-Aug-05 09:02 ppvector/data_utils/augmentor/resample.py
+-rw-rw-rw-  2.0 fat     1013 b- defN 23-Aug-05 09:02 ppvector/data_utils/augmentor/shift_perturb.py
+-rw-rw-rw-  2.0 fat     1984 b- defN 23-Aug-05 09:02 ppvector/data_utils/augmentor/speed_perturb.py
+-rw-rw-rw-  2.0 fat     1144 b- defN 23-Aug-05 09:02 ppvector/data_utils/augmentor/volume_perturb.py
 -rw-rw-rw-  2.0 fat        0 b- defN 23-Mar-16 05:03 ppvector/metric/__init__.py
--rw-rw-rw-  2.0 fat     2080 b- defN 23-Apr-02 10:20 ppvector/metric/metrics.py
+-rw-rw-rw-  2.0 fat     2080 b- defN 23-Aug-05 09:02 ppvector/metric/metrics.py
 -rw-rw-rw-  2.0 fat        0 b- defN 22-Nov-05 11:08 ppvector/models/__init__.py
--rw-rw-rw-  2.0 fat    10996 b- defN 23-Mar-21 11:41 ppvector/models/ecapa_tdnn.py
--rw-rw-rw-  2.0 fat     4058 b- defN 23-Mar-18 11:57 ppvector/models/fc.py
--rw-rw-rw-  2.0 fat     4029 b- defN 23-Mar-18 15:25 ppvector/models/loss.py
--rw-rw-rw-  2.0 fat     4541 b- defN 23-Mar-18 11:11 ppvector/models/pooling.py
--rw-rw-rw-  2.0 fat     6743 b- defN 23-Mar-19 03:37 ppvector/models/res2net.py
--rw-rw-rw-  2.0 fat     6615 b- defN 23-Mar-19 02:55 ppvector/models/resnet_se.py
--rw-rw-rw-  2.0 fat     3473 b- defN 23-Mar-18 11:58 ppvector/models/tdnn.py
+-rw-rw-rw-  2.0 fat    10996 b- defN 23-Aug-05 09:02 ppvector/models/ecapa_tdnn.py
+-rw-rw-rw-  2.0 fat     4058 b- defN 23-Aug-05 09:02 ppvector/models/fc.py
+-rw-rw-rw-  2.0 fat     4029 b- defN 23-Aug-05 09:02 ppvector/models/loss.py
+-rw-rw-rw-  2.0 fat     4541 b- defN 23-Aug-05 09:02 ppvector/models/pooling.py
+-rw-rw-rw-  2.0 fat     6743 b- defN 23-Aug-05 09:02 ppvector/models/res2net.py
+-rw-rw-rw-  2.0 fat     6615 b- defN 23-Aug-05 09:02 ppvector/models/resnet_se.py
+-rw-rw-rw-  2.0 fat     3473 b- defN 23-Aug-05 09:02 ppvector/models/tdnn.py
 -rw-rw-rw-  2.0 fat     5049 b- defN 23-Mar-18 10:41 ppvector/models/utils.py
 -rw-rw-rw-  2.0 fat        0 b- defN 22-Nov-04 14:44 ppvector/utils/__init__.py
 -rw-rw-rw-  2.0 fat     2839 b- defN 22-Nov-04 14:44 ppvector/utils/logger.py
--rw-rw-rw-  2.0 fat     1422 b- defN 23-Apr-02 10:20 ppvector/utils/lr.py
+-rw-rw-rw-  2.0 fat     1422 b- defN 23-Aug-05 09:02 ppvector/utils/lr.py
 -rw-rw-rw-  2.0 fat     1067 b- defN 23-Mar-23 11:45 ppvector/utils/record.py
 -rw-rw-rw-  2.0 fat     2790 b- defN 23-Mar-16 11:21 ppvector/utils/utils.py
--rw-rw-rw-  2.0 fat    11558 b- defN 23-Apr-02 15:02 ppvector-0.3.6.dist-info/LICENSE
--rw-rw-rw-  2.0 fat    28050 b- defN 23-Apr-02 15:02 ppvector-0.3.6.dist-info/METADATA
--rw-rw-rw-  2.0 fat       92 b- defN 23-Apr-02 15:02 ppvector-0.3.6.dist-info/WHEEL
--rw-rw-rw-  2.0 fat        9 b- defN 23-Apr-02 15:02 ppvector-0.3.6.dist-info/top_level.txt
-?rw-rw-r--  2.0 fat     3244 b- defN 23-Apr-02 15:02 ppvector-0.3.6.dist-info/RECORD
-38 files, 185112 bytes uncompressed, 53113 bytes compressed:  71.3%
+-rw-rw-rw-  2.0 fat    11558 b- defN 23-Aug-05 09:03 ppvector-0.3.7.dist-info/LICENSE
+-rw-rw-rw-  2.0 fat    28649 b- defN 23-Aug-05 09:03 ppvector-0.3.7.dist-info/METADATA
+-rw-rw-rw-  2.0 fat       92 b- defN 23-Aug-05 09:03 ppvector-0.3.7.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat        9 b- defN 23-Aug-05 09:03 ppvector-0.3.7.dist-info/top_level.txt
+?rw-rw-r--  2.0 fat     3244 b- defN 23-Aug-05 09:03 ppvector-0.3.7.dist-info/RECORD
+38 files, 186941 bytes uncompressed, 53614 bytes compressed:  71.3%
```

## zipnote {}

```diff
@@ -93,23 +93,23 @@
 
 Filename: ppvector/utils/record.py
 Comment: 
 
 Filename: ppvector/utils/utils.py
 Comment: 
 
-Filename: ppvector-0.3.6.dist-info/LICENSE
+Filename: ppvector-0.3.7.dist-info/LICENSE
 Comment: 
 
-Filename: ppvector-0.3.6.dist-info/METADATA
+Filename: ppvector-0.3.7.dist-info/METADATA
 Comment: 
 
-Filename: ppvector-0.3.6.dist-info/WHEEL
+Filename: ppvector-0.3.7.dist-info/WHEEL
 Comment: 
 
-Filename: ppvector-0.3.6.dist-info/top_level.txt
+Filename: ppvector-0.3.7.dist-info/top_level.txt
 Comment: 
 
-Filename: ppvector-0.3.6.dist-info/RECORD
+Filename: ppvector-0.3.7.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## ppvector/__init__.py

```diff
@@ -1,3 +1,3 @@
-__version__ = "0.3.6"
+__version__ = "0.3.7"
 # 项目支持的模型
 SUPPORT_MODEL = ['ecapa_tdnn', 'EcapaTdnn', 'Res2Net', 'ResNetSE', 'TDNN']
```

## ppvector/predict.py

```diff
@@ -1,9 +1,10 @@
 import os
 import pickle
+import shutil
 from io import BufferedReader
 
 import numpy as np
 import paddle
 import yaml
 from sklearn.metrics.pairwise import cosine_similarity
 from tqdm import tqdm
@@ -257,20 +258,19 @@
     def contrast(self, audio_data1, audio_data2):
         feature1 = self.predict(audio_data1)
         feature2 = self.predict(audio_data2)
         # 对角余弦值
         dist = np.dot(feature1, feature2) / (np.linalg.norm(feature1) * np.linalg.norm(feature2))
         return dist
 
-    # 声纹注册
     def register(self,
                  user_name,
                  audio_data,
                  sample_rate=16000):
-        """加载音频
+        """声纹注册
         :param user_name: 注册用户的名字
         :param audio_data: 需要识别的数据，支持文件路径，文件对象，字节，numpy。如果是字节的话，必须是完整的字节文件
         :param sample_rate: 如果传入的事numpy数据，需要指定采样率
         :return: 识别的文本结果和解码的得分数
         """
         # 加载音频文件
         if isinstance(audio_data, str):
@@ -297,14 +297,37 @@
         os.makedirs(os.path.dirname(audio_path), exist_ok=True)
         audio_segment.to_wav_file(audio_path)
         self.users_audio_path.append(audio_path.replace('\\', '/'))
         self.users_name.append(user_name)
         self.__write_index()
         return True, "注册成功"
 
-    # 声纹识别
     def recognition(self, audio_data, threshold=None, sample_rate=16000):
+        """声纹识别
+        :param audio_data: 需要识别的数据，支持文件路径，文件对象，字节，numpy。如果是字节的话，必须是完整的字节文件
+        :param threshold: 判断的阈值，如果为None则用创建对象时使用的阈值
+        :param sample_rate: 如果传入的事numpy数据，需要指定采样率
+        :return: 识别的用户名称，如果为None，即没有识别到用户
+        """
         if threshold:
             self.threshold = threshold
         feature = self.predict(audio_data, sample_rate=sample_rate)
         name = self.__retrieval(np_feature=[feature])[0]
         return name
+
+    def remove_user(self, user_name):
+        """删除用户
+
+        :param user_name: 用户名
+        :return:
+        """
+        if user_name in self.users_name:
+            indexes = [i for i in range(len(self.users_name)) if self.users_name[i] == user_name]
+            for index in sorted(indexes, reverse=True):
+                del self.users_name[index]
+                del self.users_audio_path[index]
+                self.audio_feature = np.delete(self.audio_feature, index, axis=0)
+            self.__write_index()
+            shutil.rmtree(os.path.join(self.audio_db_path, user_name))
+            return True
+        else:
+            return False
```

## ppvector/trainer.py

```diff
@@ -239,16 +239,16 @@
             los = self.loss(output, label)
             los.backward()
             self.optimizer.step()
             self.optimizer.clear_grad()
             # 计算准确率
             label = paddle.reshape(label, shape=(-1, 1))
             acc = accuracy(input=paddle.nn.functional.softmax(output), label=label)
-            accuracies.append(acc.numpy()[0])
-            loss_sum.append(los.numpy()[0])
+            accuracies.append(float(acc))
+            loss_sum.append(float(los))
             train_times.append((time.time() - start) * 1000)
 
             # 多卡训练只使用一个进程打印
             if batch_id % self.configs.train_conf.log_interval == 0 and local_rank == 0:
                 # 计算每秒训练数据量
                 train_speed = self.configs.dataset_conf.batch_size / (sum(train_times) / len(train_times) / 1000)
                 # 计算剩余时间
```

## ppvector/data_utils/audio.py

```diff
@@ -411,39 +411,31 @@
                              "than original segment.")
         start_time = random.uniform(0.0, self.duration - subsegment_length)
         self.subsegment(start_time, start_time + subsegment_length)
 
     def add_noise(self,
                   noise,
                   snr_dB,
-                  allow_downsampling=False,
                   max_gain_db=300.0):
         """以特定的信噪比添加给定的噪声段。如果噪声段比该噪声段长，则从该噪声段中采样匹配长度的随机子段。
 
         Note that this is an in-place transformation.
 
         :param noise: Noise signal to add.
         :type noise: AudioSegment
         :param snr_dB: Signal-to-Noise Ratio, in decibels.
         :type snr_dB: float
-        :param allow_downsampling: Whether to allow the noise signal to be
-                                   downsampled to match the base signal sample
-                                   rate.
-        :type allow_downsampling: bool
         :param max_gain_db: Maximum amount of gain to apply to noise signal
                             before adding it in. This is to prevent attempting
                             to apply infinite gain to a zero signal.
         :type max_gain_db: float
         :raises ValueError: If the sample rate does not match between the two
-                            audio segments when downsampling is not allowed, or
-                            if the duration of noise segments is shorter than
-                            original audio segments.
+                            audio segments, or if the duration of noise segments
+                            is shorter than original audio segments.
         """
-        if allow_downsampling and noise.sample_rate > self.sample_rate:
-            noise.resample(self.sample_rate)
         if noise.sample_rate != self.sample_rate:
             raise ValueError("噪声采样率(%d Hz)不等于基信号采样率(%d Hz)" % (noise.sample_rate, self.sample_rate))
         if noise.duration < self.duration:
             raise ValueError("噪声信号(%f秒)必须至少与基信号(%f秒)一样长" % (noise.duration, self.duration))
         noise_gain_db = min(self.rms_db - noise.rms_db - snr_dB, max_gain_db)
         noise_new = copy.deepcopy(noise)
         noise_new.random_subsegment(self.duration)
```

## ppvector/data_utils/augmentor/noise_perturb.py

```diff
@@ -36,14 +36,22 @@
         Note that this is an in-place transformation.
 
         :param audio_segment: Audio segment to add effects to.
         :type audio_segment: AudioSegmenet
         """
         if len(self.noises_path) > 0:
             for _ in range(random.randint(1, self.repetition)):
+                # 随机选择一个noises_path中的一个
                 noise_path = random.sample(self.noises_path, 1)[0]
+                # 读取噪声音频
                 noise_segment = AudioSegment.from_file(noise_path)
+                # 如果噪声采样率不等于audio_segment的采样率，则重采样
+                if noise_segment.sample_rate!= audio_segment.sample_rate:
+                    noise_segment.resample(audio_segment.sample_rate)
+                # 随机生成snr_dB的值
                 snr_dB = random.uniform(self._min_snr_dB, self._max_snr_dB)
+                # 如果噪声的长度小于audio_segment的长度，则将噪声的前面的部分填充噪声末尾补长
                 if noise_segment.duration < audio_segment.duration:
                     diff_duration = audio_segment.num_samples - noise_segment.num_samples
                     noise_segment._samples = np.pad(noise_segment.samples, (0, diff_duration), 'wrap')
-                audio_segment.add_noise(noise_segment, snr_dB, allow_downsampling=True)
+                # 将噪声添加到audio_segment中，并将snr_dB调整到最小值和最大值之间
+                audio_segment.add_noise(noise_segment, snr_dB)
```

## Comparing `ppvector-0.3.6.dist-info/LICENSE` & `ppvector-0.3.7.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `ppvector-0.3.6.dist-info/METADATA` & `ppvector-0.3.7.dist-info/METADATA`

 * *Files 14% similar despite different names*

```diff
@@ -1,453 +1,460 @@
-Metadata-Version: 2.1
-Name: ppvector
-Version: 0.3.6
-Summary: Voice Print Recognition toolkit on PaddlePaddle
-Home-page: https://github.com/yeyupiaoling/VoiceprintRecognition_PaddlePaddle
-Download-URL: https://github.com/yeyupiaoling/VoiceprintRecognition_PaddlePaddle.git
-Author: yeyupiaoling
-License: Apache License 2.0
-Keywords: Voice,paddle
-Classifier: Intended Audience :: Developers
-Classifier: License :: OSI Approved :: Apache Software License
-Classifier: Operating System :: OS Independent
-Classifier: Natural Language :: Chinese (Simplified)
-Classifier: Programming Language :: Python :: 3
-Classifier: Programming Language :: Python :: 3.5
-Classifier: Programming Language :: Python :: 3.6
-Classifier: Programming Language :: Python :: 3.7
-Classifier: Programming Language :: Python :: 3.8
-Classifier: Programming Language :: Python :: 3.9
-Classifier: Topic :: Utilities
-Description-Content-Type: text/markdown
-License-File: LICENSE
-Requires-Dist: numba (>=0.52.0)
-Requires-Dist: librosa (>=0.9.1)
-Requires-Dist: numpy (>=1.19.2)
-Requires-Dist: tqdm (>=4.59.0)
-Requires-Dist: visualdl (>=2.1.1)
-Requires-Dist: resampy (==0.2.2)
-Requires-Dist: soundfile (>=0.12.1)
-Requires-Dist: soundcard (>=0.4.2)
-Requires-Dist: pyyaml (>=5.4.1)
-Requires-Dist: termcolor (>=1.1.0)
-Requires-Dist: typeguard (==2.13.3)
-Requires-Dist: paddleaudio (>=1.0.1)
-Requires-Dist: scikit-learn (>=1.0.2)
-Requires-Dist: pydub (>=0.25.1)
-Requires-Dist: av (>=10.0.0)
-
-# 前言
-此版本为新版本，相比上一个版本，最大的变化是此版本支持pip安装，以及把预处理使用模型算子实现，这样做的好处就是可以直接使用GPU计算，大幅度提高了预处理的速度，估计预处理速度可在10~20倍。
-
-如想使用使用旧版本，请转到[release/1.0](https://github.com/yeyupiaoling/VoiceprintRecognition-PaddlePaddle/tree/release/1.0)，本项目使用了EcapaTdnn模型实现的声纹识别，不排除以后会支持更多模型，同时本项目也支持了多种数据预处理方法，损失函数参考了人脸识别项目的做法[PaddlePaddle-MobileFaceNets](https://github.com/yeyupiaoling/PaddlePaddle-MobileFaceNets) ,使用了ArcFace Loss，ArcFace loss：Additive Angular Margin Loss（加性角度间隔损失函数），对特征向量和权重归一化，对θ加上角度间隔m，角度间隔比余弦间隔在对角度的影响更加直接。
-
-
-**欢迎大家扫码入QQ群讨论**，或者直接搜索QQ群号`1169600237`，问题答案为博主Github的ID`yeyupiaoling`。
-
-<div align="center">
-  <img src="docs/images/qq.png"/>
-</div>
-
-
-使用环境：
-
- - Anaconda 3
- - Python 3.8
- - PaddlePaddle 2.4.1
- - Windows 10 or Ubuntu 18.04
-
-# 项目特性
-
-1. 支持模型：EcapaTdnn、TDNN、Res2Net、ResNetSE
-2. 支持池化层：AttentiveStatisticsPooling(ASP)、SelfAttentivePooling(SAP)、TemporalStatisticsPooling(TSP)、TemporalAveragePooling(TAP)
-3. 支持损失函数：AAMLoss、AMLoss、ARMLoss、CELoss
-4. 支持预处理方法：MelSpectrogram、LogMelSpectrogram、Spectrogram、MFCC
-
-
-# 模型下载
-
-<table align="center">
-<tr>
-  <th align="center">模型</th>
-  <th align="center">预处理方法</th>
-  <th align="center">数据集</th>
-  <th align="center">类别数量</th>
-  <th align="center">tpr</th>
-  <th align="center">fpr</th>
-  <th align="center">eer</th>
-  <th align="center">模型下载地址</th>
-</tr>
-<tr>
-  <td align="center">EcapaTdnn</td>
-  <td align="center">MelSpectrogram</td>
-  <td align="center"><a href="https://github.com/fighting41love/zhvoice">中文语音语料数据集</a></td>
-  <td align="center">3242</td>
-  <td align="center">0.98085</td>
-  <td align="center">0.01123</td>
-  <td align="center">0.03039</td>
-  <td align="center"><a href="https://download.csdn.net/download/qq_33200967/86951790">点击下载</a></td>
-</tr>
-<tr>
-  <td align="center">EcapaTdnn</td>
-  <td align="center">LogMelSpectrogram</td>
-  <td align="center"><a href="https://github.com/fighting41love/zhvoice">中文语音语料数据集</a></td>
-  <td align="center">3242</td>
-  <td align="center">0.98693</td>
-  <td align="center">0.00910</td>
-  <td align="center">0.02216</td>
-  <td align="center"><a href="https://download.csdn.net/download/qq_33200967/86987768">点击下载</a></td>
-</tr>
-<tr>
-  <td align="center">EcapaTdnn</td>
-  <td align="center">Spectrogram</td>
-  <td align="center"><a href="https://github.com/fighting41love/zhvoice">中文语音语料数据集</a></td>
-  <td align="center">3242</td>
-  <td align="center">0.98573</td>
-  <td align="center">0.01028</td>
-  <td align="center">0.02454</td>
-  <td align="center"><a href="https://download.csdn.net/download/qq_33200967/87527690">点击下载</a></td>
-</tr>
-<tr>
-  <td align="center">EcapaTdnn</td>
-  <td align="center">MFCC</td>
-  <td align="center"><a href="https://github.com/fighting41love/zhvoice">中文语音语料数据集</a></td>
-  <td align="center">3242</td>
-  <td align="center">0.98504</td>
-  <td align="center">0.00867</td>
-  <td align="center">0.02363</td>
-  <td align="center"><a href="https://download.csdn.net/download/qq_33200967/87527015">点击下载</a></td>
-</tr>
-<tr>
-  <td align="center">EcapaTdnn</td>
-  <td align="center">MelSpectrogram</td>
-  <td align="center">更大的数据集</td>
-  <td align="center">6355</td>
-  <td align="center">0.97910</td>
-  <td align="center">0.00778</td>
-  <td align="center">0.02868</td>
-  <td align="center"><a href="https://download.csdn.net/download/qq_33200967/87041912">点击下载</a></td>
-</tr>
-<tr>
-  <td align="center">EcapaTdnn</td>
-  <td align="center">MelSpectrogram</td>
-  <td align="center">超大的数据集</td>
-  <td align="center">13718</td>
-  <td align="center">0.98540</td>
-  <td align="center">0.00633</td>
-  <td align="center">0.02093</td>
-  <td align="center"><a href="https://download.csdn.net/download/qq_33200967/87106989">点击下载</a></td>
-</tr>
-</table>
-
-## 安装环境
-
- - 首先安装的是PaddlePaddle的GPU版本，如果已经安装过了，请跳过。
-```shell
-conda install paddlepaddle-gpu==2.4.1 cudatoolkit=10.2 --channel https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/Paddle/
-```
-
- - 安装ppvector库。
- 
-使用pip安装，命令如下：
-```shell
-python -m pip install ppvector -U -i https://pypi.tuna.tsinghua.edu.cn/simple
-```
-
-**建议源码安装**，源码安装能保证使用最新代码。
-```shell
-git clone https://github.com/yeyupiaoling/VoiceprintRecognition_PaddlePaddle.git
-cd VoiceprintRecognition_PaddlePaddle/
-python setup.py install
-```
-
-# 创建数据
-本教程笔者使用的是[中文语音语料数据集](https://github.com/fighting41love/zhvoice) ，这个数据集一共有3242个人的语音数据，有1130000+条语音数据，下载之前要**全部解压**数据集。如果读者有其他更好的数据集，可以混合在一起使用，但最好是要用python的工具模块aukit处理音频，降噪和去除静音。
-
-首先是创建一个数据列表，数据列表的格式为`<语音文件路径\t语音分类标签>`，创建这个列表主要是方便之后的读取，也是方便读取使用其他的语音数据集，语音分类标签是指说话人的唯一ID，不同的语音数据集，可以通过编写对应的生成数据列表的函数，把这些数据集都写在同一个数据列表中。
-
-在`create_data.py`写下以下代码，因为[中文语音语料数据集](https://github.com/fighting41love/zhvoice) 这个数据集是mp3格式的，作者发现这种格式读取速度很慢，所以笔者把全部的mp3格式的音频转换为wav格式，这个过程可能很久。当然也可以不转换，项目也是支持的MP3格式的，只要设置参数`to_wav=False`。执行下面程序完成数据准备。
-```shell
-python create_data.py
-```
-
-执行上面的程序之后，会生成以下的数据格式，如果要自定义数据，参考如下数据列表，前面是音频的相对路径，后面的是该音频对应的说话人的标签，就跟分类一样。**自定义数据集的注意**，测试数据列表的ID可以不用跟训练的ID一样，也就是说测试的数据的说话人可以不用出现在训练集，只要保证测试数据列表中同一个人相同的ID即可。
-```
-dataset/zhvoice/zhmagicdata/5_895/5_895_20170614203758.wav	3238
-dataset/zhvoice/zhmagicdata/5_895/5_895_20170614214007.wav	3238
-dataset/zhvoice/zhmagicdata/5_941/5_941_20170613151344.wav	3239
-dataset/zhvoice/zhmagicdata/5_941/5_941_20170614221329.wav	3239
-dataset/zhvoice/zhmagicdata/5_941/5_941_20170616153308.wav	3239
-dataset/zhvoice/zhmagicdata/5_968/5_968_20170614162657.wav	3240
-dataset/zhvoice/zhmagicdata/5_968/5_968_20170622194003.wav	3240
-dataset/zhvoice/zhmagicdata/5_968/5_968_20170707200554.wav	3240
-dataset/zhvoice/zhmagicdata/5_970/5_970_20170616000122.wav	3241
-```
-
-# 修改预处理方法
-
-配置文件中默认使用的是MelSpectrogram预处理方法，如果要使用其他预处理方法，可以修改配置文件中的安装下面方式修改，具体的值可以根据自己情况修改。
-
-1. `MelSpectrogram`预处理方法如下：
-
-```yaml
-preprocess_conf:
-   # 音频预处理方法，支持：LogMelSpectrogram、MelSpectrogram、Spectrogram、MFCC
-  feature_method: 'MelSpectrogram'
-
-# MelSpectrogram的参数，其他的预处理方法查看对应API设设置参数
-feature_conf:
-   sr: 16000
-   n_fft: 1024
-   hop_length: 320
-   window: 'hann'
-   win_length: 1024
-   f_min: 50.0
-   f_max: 14000.0
-   n_mels: 64
-```
-
-1. `pectrogram'`预处理方法如下：
-
-```yaml
-preprocess_conf:
-   # 音频预处理方法，支持：LogMelSpectrogram、MelSpectrogram、Spectrogram、MFCC
-  feature_method: 'Spectrogram'
-
-# Spectrogram的参数，其他的预处理方法查看对应API设设置参数
-feature_conf:
-   n_fft: 1024
-   hop_length: 320
-   window: 'hann'
-   win_length: 1024
-```
-
-3. `MFCC`预处理方法如下：
-
-```yaml
-preprocess_conf:
-   # 音频预处理方法，支持：LogMelSpectrogram、MelSpectrogram、Spectrogram、MFCC
-  feature_method: 'MFCC'
-
-# MFCC的参数，其他的预处理方法查看对应API设设置参数
-feature_conf:
-   sr: 16000
-   n_fft: 1024
-   hop_length: 320
-   window: 'hann'
-   win_length: 1024
-   f_min: 50.0
-   f_max: 14000.0
-   n_mels: 64
-   n_mfcc: 40
-```
-
-4. `MFCC`预处理方法如下：
-
-```yaml
-preprocess_conf:
-   # 音频预处理方法，支持：LogMelSpectrogram、MelSpectrogram、Spectrogram、MFCC
-  feature_method: 'LogMelSpectrogram'
-
-# LogMelSpectrogram的参数，其他的预处理方法查看对应API设设置参数
-feature_conf:
-   sr: 16000
-   n_fft: 1024
-   hop_length: 320
-   window: 'hann'
-   win_length: 1024
-   f_min: 50.0
-   f_max: 14000.0
-   n_mels: 64
-```
-
-# 训练模型
-使用`train.py`训练模型，本项目支持多个音频预处理方式，通过`configs/ecapa_tdnn.yml`配置文件的参数`preprocess_conf.feature_method`可以指定，`MelSpectrogram`为梅尔频谱，`Spectrogram`为语谱图，`MFCC`梅尔频谱倒谱系数。通过参数`augment_conf_path`可以指定数据增强方式。训练过程中，会使用VisualDL保存训练日志，通过启动VisualDL可以随时查看训练结果，启动命令`visualdl --logdir=log --host 0.0.0.0`
-```shell
-# 单卡训练
-CUDA_VISIBLE_DEVICES=0 python train.py
-# 多卡训练
-python -m paddle.distributed.launch --gpus '0,1' train.py
-```
-
-训练输出日志：
-```
-[2023-02-25 11:53:53.194706 INFO   ] utils:print_arguments:13 - ----------- 额外配置参数 -----------
-[2023-02-25 11:53:53.194706 INFO   ] utils:print_arguments:15 - augment_conf_path: configs/augmentation.json
-[2023-02-25 11:53:53.194706 INFO   ] utils:print_arguments:15 - configs: configs/ecapa_tdnn.yml
-[2023-02-25 11:53:53.194706 INFO   ] utils:print_arguments:15 - pretrained_model: None
-[2023-02-25 11:53:53.194706 INFO   ] utils:print_arguments:15 - resume_model: None
-[2023-02-25 11:53:53.194706 INFO   ] utils:print_arguments:15 - save_model_path: models/
-[2023-02-25 11:53:53.194706 INFO   ] utils:print_arguments:15 - use_gpu: True
-[2023-02-25 11:53:53.194706 INFO   ] utils:print_arguments:16 - ------------------------------------------------
-[2023-02-25 11:53:53.208669 INFO   ] utils:print_arguments:18 - ----------- 配置文件参数 -----------
-[2023-02-25 11:53:53.208669 INFO   ] utils:print_arguments:21 - dataset_conf:
-[2023-02-25 11:53:53.208669 INFO   ] utils:print_arguments:28 - 	batch_size: 64
-[2023-02-25 11:53:53.208669 INFO   ] utils:print_arguments:28 - 	chunk_duration: 3
-[2023-02-25 11:53:53.208669 INFO   ] utils:print_arguments:28 - 	do_vad: False
-[2023-02-25 11:53:53.209670 INFO   ] utils:print_arguments:28 - 	min_duration: 0.5
-[2023-02-25 11:53:53.209670 INFO   ] utils:print_arguments:28 - 	num_speakers: 3242
-[2023-02-25 11:53:53.209670 INFO   ] utils:print_arguments:28 - 	num_workers: 4
-[2023-02-25 11:53:53.209670 INFO   ] utils:print_arguments:28 - 	sample_rate: 16000
-[2023-02-25 11:53:53.209670 INFO   ] utils:print_arguments:28 - 	target_dB: -20
-[2023-02-25 11:53:53.209670 INFO   ] utils:print_arguments:28 - 	test_list: dataset/test_list.txt
-[2023-02-25 11:53:53.209670 INFO   ] utils:print_arguments:28 - 	train_list: dataset/train_list.txt
-[2023-02-25 11:53:53.209670 INFO   ] utils:print_arguments:28 - 	use_dB_normalization: True
-[2023-02-25 11:53:53.209670 INFO   ] utils:print_arguments:21 - feature_conf:
-[2023-02-25 11:53:53.209670 INFO   ] utils:print_arguments:28 - 	hop_length: 160
-[2023-02-25 11:53:53.209670 INFO   ] utils:print_arguments:28 - 	n_fft: 400
-[2023-02-25 11:53:53.209670 INFO   ] utils:print_arguments:28 - 	n_mels: 80
-[2023-02-25 11:53:53.209670 INFO   ] utils:print_arguments:28 - 	sr: 16000
-[2023-02-25 11:53:53.209670 INFO   ] utils:print_arguments:28 - 	win_length: 400
-[2023-02-25 11:53:53.209670 INFO   ] utils:print_arguments:28 - 	window: hann
-[2023-02-25 11:53:53.209670 INFO   ] utils:print_arguments:21 - model_conf:
-[2023-02-25 11:53:53.209670 INFO   ] utils:print_arguments:28 - 	channels: [512, 512, 512, 512, 1536]
-[2023-02-25 11:53:53.209670 INFO   ] utils:print_arguments:28 - 	dilations: [1, 2, 3, 4, 1]
-[2023-02-25 11:53:53.209670 INFO   ] utils:print_arguments:28 - 	kernel_sizes: [5, 3, 3, 3, 1]
-[2023-02-25 11:53:53.210667 INFO   ] utils:print_arguments:28 - 	lin_neurons: 192
-[2023-02-25 11:53:53.210667 INFO   ] utils:print_arguments:21 - optimizer_conf:
-[2023-02-25 11:53:53.210667 INFO   ] utils:print_arguments:28 - 	learning_rate: 0.001
-[2023-02-25 11:53:53.210667 INFO   ] utils:print_arguments:28 - 	weight_decay: 1e-6
-[2023-02-25 11:53:53.220680 INFO   ] utils:print_arguments:21 - preprocess_conf:
-[2023-02-25 11:53:53.220680 INFO   ] utils:print_arguments:28 - 	feature_method: MelSpectrogram
-[2023-02-25 11:53:53.220680 INFO   ] utils:print_arguments:21 - train_conf:
-[2023-02-25 11:53:53.220680 INFO   ] utils:print_arguments:28 - 	log_interval: 100
-[2023-02-25 11:53:53.220680 INFO   ] utils:print_arguments:28 - 	max_epoch: 30
-[2023-02-25 11:53:53.220680 INFO   ] utils:print_arguments:30 - use_model: ecapa_tdnn
-[2023-02-25 11:53:53.220680 INFO   ] utils:print_arguments:31 - ------------------------------------------------
-[2022-11-05 19:58:31.589525 INFO   ] augmentation:_parse_pipeline_from:126 - 数据增强配置：{'type': 'noise', 'aug_type': 'audio', 'params': {'min_snr_dB': 10, 'max_snr_dB': 50, 'repetition': 2, 'noise_dir': 'dataset/noise/'}, 'prob': 0.0}
-[2022-11-05 19:58:31.589525 INFO   ] augmentation:_parse_pipeline_from:126 - 数据增强配置：{'type': 'resample', 'aug_type': 'audio', 'params': {'new_sample_rate': [8000, 32000, 44100, 48000]}, 'prob': 0.0}
-[2022-11-05 19:58:31.589525 INFO   ] augmentation:_parse_pipeline_from:126 - 数据增强配置：{'type': 'speed', 'aug_type': 'audio', 'params': {'min_speed_rate': 0.9, 'max_speed_rate': 1.1, 'num_rates': 3}, 'prob': 0.0}
-[2022-11-05 19:58:31.589525 INFO   ] augmentation:_parse_pipeline_from:126 - 数据增强配置：{'type': 'shift', 'aug_type': 'audio', 'params': {'min_shift_ms': -5, 'max_shift_ms': 5}, 'prob': 0.0}
-[2022-11-05 19:58:31.590535 INFO   ] augmentation:_parse_pipeline_from:126 - 数据增强配置：{'type': 'volume', 'aug_type': 'audio', 'params': {'min_gain_dBFS': -15, 'max_gain_dBFS': 15}, 'prob': 0.0}
-[2022-11-05 19:58:31.590535 INFO   ] augmentation:_parse_pipeline_from:126 - 数据增强配置：{'type': 'specaug', 'aug_type': 'feature', 'params': {'inplace': True, 'max_time_warp': 5, 'max_t_ratio': 0.01, 'n_freq_masks': 2, 'max_f_ratio': 0.05, 'n_time_masks': 2, 'replace_with_zero': False}, 'prob': 0.0}
-[2022-11-05 19:58:31.590535 INFO   ] augmentation:_parse_pipeline_from:126 - 数据增强配置：{'type': 'specsub', 'aug_type': 'feature', 'params': {'max_t': 10, 'num_t_sub': 2}, 'prob': 0.0}
-I0424 08:57:03.707505  3377 nccl_context.cc:74] init nccl context nranks: 2 local rank: 0 gpu id: 0 ring id: 0
-W0424 08:57:03.930370  3377 device_context.cc:447] Please NOTE: device: 0, GPU Compute Capability: 7.5, Driver API Version: 11.6, Runtime API Version: 10.2
-W0424 08:57:03.932493  3377 device_context.cc:465] device: 0, cuDNN Version: 7.6.
-I0424 08:57:05.431638  3377 nccl_context.cc:107] init nccl context nranks: 2 local rank: 0 gpu id: 0 ring id: 10
-······
-[2023-03-16 20:30:42.559858 INFO   ] trainer:__train_epoch:232 - Train epoch: [1/30], batch: [0/16579], loss: 16.48008, accuracy: 0.01562, learning rate: 0.00000000, speed: 21.27 data/sec, eta: 17 days, 7:38:55
-[2023-03-16 20:31:15.045717 INFO   ] trainer:__train_epoch:232 - Train epoch: [1/30], batch: [100/16579], loss: 16.34529, accuracy: 0.00062, learning rate: 0.00000121, speed: 197.03 data/sec, eta: 1 day, 20:52:05
-[2023-03-16 20:31:47.086451 INFO   ] trainer:__train_epoch:232 - Train epoch: [1/30], batch: [200/16579], loss: 16.31631, accuracy: 0.00054, learning rate: 0.00000241, speed: 199.77 data/sec, eta: 1 day, 20:14:40
-[2023-03-16 20:32:19.711337 INFO   ] trainer:__train_epoch:232 - Train epoch: [1/30], batch: [300/16579], loss: 16.30544, accuracy: 0.00047, learning rate: 0.00000362, speed: 196.19 data/sec, eta: 1 day, 21:02:28
-[2023-03-16 20:32:52.853642 INFO   ] trainer:__train_epoch:232 - Train epoch: [1/30], batch: [400/16579], loss: 16.29228, accuracy: 0.00043, learning rate: 0.00000483, speed: 193.14 data/sec, eta: 1 day, 21:44:42
-[2023-03-16 20:33:25.116274 INFO   ] trainer:__train_epoch:232 - Train epoch: [1/30], batch: [500/16579], loss: 16.27346, accuracy: 0.00041, learning rate: 0.00000603, speed: 198.40 data/sec, eta: 1 day, 20:31:18
-······
-[2023-03-16 20:34:09.633572 INFO   ] trainer:train:304 - ======================================================================
-100%|███████████████████████████████████| 84/84 [00:10<00:00,  7.79it/s]
-开始两两对比音频特征...
-100%|██████████████████████████████| 5332/5332 [00:07<00:00, 749.89it/s]
-[2023-03-16 20:34:29.328638 INFO   ] trainer:train:306 - Test epoch: 1, time/epoch: 0:00:48.881889, threshold: 0.72, tpr: 0.62350, fpr: 0.04601, eer: 0.42250
-[2023-03-16 20:34:29.328840 INFO   ] trainer:train:309 - ======================================================================
-[2023-03-16 20:34:29.728986 INFO   ] trainer:__save_checkpoint:203 - 已保存模型：models/ecapa_tdnn_MelSpectrogram/best_model
-[2023-03-16 20:34:30.724868 INFO   ] trainer:__save_checkpoint:203 - 已保存模型：models/ecapa_tdnn_MelSpectrogram/epoch_1
-[2023-03-16 20:30:42.559858 INFO   ] trainer:__train_epoch:232 - Train epoch: [2/30], batch: [0/16579], loss: 16.48008, accuracy: 0.01562, learning rate: 0.00000000, speed: 21.27 data/sec, eta: 17 days, 7:38:55
-[2023-03-16 20:31:15.045717 INFO   ] trainer:__train_epoch:232 - Train epoch: [2/30], batch: [100/16579], loss: 16.34529, accuracy: 0.00062, learning rate: 0.00000121, speed: 197.03 data/sec, eta: 1 day, 20:52:05
-[2023-03-16 20:31:47.086451 INFO   ] trainer:__train_epoch:232 - Train epoch: [2/30], batch: [200/16579], loss: 16.31631, accuracy: 0.00054, learning rate: 0.00000241, speed: 199.77 data/sec, eta: 1 day, 20:14:40
-[2023-03-16 20:32:19.711337 INFO   ] trainer:__train_epoch:232 - Train epoch: [2/30], batch: [300/16579], loss: 16.30544, accuracy: 0.00047, learning rate: 0.00000362, speed: 196.19 data/sec, eta: 1 day, 21:02:28
-······
-```
-
-VisualDL页面：
-![VisualDL页面](./docs/images/log.jpg)
-
-
-# 数据增强
-本项目提供了几种音频增强操作，分布是随机裁剪，添加背景噪声，调节语速，调节音量，和SpecAugment。其中后面4种增加的参数可以在`configs/augmentation.json`修改，参数`prob`是指定该增强操作的概率，如果不想使用该增强方式，可以设置为0。要主要的是，添加背景噪声需要把多个噪声音频文件存放在`dataset/noise`，否则会跳过噪声增强
-```yaml
-noise:
-  min_snr_dB: 10
-  max_snr_dB: 30
-  noise_path: "dataset/noise"
-  prob: 0.5
-```
-
-
-
-# 评估模型
-训练结束之后会保存预测模型，我们用预测模型来预测测试集中的音频特征，然后使用音频特征进行两两对比，计算tpr、fpr、eer。
-```shell
-python eval.py
-```
-
-输出类似如下：
-```
-······
-------------------------------------------------
-W0425 08:27:32.057426 17654 device_context.cc:447] Please NOTE: device: 0, GPU Compute Capability: 7.5, Driver API Version: 11.6, Runtime API Version: 10.2
-W0425 08:27:32.065165 17654 device_context.cc:465] device: 0, cuDNN Version: 7.6.
-[2023-03-16 20:20:47.195908 INFO   ] trainer:evaluate:341 - 成功加载模型：models/ecapa_tdnn_MelSpectrogram/best_model/model.pdparams
-100%|███████████████████████████| 84/84 [00:28<00:00,  2.95it/s]
-开始两两对比音频特征...
-100%|███████████████████████████| 5332/5332 [00:05<00:00, 1027.83it/s]
-评估消耗时间：65s，threshold：0.26，tpr：0.99391, fpr: 0.00611, eer: 0.01220
-```
-
-# 声纹对比
-下面开始实现声纹对比，创建`infer_contrast.py`程序，编写`infer()`函数，在编写模型的时候，模型是有两个输出的，第一个是模型的分类输出，第二个是音频特征输出。所以在这里要输出的是音频的特征值，有了音频的特征值就可以做声纹识别了。我们输入两个语音，通过预测函数获取他们的特征数据，使用这个特征数据可以求他们的对角余弦值，得到的结果可以作为他们相识度。对于这个相识度的阈值`threshold`，读者可以根据自己项目的准确度要求进行修改。
-```shell
-python infer_contrast.py --audio_path1=audio/a_1.wav --audio_path2=audio/b_2.wav
-```
-
-输出类似如下：
-```
-[2023-04-02 18:30:48.009149 INFO   ] utils:print_arguments:13 - ----------- 额外配置参数 -----------
-[2023-04-02 18:30:48.009149 INFO   ] utils:print_arguments:15 - audio_path1: dataset/a_1.wav
-[2023-04-02 18:30:48.009149 INFO   ] utils:print_arguments:15 - audio_path2: dataset/b_2.wav
-[2023-04-02 18:30:48.009149 INFO   ] utils:print_arguments:15 - configs: configs/ecapa_tdnn.yml
-[2023-04-02 18:30:48.009149 INFO   ] utils:print_arguments:15 - model_path: models/ecapa_tdnn_MelSpectrogram/best_model/
-[2023-04-02 18:30:48.009149 INFO   ] utils:print_arguments:15 - threshold: 0.6
-[2023-04-02 18:30:48.009149 INFO   ] utils:print_arguments:15 - use_gpu: True
-[2023-04-02 18:30:48.009149 INFO   ] utils:print_arguments:16 - ------------------------------------------------
-······································································
-W0425 08:29:10.006249 21121 device_context.cc:447] Please NOTE: device: 0, GPU Compute Capability: 7.5, Driver API Version: 11.6, Runtime API Version: 10.2
-W0425 08:29:10.008555 21121 device_context.cc:465] device: 0, cuDNN Version: 7.6.
-成功加载模型参数和优化方法参数：models/ecapa_tdnn/model.pdparams
-audio/a_1.wav 和 audio/b_2.wav 不是同一个人，相似度为：-0.09565544128417969
-```
-
-# 声纹识别
-在上面的声纹对比的基础上，我们创建`infer_recognition.py`实现声纹识别。同样是使用上面声纹对比的`infer()`预测函数，通过这两个同样获取语音的特征数据。 不同的是笔者增加了`load_audio_db()`和`register()`，以及`recognition()`，第一个函数是加载声纹库中的语音数据，这些音频就是相当于已经注册的用户，他们注册的语音数据会存放在这里，如果有用户需要通过声纹登录，就需要拿到用户的语音和语音库中的语音进行声纹对比，如果对比成功，那就相当于登录成功并且获取用户注册时的信息数据。第二个函数`register()`其实就是把录音保存在声纹库中，同时获取该音频的特征添加到待对比的数据特征中。最后`recognition()`函数中，这个函数就是将输入的语音和语音库中的语音一一对比。
-有了上面的声纹识别的函数，读者可以根据自己项目的需求完成声纹识别的方式，例如笔者下面提供的是通过录音来完成声纹识别。首先必须要加载语音库中的语音，语音库文件夹为`audio_db`，然后用户回车后录音3秒钟，然后程序会自动录音，并使用录音到的音频进行声纹识别，去匹配语音库中的语音，获取用户的信息。通过这样方式，读者也可以修改成通过服务请求的方式完成声纹识别，例如提供一个API供APP调用，用户在APP上通过声纹登录时，把录音到的语音发送到后端完成声纹识别，再把结果返回给APP，前提是用户已经使用语音注册，并成功把语音数据存放在`audio_db`文件夹中。
-```shell
-python infer_recognition.py
-```
-
-输出类似如下：
-```
-[2023-04-02 18:31:20.521040 INFO   ] utils:print_arguments:13 - ----------- 额外配置参数 -----------
-[2023-04-02 18:31:20.521040 INFO   ] utils:print_arguments:15 - audio_db_path: audio_db/
-[2023-04-02 18:31:20.521040 INFO   ] utils:print_arguments:15 - configs: configs/ecapa_tdnn.yml
-[2023-04-02 18:31:20.521040 INFO   ] utils:print_arguments:15 - model_path: models/ecapa_tdnn_MelSpectrogram/best_model/
-[2023-04-02 18:31:20.521040 INFO   ] utils:print_arguments:15 - record_seconds: 3
-[2023-04-02 18:31:20.521040 INFO   ] utils:print_arguments:15 - threshold: 0.6
-[2023-04-02 18:31:20.521040 INFO   ] utils:print_arguments:15 - use_gpu: True
-[2023-04-02 18:31:20.521040 INFO   ] utils:print_arguments:16 - ------------------------------------------------
-······································································
-W0425 08:30:13.257884 23889 device_context.cc:447] Please NOTE: device: 0, GPU Compute Capability: 7.5, Driver API Version: 11.6, Runtime API Version: 10.2
-W0425 08:30:13.260191 23889 device_context.cc:465] device: 0, cuDNN Version: 7.6.
-成功加载模型参数和优化方法参数：models/ecapa_tdnn/model.pdparams
-Loaded 沙瑞金 audio.
-Loaded 李达康 audio.
-请选择功能，0为注册音频到声纹库，1为执行声纹识别：0
-按下回车键开机录音，录音3秒中：
-开始录音......
-录音已结束!
-请输入该音频用户的名称：夜雨飘零
-请选择功能，0为注册音频到声纹库，1为执行声纹识别：1
-按下回车键开机录音，录音3秒中：
-开始录音......
-录音已结束!
-识别说话的为：夜雨飘零，相似度为：0.920434
-```
-
-# 其他版本
- - Tensorflow：[VoiceprintRecognition-Tensorflow](https://github.com/yeyupiaoling/VoiceprintRecognition-Tensorflow)
- - Pytorch：[VoiceprintRecognition-Pytorch](https://github.com/yeyupiaoling/VoiceprintRecognition-Pytorch)
- - Keras：[VoiceprintRecognition-Keras](https://github.com/yeyupiaoling/VoiceprintRecognition-Keras)
-
-
-# 参考资料
-1. https://github.com/PaddlePaddle/PaddleSpeech
-2. https://github.com/yeyupiaoling/PaddlePaddle-MobileFaceNets
-3. https://github.com/yeyupiaoling/PPASR
+Metadata-Version: 2.1
+Name: ppvector
+Version: 0.3.7
+Summary: Voice Print Recognition toolkit on PaddlePaddle
+Home-page: https://github.com/yeyupiaoling/VoiceprintRecognition_PaddlePaddle
+Download-URL: https://github.com/yeyupiaoling/VoiceprintRecognition_PaddlePaddle.git
+Author: yeyupiaoling
+License: Apache License 2.0
+Keywords: Voice,paddle
+Classifier: Intended Audience :: Developers
+Classifier: License :: OSI Approved :: Apache Software License
+Classifier: Operating System :: OS Independent
+Classifier: Natural Language :: Chinese (Simplified)
+Classifier: Programming Language :: Python :: 3
+Classifier: Programming Language :: Python :: 3.5
+Classifier: Programming Language :: Python :: 3.6
+Classifier: Programming Language :: Python :: 3.7
+Classifier: Programming Language :: Python :: 3.8
+Classifier: Programming Language :: Python :: 3.9
+Classifier: Topic :: Utilities
+Description-Content-Type: text/markdown
+License-File: LICENSE
+Requires-Dist: numba (>=0.52.0)
+Requires-Dist: librosa (>=0.9.1)
+Requires-Dist: numpy (>=1.19.2)
+Requires-Dist: tqdm (>=4.59.0)
+Requires-Dist: visualdl (>=2.1.1)
+Requires-Dist: resampy (==0.2.2)
+Requires-Dist: soundfile (>=0.12.1)
+Requires-Dist: soundcard (>=0.4.2)
+Requires-Dist: pyyaml (>=5.4.1)
+Requires-Dist: termcolor (>=1.1.0)
+Requires-Dist: typeguard (==2.13.3)
+Requires-Dist: paddleaudio (>=1.0.1)
+Requires-Dist: scikit-learn (>=1.0.2)
+Requires-Dist: pydub (>=0.25.1)
+Requires-Dist: av (>=10.0.0)
+
+# 前言
+此版本为新版本，相比上一个版本，最大的变化是此版本支持pip安装，以及把预处理使用模型算子实现，这样做的好处就是可以直接使用GPU计算，大幅度提高了预处理的速度，估计预处理速度可在10~20倍。
+
+如想使用使用旧版本，请转到[release/1.0](https://github.com/yeyupiaoling/VoiceprintRecognition-PaddlePaddle/tree/release/1.0)，本项目使用了EcapaTdnn模型实现的声纹识别，不排除以后会支持更多模型，同时本项目也支持了多种数据预处理方法，损失函数参考了人脸识别项目的做法[PaddlePaddle-MobileFaceNets](https://github.com/yeyupiaoling/PaddlePaddle-MobileFaceNets) ,使用了ArcFace Loss，ArcFace loss：Additive Angular Margin Loss（加性角度间隔损失函数），对特征向量和权重归一化，对θ加上角度间隔m，角度间隔比余弦间隔在对角度的影响更加直接。
+
+
+**欢迎大家扫码入QQ群讨论**，或者直接搜索QQ群号`758170167`，问题答案为博主Github的ID`yeyupiaoling`。
+
+<div align="center">
+  <img src="docs/images/qq.jpg"/>
+</div>
+
+
+使用环境：
+
+ - Anaconda 3
+ - Python 3.8
+ - PaddlePaddle 2.4.1
+ - Windows 10 or Ubuntu 18.04
+
+# 项目特性
+
+1. 支持模型：EcapaTdnn、TDNN、Res2Net、ResNetSE
+2. 支持池化层：AttentiveStatisticsPooling(ASP)、SelfAttentivePooling(SAP)、TemporalStatisticsPooling(TSP)、TemporalAveragePooling(TAP)
+3. 支持损失函数：AAMLoss、AMLoss、ARMLoss、CELoss
+4. 支持预处理方法：MelSpectrogram、LogMelSpectrogram、Spectrogram、MFCC
+
+
+# 模型下载
+
+<table align="center">
+<tr>
+  <th align="center">模型</th>
+  <th align="center">预处理方法</th>
+  <th align="center">数据集</th>
+  <th align="center">类别数量</th>
+  <th align="center">tpr</th>
+  <th align="center">fpr</th>
+  <th align="center">eer</th>
+  <th align="center">模型下载地址</th>
+</tr>
+<tr>
+  <td align="center">EcapaTdnn</td>
+  <td align="center">MelSpectrogram</td>
+  <td align="center"><a href="https://aistudio.baidu.com/aistudio/datasetdetail/133922">zhvoice</a></td>
+  <td align="center">3242</td>
+  <td align="center">0.98085</td>
+  <td align="center">0.01123</td>
+  <td align="center">0.03039</td>
+  <td align="center"><a href="https://download.csdn.net/download/qq_33200967/86951790">点击下载</a></td>
+</tr>
+<tr>
+  <td align="center">EcapaTdnn</td>
+  <td align="center">LogMelSpectrogram</td>
+  <td align="center"><a href="https://aistudio.baidu.com/aistudio/datasetdetail/133922">zhvoice</a></td>
+  <td align="center">3242</td>
+  <td align="center">0.98693</td>
+  <td align="center">0.00910</td>
+  <td align="center">0.02216</td>
+  <td align="center"><a href="https://download.csdn.net/download/qq_33200967/86987768">点击下载</a></td>
+</tr>
+<tr>
+  <td align="center">EcapaTdnn</td>
+  <td align="center">Spectrogram</td>
+  <td align="center"><a href="https://aistudio.baidu.com/aistudio/datasetdetail/133922">zhvoice</a></td>
+  <td align="center">3242</td>
+  <td align="center">0.98573</td>
+  <td align="center">0.01028</td>
+  <td align="center">0.02454</td>
+  <td align="center"><a href="https://download.csdn.net/download/qq_33200967/87527690">点击下载</a></td>
+</tr>
+<tr>
+  <td align="center">EcapaTdnn</td>
+  <td align="center">MFCC</td>
+  <td align="center"><a href="https://aistudio.baidu.com/aistudio/datasetdetail/133922">zhvoice</a></td>
+  <td align="center">3242</td>
+  <td align="center">0.98504</td>
+  <td align="center">0.00867</td>
+  <td align="center">0.02363</td>
+  <td align="center"><a href="https://download.csdn.net/download/qq_33200967/87527015">点击下载</a></td>
+</tr>
+<tr>
+  <td align="center">EcapaTdnn</td>
+  <td align="center">MelSpectrogram</td>
+  <td align="center">更大的数据集</td>
+  <td align="center">6355</td>
+  <td align="center">0.97910</td>
+  <td align="center">0.00778</td>
+  <td align="center">0.02868</td>
+  <td align="center"><a href="https://download.csdn.net/download/qq_33200967/87041912">点击下载</a></td>
+</tr>
+<tr>
+  <td align="center">EcapaTdnn</td>
+  <td align="center">MelSpectrogram</td>
+  <td align="center">超大的数据集</td>
+  <td align="center">13718</td>
+  <td align="center">0.98540</td>
+  <td align="center">0.00633</td>
+  <td align="center">0.02093</td>
+  <td align="center"><a href="https://download.csdn.net/download/qq_33200967/87106989">点击下载</a></td>
+</tr>
+</table>
+
+## 安装环境
+
+ - 首先安装的是PaddlePaddle的GPU版本，如果已经安装过了，请跳过。
+```shell
+conda install paddlepaddle-gpu==2.4.1 cudatoolkit=10.2 --channel https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/Paddle/
+```
+
+ - 安装ppvector库。
+ 
+使用pip安装，命令如下：
+```shell
+python -m pip install ppvector -U -i https://pypi.tuna.tsinghua.edu.cn/simple
+```
+
+**建议源码安装**，源码安装能保证使用最新代码。
+```shell
+git clone https://github.com/yeyupiaoling/VoiceprintRecognition_PaddlePaddle.git
+cd VoiceprintRecognition_PaddlePaddle/
+python setup.py install
+```
+
+# 创建数据
+本教程笔者使用的是[zhvoice](https://aistudio.baidu.com/aistudio/datasetdetail/133922) ，这个数据集一共有3242个人的语音数据，有1130000+条语音数据，下载之前要**全部解压**数据集。如果读者有其他更好的数据集，可以混合在一起使用，但最好是要用python的工具模块aukit处理音频，降噪和去除静音。
+
+首先是创建一个数据列表，数据列表的格式为`<语音文件路径\t语音分类标签>`，创建这个列表主要是方便之后的读取，也是方便读取使用其他的语音数据集，语音分类标签是指说话人的唯一ID，不同的语音数据集，可以通过编写对应的生成数据列表的函数，把这些数据集都写在同一个数据列表中。
+
+在`create_data.py`写下以下代码，因为[zhvoice](https://aistudio.baidu.com/aistudio/datasetdetail/133922) 这个数据集是mp3格式的，作者发现这种格式读取速度很慢，所以笔者把全部的mp3格式的音频转换为wav格式，这个过程可能很久。当然也可以不转换，项目也是支持的MP3格式的，只要设置参数`to_wav=False`。执行下面程序完成数据准备。
+```shell
+python create_data.py
+```
+
+执行上面的程序之后，会生成以下的数据格式，如果要自定义数据，参考如下数据列表，前面是音频的相对路径，后面的是该音频对应的说话人的标签，就跟分类一样。**自定义数据集的注意**，测试数据列表的ID可以不用跟训练的ID一样，也就是说测试的数据的说话人可以不用出现在训练集，只要保证测试数据列表中同一个人相同的ID即可。
+```
+dataset/zhvoice/zhmagicdata/5_895/5_895_20170614203758.wav	3238
+dataset/zhvoice/zhmagicdata/5_895/5_895_20170614214007.wav	3238
+dataset/zhvoice/zhmagicdata/5_941/5_941_20170613151344.wav	3239
+dataset/zhvoice/zhmagicdata/5_941/5_941_20170614221329.wav	3239
+dataset/zhvoice/zhmagicdata/5_941/5_941_20170616153308.wav	3239
+dataset/zhvoice/zhmagicdata/5_968/5_968_20170614162657.wav	3240
+dataset/zhvoice/zhmagicdata/5_968/5_968_20170622194003.wav	3240
+dataset/zhvoice/zhmagicdata/5_968/5_968_20170707200554.wav	3240
+dataset/zhvoice/zhmagicdata/5_970/5_970_20170616000122.wav	3241
+```
+
+# 修改预处理方法
+
+配置文件中默认使用的是MelSpectrogram预处理方法，如果要使用其他预处理方法，可以修改配置文件中的安装下面方式修改，具体的值可以根据自己情况修改。
+
+1. `MelSpectrogram`预处理方法如下：
+
+```yaml
+preprocess_conf:
+   # 音频预处理方法，支持：LogMelSpectrogram、MelSpectrogram、Spectrogram、MFCC
+  feature_method: 'MelSpectrogram'
+
+# MelSpectrogram的参数，其他的预处理方法查看对应API设设置参数
+feature_conf:
+   sr: 16000
+   n_fft: 1024
+   hop_length: 320
+   window: 'hann'
+   win_length: 1024
+   f_min: 50.0
+   f_max: 14000.0
+   n_mels: 64
+```
+
+1. `pectrogram'`预处理方法如下：
+
+```yaml
+preprocess_conf:
+   # 音频预处理方法，支持：LogMelSpectrogram、MelSpectrogram、Spectrogram、MFCC
+  feature_method: 'Spectrogram'
+
+# Spectrogram的参数，其他的预处理方法查看对应API设设置参数
+feature_conf:
+   n_fft: 1024
+   hop_length: 320
+   window: 'hann'
+   win_length: 1024
+```
+
+3. `MFCC`预处理方法如下：
+
+```yaml
+preprocess_conf:
+   # 音频预处理方法，支持：LogMelSpectrogram、MelSpectrogram、Spectrogram、MFCC
+  feature_method: 'MFCC'
+
+# MFCC的参数，其他的预处理方法查看对应API设设置参数
+feature_conf:
+   sr: 16000
+   n_fft: 1024
+   hop_length: 320
+   window: 'hann'
+   win_length: 1024
+   f_min: 50.0
+   f_max: 14000.0
+   n_mels: 64
+   n_mfcc: 40
+```
+
+4. `MFCC`预处理方法如下：
+
+```yaml
+preprocess_conf:
+   # 音频预处理方法，支持：LogMelSpectrogram、MelSpectrogram、Spectrogram、MFCC
+  feature_method: 'LogMelSpectrogram'
+
+# LogMelSpectrogram的参数，其他的预处理方法查看对应API设设置参数
+feature_conf:
+   sr: 16000
+   n_fft: 1024
+   hop_length: 320
+   window: 'hann'
+   win_length: 1024
+   f_min: 50.0
+   f_max: 14000.0
+   n_mels: 64
+```
+
+# 训练模型
+使用`train.py`训练模型，本项目支持多个音频预处理方式，通过`configs/ecapa_tdnn.yml`配置文件的参数`preprocess_conf.feature_method`可以指定，`MelSpectrogram`为梅尔频谱，`Spectrogram`为语谱图，`MFCC`梅尔频谱倒谱系数。通过参数`augment_conf_path`可以指定数据增强方式。训练过程中，会使用VisualDL保存训练日志，通过启动VisualDL可以随时查看训练结果，启动命令`visualdl --logdir=log --host 0.0.0.0`
+```shell
+# 单卡训练
+CUDA_VISIBLE_DEVICES=0 python train.py
+# 多卡训练
+python -m paddle.distributed.launch --gpus '0,1' train.py
+```
+
+训练输出日志：
+```
+[2023-02-25 11:53:53.194706 INFO   ] utils:print_arguments:13 - ----------- 额外配置参数 -----------
+[2023-02-25 11:53:53.194706 INFO   ] utils:print_arguments:15 - augment_conf_path: configs/augmentation.json
+[2023-02-25 11:53:53.194706 INFO   ] utils:print_arguments:15 - configs: configs/ecapa_tdnn.yml
+[2023-02-25 11:53:53.194706 INFO   ] utils:print_arguments:15 - pretrained_model: None
+[2023-02-25 11:53:53.194706 INFO   ] utils:print_arguments:15 - resume_model: None
+[2023-02-25 11:53:53.194706 INFO   ] utils:print_arguments:15 - save_model_path: models/
+[2023-02-25 11:53:53.194706 INFO   ] utils:print_arguments:15 - use_gpu: True
+[2023-02-25 11:53:53.194706 INFO   ] utils:print_arguments:16 - ------------------------------------------------
+[2023-02-25 11:53:53.208669 INFO   ] utils:print_arguments:18 - ----------- 配置文件参数 -----------
+[2023-02-25 11:53:53.208669 INFO   ] utils:print_arguments:21 - dataset_conf:
+[2023-02-25 11:53:53.208669 INFO   ] utils:print_arguments:28 - 	batch_size: 64
+[2023-02-25 11:53:53.208669 INFO   ] utils:print_arguments:28 - 	chunk_duration: 3
+[2023-02-25 11:53:53.208669 INFO   ] utils:print_arguments:28 - 	do_vad: False
+[2023-02-25 11:53:53.209670 INFO   ] utils:print_arguments:28 - 	min_duration: 0.5
+[2023-02-25 11:53:53.209670 INFO   ] utils:print_arguments:28 - 	num_speakers: 3242
+[2023-02-25 11:53:53.209670 INFO   ] utils:print_arguments:28 - 	num_workers: 4
+[2023-02-25 11:53:53.209670 INFO   ] utils:print_arguments:28 - 	sample_rate: 16000
+[2023-02-25 11:53:53.209670 INFO   ] utils:print_arguments:28 - 	target_dB: -20
+[2023-02-25 11:53:53.209670 INFO   ] utils:print_arguments:28 - 	test_list: dataset/test_list.txt
+[2023-02-25 11:53:53.209670 INFO   ] utils:print_arguments:28 - 	train_list: dataset/train_list.txt
+[2023-02-25 11:53:53.209670 INFO   ] utils:print_arguments:28 - 	use_dB_normalization: True
+[2023-02-25 11:53:53.209670 INFO   ] utils:print_arguments:21 - feature_conf:
+[2023-02-25 11:53:53.209670 INFO   ] utils:print_arguments:28 - 	hop_length: 160
+[2023-02-25 11:53:53.209670 INFO   ] utils:print_arguments:28 - 	n_fft: 400
+[2023-02-25 11:53:53.209670 INFO   ] utils:print_arguments:28 - 	n_mels: 80
+[2023-02-25 11:53:53.209670 INFO   ] utils:print_arguments:28 - 	sr: 16000
+[2023-02-25 11:53:53.209670 INFO   ] utils:print_arguments:28 - 	win_length: 400
+[2023-02-25 11:53:53.209670 INFO   ] utils:print_arguments:28 - 	window: hann
+[2023-02-25 11:53:53.209670 INFO   ] utils:print_arguments:21 - model_conf:
+[2023-02-25 11:53:53.209670 INFO   ] utils:print_arguments:28 - 	channels: [512, 512, 512, 512, 1536]
+[2023-02-25 11:53:53.209670 INFO   ] utils:print_arguments:28 - 	dilations: [1, 2, 3, 4, 1]
+[2023-02-25 11:53:53.209670 INFO   ] utils:print_arguments:28 - 	kernel_sizes: [5, 3, 3, 3, 1]
+[2023-02-25 11:53:53.210667 INFO   ] utils:print_arguments:28 - 	lin_neurons: 192
+[2023-02-25 11:53:53.210667 INFO   ] utils:print_arguments:21 - optimizer_conf:
+[2023-02-25 11:53:53.210667 INFO   ] utils:print_arguments:28 - 	learning_rate: 0.001
+[2023-02-25 11:53:53.210667 INFO   ] utils:print_arguments:28 - 	weight_decay: 1e-6
+[2023-02-25 11:53:53.220680 INFO   ] utils:print_arguments:21 - preprocess_conf:
+[2023-02-25 11:53:53.220680 INFO   ] utils:print_arguments:28 - 	feature_method: MelSpectrogram
+[2023-02-25 11:53:53.220680 INFO   ] utils:print_arguments:21 - train_conf:
+[2023-02-25 11:53:53.220680 INFO   ] utils:print_arguments:28 - 	log_interval: 100
+[2023-02-25 11:53:53.220680 INFO   ] utils:print_arguments:28 - 	max_epoch: 30
+[2023-02-25 11:53:53.220680 INFO   ] utils:print_arguments:30 - use_model: ecapa_tdnn
+[2023-02-25 11:53:53.220680 INFO   ] utils:print_arguments:31 - ------------------------------------------------
+[2022-11-05 19:58:31.589525 INFO   ] augmentation:_parse_pipeline_from:126 - 数据增强配置：{'type': 'noise', 'aug_type': 'audio', 'params': {'min_snr_dB': 10, 'max_snr_dB': 50, 'repetition': 2, 'noise_dir': 'dataset/noise/'}, 'prob': 0.0}
+[2022-11-05 19:58:31.589525 INFO   ] augmentation:_parse_pipeline_from:126 - 数据增强配置：{'type': 'resample', 'aug_type': 'audio', 'params': {'new_sample_rate': [8000, 32000, 44100, 48000]}, 'prob': 0.0}
+[2022-11-05 19:58:31.589525 INFO   ] augmentation:_parse_pipeline_from:126 - 数据增强配置：{'type': 'speed', 'aug_type': 'audio', 'params': {'min_speed_rate': 0.9, 'max_speed_rate': 1.1, 'num_rates': 3}, 'prob': 0.0}
+[2022-11-05 19:58:31.589525 INFO   ] augmentation:_parse_pipeline_from:126 - 数据增强配置：{'type': 'shift', 'aug_type': 'audio', 'params': {'min_shift_ms': -5, 'max_shift_ms': 5}, 'prob': 0.0}
+[2022-11-05 19:58:31.590535 INFO   ] augmentation:_parse_pipeline_from:126 - 数据增强配置：{'type': 'volume', 'aug_type': 'audio', 'params': {'min_gain_dBFS': -15, 'max_gain_dBFS': 15}, 'prob': 0.0}
+[2022-11-05 19:58:31.590535 INFO   ] augmentation:_parse_pipeline_from:126 - 数据增强配置：{'type': 'specaug', 'aug_type': 'feature', 'params': {'inplace': True, 'max_time_warp': 5, 'max_t_ratio': 0.01, 'n_freq_masks': 2, 'max_f_ratio': 0.05, 'n_time_masks': 2, 'replace_with_zero': False}, 'prob': 0.0}
+[2022-11-05 19:58:31.590535 INFO   ] augmentation:_parse_pipeline_from:126 - 数据增强配置：{'type': 'specsub', 'aug_type': 'feature', 'params': {'max_t': 10, 'num_t_sub': 2}, 'prob': 0.0}
+I0424 08:57:03.707505  3377 nccl_context.cc:74] init nccl context nranks: 2 local rank: 0 gpu id: 0 ring id: 0
+W0424 08:57:03.930370  3377 device_context.cc:447] Please NOTE: device: 0, GPU Compute Capability: 7.5, Driver API Version: 11.6, Runtime API Version: 10.2
+W0424 08:57:03.932493  3377 device_context.cc:465] device: 0, cuDNN Version: 7.6.
+I0424 08:57:05.431638  3377 nccl_context.cc:107] init nccl context nranks: 2 local rank: 0 gpu id: 0 ring id: 10
+······
+[2023-03-16 20:30:42.559858 INFO   ] trainer:__train_epoch:232 - Train epoch: [1/30], batch: [0/16579], loss: 16.48008, accuracy: 0.01562, learning rate: 0.00000000, speed: 21.27 data/sec, eta: 17 days, 7:38:55
+[2023-03-16 20:31:15.045717 INFO   ] trainer:__train_epoch:232 - Train epoch: [1/30], batch: [100/16579], loss: 16.34529, accuracy: 0.00062, learning rate: 0.00000121, speed: 197.03 data/sec, eta: 1 day, 20:52:05
+[2023-03-16 20:31:47.086451 INFO   ] trainer:__train_epoch:232 - Train epoch: [1/30], batch: [200/16579], loss: 16.31631, accuracy: 0.00054, learning rate: 0.00000241, speed: 199.77 data/sec, eta: 1 day, 20:14:40
+[2023-03-16 20:32:19.711337 INFO   ] trainer:__train_epoch:232 - Train epoch: [1/30], batch: [300/16579], loss: 16.30544, accuracy: 0.00047, learning rate: 0.00000362, speed: 196.19 data/sec, eta: 1 day, 21:02:28
+[2023-03-16 20:32:52.853642 INFO   ] trainer:__train_epoch:232 - Train epoch: [1/30], batch: [400/16579], loss: 16.29228, accuracy: 0.00043, learning rate: 0.00000483, speed: 193.14 data/sec, eta: 1 day, 21:44:42
+[2023-03-16 20:33:25.116274 INFO   ] trainer:__train_epoch:232 - Train epoch: [1/30], batch: [500/16579], loss: 16.27346, accuracy: 0.00041, learning rate: 0.00000603, speed: 198.40 data/sec, eta: 1 day, 20:31:18
+······
+[2023-03-16 20:34:09.633572 INFO   ] trainer:train:304 - ======================================================================
+100%|███████████████████████████████████| 84/84 [00:10<00:00,  7.79it/s]
+开始两两对比音频特征...
+100%|██████████████████████████████| 5332/5332 [00:07<00:00, 749.89it/s]
+[2023-03-16 20:34:29.328638 INFO   ] trainer:train:306 - Test epoch: 1, time/epoch: 0:00:48.881889, threshold: 0.72, tpr: 0.62350, fpr: 0.04601, eer: 0.42250
+[2023-03-16 20:34:29.328840 INFO   ] trainer:train:309 - ======================================================================
+[2023-03-16 20:34:29.728986 INFO   ] trainer:__save_checkpoint:203 - 已保存模型：models/ecapa_tdnn_MelSpectrogram/best_model
+[2023-03-16 20:34:30.724868 INFO   ] trainer:__save_checkpoint:203 - 已保存模型：models/ecapa_tdnn_MelSpectrogram/epoch_1
+[2023-03-16 20:30:42.559858 INFO   ] trainer:__train_epoch:232 - Train epoch: [2/30], batch: [0/16579], loss: 16.48008, accuracy: 0.01562, learning rate: 0.00000000, speed: 21.27 data/sec, eta: 17 days, 7:38:55
+[2023-03-16 20:31:15.045717 INFO   ] trainer:__train_epoch:232 - Train epoch: [2/30], batch: [100/16579], loss: 16.34529, accuracy: 0.00062, learning rate: 0.00000121, speed: 197.03 data/sec, eta: 1 day, 20:52:05
+[2023-03-16 20:31:47.086451 INFO   ] trainer:__train_epoch:232 - Train epoch: [2/30], batch: [200/16579], loss: 16.31631, accuracy: 0.00054, learning rate: 0.00000241, speed: 199.77 data/sec, eta: 1 day, 20:14:40
+[2023-03-16 20:32:19.711337 INFO   ] trainer:__train_epoch:232 - Train epoch: [2/30], batch: [300/16579], loss: 16.30544, accuracy: 0.00047, learning rate: 0.00000362, speed: 196.19 data/sec, eta: 1 day, 21:02:28
+······
+```
+
+VisualDL页面：
+![VisualDL页面](./docs/images/log.jpg)
+
+
+# 数据增强
+本项目提供了几种音频增强操作，分布是随机裁剪，添加背景噪声，调节语速，调节音量，和SpecAugment。其中后面4种增加的参数可以在`configs/augmentation.json`修改，参数`prob`是指定该增强操作的概率，如果不想使用该增强方式，可以设置为0。要主要的是，添加背景噪声需要把多个噪声音频文件存放在`dataset/noise`，否则会跳过噪声增强
+```yaml
+noise:
+  min_snr_dB: 10
+  max_snr_dB: 30
+  noise_path: "dataset/noise"
+  prob: 0.5
+```
+
+
+
+# 评估模型
+训练结束之后会保存预测模型，我们用预测模型来预测测试集中的音频特征，然后使用音频特征进行两两对比，计算tpr、fpr、eer。
+```shell
+python eval.py
+```
+
+输出类似如下：
+```
+······
+------------------------------------------------
+W0425 08:27:32.057426 17654 device_context.cc:447] Please NOTE: device: 0, GPU Compute Capability: 7.5, Driver API Version: 11.6, Runtime API Version: 10.2
+W0425 08:27:32.065165 17654 device_context.cc:465] device: 0, cuDNN Version: 7.6.
+[2023-03-16 20:20:47.195908 INFO   ] trainer:evaluate:341 - 成功加载模型：models/ecapa_tdnn_MelSpectrogram/best_model/model.pdparams
+100%|███████████████████████████| 84/84 [00:28<00:00,  2.95it/s]
+开始两两对比音频特征...
+100%|███████████████████████████| 5332/5332 [00:05<00:00, 1027.83it/s]
+评估消耗时间：65s，threshold：0.26，tpr：0.99391, fpr: 0.00611, eer: 0.01220
+```
+
+# 声纹对比
+下面开始实现声纹对比，创建`infer_contrast.py`程序，编写`infer()`函数，在编写模型的时候，模型是有两个输出的，第一个是模型的分类输出，第二个是音频特征输出。所以在这里要输出的是音频的特征值，有了音频的特征值就可以做声纹识别了。我们输入两个语音，通过预测函数获取他们的特征数据，使用这个特征数据可以求他们的对角余弦值，得到的结果可以作为他们相识度。对于这个相识度的阈值`threshold`，读者可以根据自己项目的准确度要求进行修改。
+```shell
+python infer_contrast.py --audio_path1=audio/a_1.wav --audio_path2=audio/b_2.wav
+```
+
+输出类似如下：
+```
+[2023-04-02 18:30:48.009149 INFO   ] utils:print_arguments:13 - ----------- 额外配置参数 -----------
+[2023-04-02 18:30:48.009149 INFO   ] utils:print_arguments:15 - audio_path1: dataset/a_1.wav
+[2023-04-02 18:30:48.009149 INFO   ] utils:print_arguments:15 - audio_path2: dataset/b_2.wav
+[2023-04-02 18:30:48.009149 INFO   ] utils:print_arguments:15 - configs: configs/ecapa_tdnn.yml
+[2023-04-02 18:30:48.009149 INFO   ] utils:print_arguments:15 - model_path: models/ecapa_tdnn_MelSpectrogram/best_model/
+[2023-04-02 18:30:48.009149 INFO   ] utils:print_arguments:15 - threshold: 0.6
+[2023-04-02 18:30:48.009149 INFO   ] utils:print_arguments:15 - use_gpu: True
+[2023-04-02 18:30:48.009149 INFO   ] utils:print_arguments:16 - ------------------------------------------------
+······································································
+W0425 08:29:10.006249 21121 device_context.cc:447] Please NOTE: device: 0, GPU Compute Capability: 7.5, Driver API Version: 11.6, Runtime API Version: 10.2
+W0425 08:29:10.008555 21121 device_context.cc:465] device: 0, cuDNN Version: 7.6.
+成功加载模型参数和优化方法参数：models/ecapa_tdnn/model.pdparams
+audio/a_1.wav 和 audio/b_2.wav 不是同一个人，相似度为：-0.09565544128417969
+```
+
+# 声纹识别
+在上面的声纹对比的基础上，我们创建`infer_recognition.py`实现声纹识别。同样是使用上面声纹对比的`infer()`预测函数，通过这两个同样获取语音的特征数据。 不同的是笔者增加了`load_audio_db()`和`register()`，以及`recognition()`，第一个函数是加载声纹库中的语音数据，这些音频就是相当于已经注册的用户，他们注册的语音数据会存放在这里，如果有用户需要通过声纹登录，就需要拿到用户的语音和语音库中的语音进行声纹对比，如果对比成功，那就相当于登录成功并且获取用户注册时的信息数据。第二个函数`register()`其实就是把录音保存在声纹库中，同时获取该音频的特征添加到待对比的数据特征中。最后`recognition()`函数中，这个函数就是将输入的语音和语音库中的语音一一对比。
+有了上面的声纹识别的函数，读者可以根据自己项目的需求完成声纹识别的方式，例如笔者下面提供的是通过录音来完成声纹识别。首先必须要加载语音库中的语音，语音库文件夹为`audio_db`，然后用户回车后录音3秒钟，然后程序会自动录音，并使用录音到的音频进行声纹识别，去匹配语音库中的语音，获取用户的信息。通过这样方式，读者也可以修改成通过服务请求的方式完成声纹识别，例如提供一个API供APP调用，用户在APP上通过声纹登录时，把录音到的语音发送到后端完成声纹识别，再把结果返回给APP，前提是用户已经使用语音注册，并成功把语音数据存放在`audio_db`文件夹中。
+```shell
+python infer_recognition.py
+```
+
+输出类似如下：
+```
+[2023-04-02 18:31:20.521040 INFO   ] utils:print_arguments:13 - ----------- 额外配置参数 -----------
+[2023-04-02 18:31:20.521040 INFO   ] utils:print_arguments:15 - audio_db_path: audio_db/
+[2023-04-02 18:31:20.521040 INFO   ] utils:print_arguments:15 - configs: configs/ecapa_tdnn.yml
+[2023-04-02 18:31:20.521040 INFO   ] utils:print_arguments:15 - model_path: models/ecapa_tdnn_MelSpectrogram/best_model/
+[2023-04-02 18:31:20.521040 INFO   ] utils:print_arguments:15 - record_seconds: 3
+[2023-04-02 18:31:20.521040 INFO   ] utils:print_arguments:15 - threshold: 0.6
+[2023-04-02 18:31:20.521040 INFO   ] utils:print_arguments:15 - use_gpu: True
+[2023-04-02 18:31:20.521040 INFO   ] utils:print_arguments:16 - ------------------------------------------------
+······································································
+W0425 08:30:13.257884 23889 device_context.cc:447] Please NOTE: device: 0, GPU Compute Capability: 7.5, Driver API Version: 11.6, Runtime API Version: 10.2
+W0425 08:30:13.260191 23889 device_context.cc:465] device: 0, cuDNN Version: 7.6.
+成功加载模型参数和优化方法参数：models/ecapa_tdnn/model.pdparams
+Loaded 沙瑞金 audio.
+Loaded 李达康 audio.
+请选择功能，0为注册音频到声纹库，1为执行声纹识别：0
+按下回车键开机录音，录音3秒中：
+开始录音......
+录音已结束!
+请输入该音频用户的名称：夜雨飘零
+请选择功能，0为注册音频到声纹库，1为执行声纹识别：1
+按下回车键开机录音，录音3秒中：
+开始录音......
+录音已结束!
+识别说话的为：夜雨飘零，相似度为：0.920434
+```
+
+# 其他版本
+ - Tensorflow：[VoiceprintRecognition-Tensorflow](https://github.com/yeyupiaoling/VoiceprintRecognition-Tensorflow)
+ - Pytorch：[VoiceprintRecognition-Pytorch](https://github.com/yeyupiaoling/VoiceprintRecognition-Pytorch)
+ - Keras：[VoiceprintRecognition-Keras](https://github.com/yeyupiaoling/VoiceprintRecognition-Keras)
+
+## 打赏作者
+
+<br/>
+<div align="center">
+<p>打赏一块钱支持一下作者</p>
+<img src="https://yeyupiaoling.cn/reward.png" alt="打赏作者" width="400">
+</div>
+
+# 参考资料
+1. https://github.com/PaddlePaddle/PaddleSpeech
+2. https://github.com/yeyupiaoling/PaddlePaddle-MobileFaceNets
+3. https://github.com/yeyupiaoling/PPASR
```

### html2text {}

```diff
@@ -1,8 +1,8 @@
-Metadata-Version: 2.1 Name: ppvector Version: 0.3.6 Summary: Voice Print
+Metadata-Version: 2.1 Name: ppvector Version: 0.3.7 Summary: Voice Print
 Recognition toolkit on PaddlePaddle Home-page: https://github.com/yeyupiaoling/
 VoiceprintRecognition_PaddlePaddle Download-URL: https://github.com/
 yeyupiaoling/VoiceprintRecognition_PaddlePaddle.git Author: yeyupiaoling
 License: Apache License 2.0 Keywords: Voice,paddle Classifier: Intended
 Audience :: Developers Classifier: License :: OSI Approved :: Apache Software
 License Classifier: Operating System :: OS Independent Classifier: Natural
 Language :: Chinese (Simplified) Classifier: Programming Language :: Python ::
@@ -21,46 +21,46 @@
 æ­¤çæ¬ä¸ºæ°çæ¬ï¼ç¸æ¯ä¸ä¸ä¸ªçæ¬ï¼æå¤§çååæ¯æ­¤çæ¬æ¯æpipå®è£ï¼ä»¥åæé¢å¤çä½¿ç¨æ¨¡åç®å­å®ç°ï¼è¿æ ·åçå¥½å¤å°±æ¯å¯ä»¥ç´æ¥ä½¿ç¨GPUè®¡ç®ï¼å¤§å¹åº¦æé«äºé¢å¤ççéåº¦ï¼ä¼°è®¡é¢å¤çéåº¦å¯å¨10~20åã
 å¦æ³ä½¿ç¨ä½¿ç¨æ§çæ¬ï¼è¯·è½¬å°[release/1.0](https://github.com/
 yeyupiaoling/VoiceprintRecognition-PaddlePaddle/tree/release/
 1.0)ï¼æ¬é¡¹ç®ä½¿ç¨äºEcapaTdnnæ¨¡åå®ç°çå£°çº¹è¯å«ï¼ä¸æé¤ä»¥åä¼æ¯ææ´å¤æ¨¡åï¼åæ¶æ¬é¡¹ç®ä¹æ¯æäºå¤ç§æ°æ®é¢å¤çæ¹æ³ï¼æå¤±å½æ°åèäºäººè¸è¯å«é¡¹ç®çåæ³
 [PaddlePaddle-MobileFaceNets](https://github.com/yeyupiaoling/PaddlePaddle-
 MobileFaceNets) ,ä½¿ç¨äºArcFace Lossï¼ArcFace lossï¼Additive Angular Margin
 Lossï¼å æ§è§åº¦é´éæå¤±å½æ°ï¼ï¼å¯¹ç¹å¾åéåæéå½ä¸åï¼å¯¹Î¸å ä¸è§åº¦é´émï¼è§åº¦é´éæ¯ä½å¼¦é´éå¨å¯¹è§åº¦çå½±åæ´å ç´æ¥ã
-**æ¬¢è¿å¤§å®¶æ«ç å¥QQç¾¤è®¨è®º**ï¼æèç´æ¥æç´¢QQç¾¤å·`1169600237`ï¼é®é¢ç­æ¡ä¸ºåä¸»GithubçID`yeyupiaoling`ã
-                             [docs/images/qq.png]
+**æ¬¢è¿å¤§å®¶æ«ç å¥QQç¾¤è®¨è®º**ï¼æèç´æ¥æç´¢QQç¾¤å·`758170167`ï¼é®é¢ç­æ¡ä¸ºåä¸»GithubçID`yeyupiaoling`ã
+                             [docs/images/qq.jpg]
 ä½¿ç¨ç¯å¢ï¼ - Anaconda 3 - Python 3.8 - PaddlePaddle 2.4.1 - Windows 10 or
 Ubuntu 18.04 # é¡¹ç®ç¹æ§ 1.
 æ¯ææ¨¡åï¼EcapaTdnnãTDNNãRes2NetãResNetSE 2.
 æ¯ææ± åå±ï¼AttentiveStatisticsPooling(ASP)ãSelfAttentivePooling
 (SAP)ãTemporalStatisticsPooling(TSP)ãTemporalAveragePooling(TAP) 3.
 æ¯ææå¤±å½æ°ï¼AAMLossãAMLossãARMLossãCELoss 4.
 æ¯æé¢å¤çæ¹æ³ï¼MelSpectrogramãLogMelSpectrogramãSpectrogramãMFCC
 # æ¨¡åä¸è½½
- æ¨¡å� é¢å¤ç�         æ°æ®é ç±»å«æ  tpr�  fpr     eer  æ¨¡åä¸è½½å°å
-EcapaTdnn   MelSpectrogram  ä¸­æè¯­é³è¯­    3242�0.98085�0.01123 0.03039    ç¹å»ä¸è½½
-EcapaTdnn LogMelSpectrogram ä¸­æè¯­é³è¯­    3242�0.98693�0.00910 0.02216    ç¹å»ä¸è½½
-EcapaTdnn     Spectrogram   ä¸­æè¯­é³è¯­    3242�0.98573�0.01028 0.02454    ç¹å»ä¸è½½
-EcapaTdnn          MFCC     ä¸­æè¯­é³è¯­    3242�0.98504�0.00867 0.02363    ç¹å»ä¸è½½
-EcapaTdnn   MelSpectrogram      æ´å¤§çæ°æ    6355 0.97910 0.00778 0.02868    ç¹å»ä¸è½½
-EcapaTdnn   MelSpectrogram      è¶å¤§çæ°æ   13718 0.98540 0.00633 0.02093    ç¹å»ä¸è½½
+ æ¨¡å� é¢å¤ç�    æ°æ®é�ç±»å«æ  tpr�  fpr     eer  æ¨¡åä¸è½½å°å
+EcapaTdnn   MelSpectrogram          zhvoice          3242   0.98085 0.01123 0.03039    ç¹å»ä¸è½½
+EcapaTdnn LogMelSpectrogram         zhvoice          3242   0.98693 0.00910 0.02216    ç¹å»ä¸è½½
+EcapaTdnn     Spectrogram           zhvoice          3242   0.98573 0.01028 0.02454    ç¹å»ä¸è½½
+EcapaTdnn          MFCC             zhvoice          3242   0.98504 0.00867 0.02363    ç¹å»ä¸è½½
+EcapaTdnn   MelSpectrogram  æ´å¤§çæ�    6355�é0.97910 0.00778 0.02868    ç¹å»ä¸è½½
+EcapaTdnn   MelSpectrogram  è¶å¤§çæ�   13718�é0.98540 0.00633 0.02093    ç¹å»ä¸è½½
 ## å®è£ç¯å¢ -
 é¦åå®è£çæ¯PaddlePaddleçGPUçæ¬ï¼å¦æå·²ç»å®è£è¿äºï¼è¯·è·³è¿ã
 ```shell conda install paddlepaddle-gpu==2.4.1 cudatoolkit=10.2 --channel
 https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/Paddle/ ``` -
 å®è£ppvectoråºã ä½¿ç¨pipå®è£ï¼å½ä»¤å¦ä¸ï¼ ```shell python -m pip
 install ppvector -U -i https://pypi.tuna.tsinghua.edu.cn/simple ```
 **å»ºè®®æºç å®è£**ï¼æºç å®è£è½ä¿è¯ä½¿ç¨ææ°ä»£ç ã ```shell
 git clone https://github.com/yeyupiaoling/
 VoiceprintRecognition_PaddlePaddle.git cd VoiceprintRecognition_PaddlePaddle/
-python setup.py install ``` # åå»ºæ°æ® æ¬æç¨ç¬èä½¿ç¨çæ¯
-[ä¸­æè¯­é³è¯­ææ°æ®é](https://github.com/fighting41love/zhvoice)
+python setup.py install ``` # åå»ºæ°æ® æ¬æç¨ç¬èä½¿ç¨çæ¯[zhvoice]
+(https://aistudio.baidu.com/aistudio/datasetdetail/133922)
 ï¼è¿ä¸ªæ°æ®éä¸å±æ3242ä¸ªäººçè¯­é³æ°æ®ï¼æ1130000+æ¡è¯­é³æ°æ®ï¼ä¸è½½ä¹åè¦**å¨é¨è§£å**æ°æ®éãå¦æè¯»èæå¶ä»æ´å¥½çæ°æ®éï¼å¯ä»¥æ··åå¨ä¸èµ·ä½¿ç¨ï¼ä½æå¥½æ¯è¦ç¨pythonçå·¥å·æ¨¡åaukitå¤çé³é¢ï¼éåªåå»é¤éé³ã
 é¦åæ¯åå»ºä¸ä¸ªæ°æ®åè¡¨ï¼æ°æ®åè¡¨çæ ¼å¼ä¸º`<è¯­é³æä»¶è·¯å¾\tè¯­é³åç±»æ ç­¾>`ï¼åå»ºè¿ä¸ªåè¡¨ä¸»è¦æ¯æ¹ä¾¿ä¹åçè¯»åï¼ä¹æ¯æ¹ä¾¿è¯»åä½¿ç¨å¶ä»çè¯­é³æ°æ®éï¼è¯­é³åç±»æ ç­¾æ¯æè¯´è¯äººçå¯ä¸IDï¼ä¸åçè¯­é³æ°æ®éï¼å¯ä»¥éè¿ç¼åå¯¹åºççææ°æ®åè¡¨çå½æ°ï¼æè¿äºæ°æ®éé½åå¨åä¸ä¸ªæ°æ®åè¡¨ä¸­ã
-å¨`create_data.py`åä¸ä»¥ä¸ä»£ç ï¼å ä¸º[ä¸­æè¯­é³è¯­ææ°æ®é]
-(https://github.com/fighting41love/zhvoice)
+å¨`create_data.py`åä¸ä»¥ä¸ä»£ç ï¼å ä¸º[zhvoice](https://
+aistudio.baidu.com/aistudio/datasetdetail/133922)
 è¿ä¸ªæ°æ®éæ¯mp3æ ¼å¼çï¼ä½èåç°è¿ç§æ ¼å¼è¯»åéåº¦å¾æ¢ï¼æä»¥ç¬èæå¨é¨çmp3æ ¼å¼çé³é¢è½¬æ¢ä¸ºwavæ ¼å¼ï¼è¿ä¸ªè¿ç¨å¯è½å¾ä¹ãå½ç¶ä¹å¯ä»¥ä¸è½¬æ¢ï¼é¡¹ç®ä¹æ¯æ¯æçMP3æ ¼å¼çï¼åªè¦è®¾ç½®åæ°`to_wav=False`ãæ§è¡ä¸é¢ç¨åºå®ææ°æ®åå¤ã
 ```shell python create_data.py ```
 æ§è¡ä¸é¢çç¨åºä¹åï¼ä¼çæä»¥ä¸çæ°æ®æ ¼å¼ï¼å¦æè¦èªå®ä¹æ°æ®ï¼åèå¦ä¸æ°æ®åè¡¨ï¼åé¢æ¯é³é¢çç¸å¯¹è·¯å¾ï¼åé¢çæ¯è¯¥é³é¢å¯¹åºçè¯´è¯äººçæ ç­¾ï¼å°±è·åç±»ä¸æ ·ã**èªå®ä¹æ°æ®éçæ³¨æ**ï¼æµè¯æ°æ®åè¡¨çIDå¯ä»¥ä¸ç¨è·è®­ç»çIDä¸æ ·ï¼ä¹å°±æ¯è¯´æµè¯çæ°æ®çè¯´è¯äººå¯ä»¥ä¸ç¨åºç°å¨è®­ç»éï¼åªè¦ä¿è¯æµè¯æ°æ®åè¡¨ä¸­åä¸ä¸ªäººç¸åçIDå³å¯ã
 ``` dataset/zhvoice/zhmagicdata/5_895/5_895_20170614203758.wav 3238 dataset/
 zhvoice/zhmagicdata/5_895/5_895_20170614214007.wav 3238 dataset/zhvoice/
 zhmagicdata/5_941/5_941_20170613151344.wav 3239 dataset/zhvoice/zhmagicdata/
 5_941/5_941_20170614221329.wav 3239 dataset/zhvoice/zhmagicdata/5_941/
@@ -288,10 +288,13 @@
 è¯·éæ©åè½ï¼0ä¸ºæ³¨åé³é¢å°å£°çº¹åºï¼1ä¸ºæ§è¡å£°çº¹è¯å«ï¼1
 æä¸åè½¦é®å¼æºå½é³ï¼å½é³3ç§ä¸­ï¼ å¼å§å½é³......
 å½é³å·²ç»æ! è¯å«è¯´è¯çä¸ºï¼å¤é¨é£é¶ï¼ç¸ä¼¼åº¦ä¸ºï¼0.920434
 ``` # å¶ä»çæ¬ - Tensorflowï¼[VoiceprintRecognition-Tensorflow](https://
 github.com/yeyupiaoling/VoiceprintRecognition-Tensorflow) - Pytorchï¼
 [VoiceprintRecognition-Pytorch](https://github.com/yeyupiaoling/
 VoiceprintRecognition-Pytorch) - Kerasï¼[VoiceprintRecognition-Keras](https://
-github.com/yeyupiaoling/VoiceprintRecognition-Keras) # åèèµæ 1. https://
-github.com/PaddlePaddle/PaddleSpeech 2. https://github.com/yeyupiaoling/
-PaddlePaddle-MobileFaceNets 3. https://github.com/yeyupiaoling/PPASR
+github.com/yeyupiaoling/VoiceprintRecognition-Keras) ## æèµä½è
+                       æèµä¸åé±æ¯æä¸ä¸ä½è
+                                [æèµä½è]
+# åèèµæ 1. https://github.com/PaddlePaddle/PaddleSpeech 2. https://
+github.com/yeyupiaoling/PaddlePaddle-MobileFaceNets 3. https://github.com/
+yeyupiaoling/PPASR
```

## Comparing `ppvector-0.3.6.dist-info/RECORD` & `ppvector-0.3.7.dist-info/RECORD`

 * *Files 16% similar despite different names*

```diff
@@ -1,20 +1,20 @@
-ppvector/__init__.py,sha256=p9_zLvghBsJs7k7gHj6qkdpl619KnG5PAOOvDG5DF-o,124
-ppvector/predict.py,sha256=amcRJhXTQJ8acFsvyjvW2XITfW7rxTMBtKw8Lka16Eo,15091
-ppvector/trainer.py,sha256=XsHmsTH9HYRiJt3E8k73RzUaX5FFGDZQnF1IiW5984c,24585
+ppvector/__init__.py,sha256=JjXzNbOmadABiR3Crdyg5FeAaaR1_oACSf1AKru0CUY,124
+ppvector/predict.py,sha256=78c-j3XpRm2Ucu4bzQgUxGT5rLBzDuxumWbPZEBh7Gw,16208
+ppvector/trainer.py,sha256=hZp6Gu4Q16Ejc_Cii1tQ_3JEScHRhcjpPatJLpwfEGc,24577
 ppvector/data_utils/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-ppvector/data_utils/audio.py,sha256=FLZy8YxDA4V1PajySTFLBCC-MG5FpTn7eYzdjhB6J00,22703
+ppvector/data_utils/audio.py,sha256=N895jjSfl__tn2s8wtnax4QAOYP01Me98pOVhLdPygs,22239
 ppvector/data_utils/collate_fn.py,sha256=tb5yu7VH69x1nGKxJUBoZ5UNsVgYxZ_zbth2YN52zPw,909
 ppvector/data_utils/featurizer.py,sha256=Lcb1CtiRvbnmSxokUdXfkHX9d3uzCFWxXdAF3BYGBtk,2918
 ppvector/data_utils/reader.py,sha256=vsmSoYsdcbkI9jkibk107aChqPIfIoofsEUEDDRbgzs,3121
 ppvector/data_utils/utils.py,sha256=J-4RwA_DTZReBRU9l2oSAVXxsdqLitEKqZNYp-vRsyw,4653
 ppvector/data_utils/augmentor/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 ppvector/data_utils/augmentor/augmentation.py,sha256=7VJ9YhCzNBYfcWI-KwsKW7L6KPhX_czkTE2Ugi3W-Ck,4274
 ppvector/data_utils/augmentor/base.py,sha256=Qw5AsIjpqDkggVkVyfPGTI_IwnfSuIWViRdgo7g2ze0,965
-ppvector/data_utils/augmentor/noise_perturb.py,sha256=RcofkXu-SEuSgPwrVQjARPxlY_2mgRcpomPHv_cjCIc,1990
+ppvector/data_utils/augmentor/noise_perturb.py,sha256=2zFB3tXGoNg-gbUaQLPMqfWg_1DtrYXeNCXVcpz-2S8,2575
 ppvector/data_utils/augmentor/resample.py,sha256=j1xbFirDKiR4qg3GhJCkIWr_OI9VVS9XPklWKOtl3cY,983
 ppvector/data_utils/augmentor/shift_perturb.py,sha256=-_4hxIYk8U2t9uG_oTDGl1iHRtrNEevDeR1aavkfVTQ,1013
 ppvector/data_utils/augmentor/speed_perturb.py,sha256=4ZBH2syEzrhX4FsrAwmqH7-ZA3kcWAhjd2UOWUk_XXI,1984
 ppvector/data_utils/augmentor/volume_perturb.py,sha256=O1r_yNKvdwH1n-FKVROMziRmp9E6GNoTe1eWwLmeOkE,1144
 ppvector/metric/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 ppvector/metric/metrics.py,sha256=DMn3yroqPMiSYRkjo6B2KDVsQbO5fwO0W0NNMOLK26Y,2080
 ppvector/models/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
@@ -27,12 +27,12 @@
 ppvector/models/tdnn.py,sha256=Zf2AgQWxq9LxTSP9BnvOjnNE5eIKXNpFDtbWJ5q-2M0,3473
 ppvector/models/utils.py,sha256=WFaGb66sSLalS-2yr9VIOnICrLjR8rGmiggGaWnZV_Y,5049
 ppvector/utils/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 ppvector/utils/logger.py,sha256=-ssorx8FlHA_wrd2Eq6f4HkOqaOG2YseBGvYAo8NXN8,2839
 ppvector/utils/lr.py,sha256=xcuUUhgaGq6JQapIh550kkadeY7R-1-umacAooAl_c4,1422
 ppvector/utils/record.py,sha256=2i4kz5kPDa9KkbAK_Q34sVIXOkD9TroPROIe5QdzqWg,1067
 ppvector/utils/utils.py,sha256=zzBjiCYwNwxYGMfZyo_wubYp5MTmN3YRuFMKXZ65S7c,2790
-ppvector-0.3.6.dist-info/LICENSE,sha256=HrhfyXIkWY2tGFK11kg7vPCqhgh5DcxleloqdhrpyMY,11558
-ppvector-0.3.6.dist-info/METADATA,sha256=CjblzpRUcJiRA2QL7QZztJNSPaKN5bFpSk9vmcIwigY,28050
-ppvector-0.3.6.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
-ppvector-0.3.6.dist-info/top_level.txt,sha256=NywKjkr9phu2LhphLKQRNvdJUG8iJGaZQbe_HC0PhcQ,9
-ppvector-0.3.6.dist-info/RECORD,,
+ppvector-0.3.7.dist-info/LICENSE,sha256=HrhfyXIkWY2tGFK11kg7vPCqhgh5DcxleloqdhrpyMY,11558
+ppvector-0.3.7.dist-info/METADATA,sha256=E4rQdpHBG-Fktg0nnLDVEFeigp6k5cMtqUAvlARxaYQ,28649
+ppvector-0.3.7.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
+ppvector-0.3.7.dist-info/top_level.txt,sha256=NywKjkr9phu2LhphLKQRNvdJUG8iJGaZQbe_HC0PhcQ,9
+ppvector-0.3.7.dist-info/RECORD,,
```

