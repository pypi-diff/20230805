# Comparing `tmp/rayfed_nightly-0.1.0b20230722.dev0-py3-none-any.whl.zip` & `tmp/rayfed_nightly-0.1.0b20230805.dev0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,30 +1,28 @@
-Zip file size: 41548 bytes, number of entries: 28
--rw-r--r--  2.0 unx      859 b- defN 23-Jul-22 01:33 fed/__init__.py
--rw-r--r--  2.0 unx    13013 b- defN 23-Jul-22 01:33 fed/api.py
--rw-r--r--  2.0 unx     3676 b- defN 23-Jul-22 01:33 fed/cleanup.py
--rw-r--r--  2.0 unx     6368 b- defN 23-Jul-22 01:33 fed/config.py
--rw-r--r--  2.0 unx     2877 b- defN 23-Jul-22 01:33 fed/fed_object.py
--rw-r--r--  2.0 unx     7908 b- defN 23-Jul-22 01:33 fed/tree_util.py
--rw-r--r--  2.0 unx     7505 b- defN 23-Jul-22 01:33 fed/utils.py
--rw-r--r--  2.0 unx      581 b- defN 23-Jul-22 01:33 fed/_private/__init__.py
--rw-r--r--  2.0 unx      581 b- defN 23-Jul-22 01:33 fed/_private/api.py
--rw-r--r--  2.0 unx     5269 b- defN 23-Jul-22 01:33 fed/_private/compatible_utils.py
--rw-r--r--  2.0 unx     1074 b- defN 23-Jul-22 01:33 fed/_private/constants.py
--rw-r--r--  2.0 unx     3851 b- defN 23-Jul-22 01:33 fed/_private/fed_actor.py
--rw-r--r--  2.0 unx     3947 b- defN 23-Jul-22 01:33 fed/_private/fed_call_holder.py
--rw-r--r--  2.0 unx     1262 b- defN 23-Jul-22 01:33 fed/_private/global_context.py
--rw-r--r--  2.0 unx     2419 b- defN 23-Jul-22 01:33 fed/_private/serialization_utils.py
--rw-r--r--  2.0 unx      581 b- defN 23-Jul-22 01:33 fed/grpc/__init__.py
--rw-r--r--  2.0 unx     2968 b- defN 23-Jul-22 01:33 fed/grpc/fed_pb2_grpc_in_protobuf3.py
--rw-r--r--  2.0 unx     2989 b- defN 23-Jul-22 01:33 fed/grpc/fed_pb2_grpc_in_protobuf4.py
--rw-r--r--  2.0 unx     2095 b- defN 23-Jul-22 01:33 fed/grpc/fed_pb2_in_protobuf3.py
--rw-r--r--  2.0 unx     2113 b- defN 23-Jul-22 01:33 fed/grpc/fed_pb2_in_protobuf4.py
--rw-r--r--  2.0 unx      613 b- defN 23-Jul-22 01:33 fed/proxy/__init__.py
--rw-r--r--  2.0 unx     9796 b- defN 23-Jul-22 01:33 fed/proxy/barriers.py
--rw-r--r--  2.0 unx     2025 b- defN 23-Jul-22 01:33 fed/proxy/base_proxy.py
--rw-r--r--  2.0 unx    11356 b- defN 23-Jul-22 01:35 rayfed_nightly-0.1.0b20230722.dev0.dist-info/LICENSE
--rw-r--r--  2.0 unx     7327 b- defN 23-Jul-22 01:35 rayfed_nightly-0.1.0b20230722.dev0.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-Jul-22 01:35 rayfed_nightly-0.1.0b20230722.dev0.dist-info/WHEEL
--rw-r--r--  2.0 unx        4 b- defN 23-Jul-22 01:35 rayfed_nightly-0.1.0b20230722.dev0.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     2339 b- defN 23-Jul-22 01:35 rayfed_nightly-0.1.0b20230722.dev0.dist-info/RECORD
-28 files, 105488 bytes uncompressed, 37788 bytes compressed:  64.2%
+Zip file size: 41482 bytes, number of entries: 26
+-rw-r--r--  2.0 unx      859 b- defN 23-Aug-05 01:19 fed/__init__.py
+-rw-r--r--  2.0 unx    13964 b- defN 23-Aug-05 01:19 fed/api.py
+-rw-r--r--  2.0 unx     3668 b- defN 23-Aug-05 01:19 fed/cleanup.py
+-rw-r--r--  2.0 unx     6254 b- defN 23-Aug-05 01:19 fed/config.py
+-rw-r--r--  2.0 unx     2877 b- defN 23-Aug-05 01:19 fed/fed_object.py
+-rw-r--r--  2.0 unx     7912 b- defN 23-Aug-05 01:19 fed/tree_util.py
+-rw-r--r--  2.0 unx     7614 b- defN 23-Aug-05 01:19 fed/utils.py
+-rw-r--r--  2.0 unx      581 b- defN 23-Aug-05 01:19 fed/_private/__init__.py
+-rw-r--r--  2.0 unx     5269 b- defN 23-Aug-05 01:19 fed/_private/compatible_utils.py
+-rw-r--r--  2.0 unx     1078 b- defN 23-Aug-05 01:19 fed/_private/constants.py
+-rw-r--r--  2.0 unx     3851 b- defN 23-Aug-05 01:19 fed/_private/fed_actor.py
+-rw-r--r--  2.0 unx     3947 b- defN 23-Aug-05 01:19 fed/_private/fed_call_holder.py
+-rw-r--r--  2.0 unx     1262 b- defN 23-Aug-05 01:19 fed/_private/global_context.py
+-rw-r--r--  2.0 unx     2559 b- defN 23-Aug-05 01:19 fed/_private/serialization_utils.py
+-rw-r--r--  2.0 unx      581 b- defN 23-Aug-05 01:19 fed/grpc/__init__.py
+-rw-r--r--  2.0 unx      613 b- defN 23-Aug-05 01:19 fed/proxy/__init__.py
+-rw-r--r--  2.0 unx    14523 b- defN 23-Aug-05 01:19 fed/proxy/barriers.py
+-rw-r--r--  2.0 unx     2661 b- defN 23-Aug-05 01:19 fed/proxy/base_proxy.py
+-rw-r--r--  2.0 unx      581 b- defN 23-Aug-05 01:19 fed/proxy/grpc/__init__.py
+-rw-r--r--  2.0 unx     2569 b- defN 23-Aug-05 01:19 fed/proxy/grpc/grpc_options.py
+-rw-r--r--  2.0 unx    12361 b- defN 23-Aug-05 01:19 fed/proxy/grpc/grpc_proxy.py
+-rw-r--r--  2.0 unx    11356 b- defN 23-Aug-05 01:21 rayfed_nightly-0.1.0b20230805.dev0.dist-info/LICENSE
+-rw-r--r--  2.0 unx     7327 b- defN 23-Aug-05 01:21 rayfed_nightly-0.1.0b20230805.dev0.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-Aug-05 01:21 rayfed_nightly-0.1.0b20230805.dev0.dist-info/WHEEL
+-rw-r--r--  2.0 unx        4 b- defN 23-Aug-05 01:21 rayfed_nightly-0.1.0b20230805.dev0.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     2154 b- defN 23-Aug-05 01:21 rayfed_nightly-0.1.0b20230805.dev0.dist-info/RECORD
+26 files, 116517 bytes uncompressed, 38020 bytes compressed:  67.4%
```

## zipnote {}

```diff
@@ -18,17 +18,14 @@
 
 Filename: fed/utils.py
 Comment: 
 
 Filename: fed/_private/__init__.py
 Comment: 
 
-Filename: fed/_private/api.py
-Comment: 
-
 Filename: fed/_private/compatible_utils.py
 Comment: 
 
 Filename: fed/_private/constants.py
 Comment: 
 
 Filename: fed/_private/fed_actor.py
@@ -42,44 +39,41 @@
 
 Filename: fed/_private/serialization_utils.py
 Comment: 
 
 Filename: fed/grpc/__init__.py
 Comment: 
 
-Filename: fed/grpc/fed_pb2_grpc_in_protobuf3.py
-Comment: 
-
-Filename: fed/grpc/fed_pb2_grpc_in_protobuf4.py
+Filename: fed/proxy/__init__.py
 Comment: 
 
-Filename: fed/grpc/fed_pb2_in_protobuf3.py
+Filename: fed/proxy/barriers.py
 Comment: 
 
-Filename: fed/grpc/fed_pb2_in_protobuf4.py
+Filename: fed/proxy/base_proxy.py
 Comment: 
 
-Filename: fed/proxy/__init__.py
+Filename: fed/proxy/grpc/__init__.py
 Comment: 
 
-Filename: fed/proxy/barriers.py
+Filename: fed/proxy/grpc/grpc_options.py
 Comment: 
 
-Filename: fed/proxy/base_proxy.py
+Filename: fed/proxy/grpc/grpc_proxy.py
 Comment: 
 
-Filename: rayfed_nightly-0.1.0b20230722.dev0.dist-info/LICENSE
+Filename: rayfed_nightly-0.1.0b20230805.dev0.dist-info/LICENSE
 Comment: 
 
-Filename: rayfed_nightly-0.1.0b20230722.dev0.dist-info/METADATA
+Filename: rayfed_nightly-0.1.0b20230805.dev0.dist-info/METADATA
 Comment: 
 
-Filename: rayfed_nightly-0.1.0b20230722.dev0.dist-info/WHEEL
+Filename: rayfed_nightly-0.1.0b20230805.dev0.dist-info/WHEEL
 Comment: 
 
-Filename: rayfed_nightly-0.1.0b20230722.dev0.dist-info/top_level.txt
+Filename: rayfed_nightly-0.1.0b20230805.dev0.dist-info/top_level.txt
 Comment: 
 
-Filename: rayfed_nightly-0.1.0b20230722.dev0.dist-info/RECORD
+Filename: rayfed_nightly-0.1.0b20230805.dev0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## fed/api.py

```diff
@@ -29,31 +29,35 @@
 from fed._private.global_context import get_global_context, clear_global_context
 from fed.proxy.barriers import (
     ping_others,
     recv,
     send,
     _start_receiver_proxy,
     _start_sender_proxy,
+    _start_sender_receiver_proxy,
+    set_receiver_proxy_actor_name,
+    set_sender_proxy_actor_name,
 )
-from fed.proxy.grpc.grpc_proxy import SenderProxy, ReceiverProxy
-from fed.config import GrpcCrossSiloMessageConfig
+from fed.proxy.base_proxy import SenderProxy, ReceiverProxy, SenderReceiverProxy
+from fed.config import CrossSiloMessageConfig
 from fed.fed_object import FedObject
 from fed.utils import is_ray_object_refs, setup_logger
 
 logger = logging.getLogger(__name__)
 
 
 def init(
     addresses: Dict = None,
     party: str = None,
     config: Dict = {},
     tls_config: Dict = None,
     logging_level: str = 'info',
     sender_proxy_cls: SenderProxy = None,
     receiver_proxy_cls: ReceiverProxy = None,
+    receiver_sender_proxy_cls: SenderReceiverProxy = None,
 ):
     """
     Initialize a RayFed client.
 
     Args:
         addresses: optional; a dict describes the addresses configurations. E.g.
 
@@ -108,78 +112,98 @@
 
     tls_config = {} if tls_config is None else tls_config
     if tls_config:
         assert (
             'cert' in tls_config and 'key' in tls_config
         ), 'Cert or key are not in tls_config.'
 
-    cross_silo_message_dict = config.get("cross_silo_message", {})
-    cross_silo_message_config = GrpcCrossSiloMessageConfig.from_dict(
-        cross_silo_message_dict)
+    cross_silo_comm_dict = config.get("cross_silo_comm", {})
     # A Ray private accessing, should be replaced in public API.
     compatible_utils._init_internal_kv()
 
     cluster_config = {
         constants.KEY_OF_CLUSTER_ADDRESSES: addresses,
         constants.KEY_OF_CURRENT_PARTY_NAME: party,
         constants.KEY_OF_TLS_CONFIG: tls_config,
     }
 
     job_config = {
-        constants.KEY_OF_CROSS_SILO_MESSAGE_CONFIG:
-            cross_silo_message_config,
+        constants.KEY_OF_CROSS_SILO_COMM_CONFIG_DICT: cross_silo_comm_dict,
     }
-    compatible_utils.kv.put(constants.KEY_OF_CLUSTER_CONFIG,
-                            cloudpickle.dumps(cluster_config))
+    compatible_utils.kv.put(
+        constants.KEY_OF_CLUSTER_CONFIG, cloudpickle.dumps(cluster_config)
+    )
     compatible_utils.kv.put(constants.KEY_OF_JOB_CONFIG, cloudpickle.dumps(job_config))
     # Set logger.
     # Note(NKcqx): This should be called after internal_kv has party value, i.e.
     # after `ray.init` and
     # `internal_kv._internal_kv_put(RAYFED_PARTY_KEY, cloudpickle.dumps(party))`
     setup_logger(
         logging_level=logging_level,
         logging_format=constants.RAYFED_LOG_FMT,
         date_format=constants.RAYFED_DATE_FMT,
         party_val=_get_party(),
     )
 
     logger.info(f'Started rayfed with {cluster_config}')
+    cross_silo_comm_config = CrossSiloMessageConfig.from_dict(cross_silo_comm_dict)
     get_global_context().get_cleanup_manager().start(
-        exit_when_failure_sending=cross_silo_message_config.exit_on_sending_failure) # noqa
-
-    if receiver_proxy_cls is None:
-        logger.debug(
-            "There is no receiver proxy class specified, it uses `GrpcRecvProxy` by "
-            "default.")
-        from fed.proxy.grpc.grpc_proxy import GrpcReceiverProxy
-        receiver_proxy_cls = GrpcReceiverProxy
-    _start_receiver_proxy(
-        addresses=addresses,
-        party=party,
-        logging_level=logging_level,
-        tls_config=tls_config,
-        proxy_cls=receiver_proxy_cls,
-        proxy_config=cross_silo_message_config
-    )
-
-    if sender_proxy_cls is None:
-        logger.debug(
-            "There is no sender proxy class specified, it uses `GrpcRecvProxy` by "
-            "default.")
-        from fed.proxy.grpc.grpc_proxy import GrpcSenderProxy
-        sender_proxy_cls = GrpcSenderProxy
-    _start_sender_proxy(
-        addresses=addresses,
-        party=party,
-        logging_level=logging_level,
-        tls_config=tls_config,
-        proxy_cls=sender_proxy_cls,
-        # TODO(qwang): proxy_config -> cross_silo_message_config
-        proxy_config=cross_silo_message_config
+        exit_on_sending_failure=cross_silo_comm_config.exit_on_sending_failure
     )
+    if receiver_sender_proxy_cls is not None:
+        proxy_actor_name = 'sender_recevier_actor'
+        set_sender_proxy_actor_name(proxy_actor_name)
+        set_receiver_proxy_actor_name(proxy_actor_name)
+        _start_sender_receiver_proxy(
+            addresses=addresses,
+            party=party,
+            logging_level=logging_level,
+            tls_config=tls_config,
+            proxy_cls=receiver_sender_proxy_cls,
+            proxy_config=cross_silo_comm_dict,
+            ready_timeout_second=cross_silo_comm_config.timeout_in_ms / 1000,
+        )
+    else:
+        if receiver_proxy_cls is None:
+            logger.debug(
+                (
+                    "There is no receiver proxy class specified, "
+                    "it uses `GrpcRecvProxy` by default."
+                )
+            )
+            from fed.proxy.grpc.grpc_proxy import GrpcReceiverProxy
+
+            receiver_proxy_cls = GrpcReceiverProxy
+        _start_receiver_proxy(
+            addresses=addresses,
+            party=party,
+            logging_level=logging_level,
+            tls_config=tls_config,
+            proxy_cls=receiver_proxy_cls,
+            proxy_config=cross_silo_comm_dict,
+            ready_timeout_second=cross_silo_comm_config.timeout_in_ms / 1000,
+        )
+
+        if sender_proxy_cls is None:
+            logger.debug(
+                "There is no sender proxy class specified, it uses `GrpcRecvProxy` by "
+                "default."
+            )
+            from fed.proxy.grpc.grpc_proxy import GrpcSenderProxy
+
+            sender_proxy_cls = GrpcSenderProxy
+        _start_sender_proxy(
+            addresses=addresses,
+            party=party,
+            logging_level=logging_level,
+            tls_config=tls_config,
+            proxy_cls=sender_proxy_cls,
+            proxy_config=cross_silo_comm_dict,
+            ready_timeout_second=cross_silo_comm_config.timeout_in_ms / 1000,
+        )
 
     if config.get("barrier_on_initializing", False):
         # TODO(zhouaihui): can be removed after we have a better retry strategy.
         ping_others(addresses=addresses, self_party=party, max_retries=3600)
 
 
 def shutdown():
```

## fed/cleanup.py

```diff
@@ -43,16 +43,16 @@
     def __init__(self) -> None:
         # `deque()` is thread safe on `popleft` and `append` operations.
         # See https://docs.python.org/3/library/collections.html#deque-objects
         self._sending_obj_refs_q = deque()
         self._check_send_thread = None
         self._monitor_thread = None
 
-    def start(self, exit_when_failure_sending=False):
-        self._exit_when_failure_sending = exit_when_failure_sending
+    def start(self, exit_on_sending_failure=False):
+        self._exit_on_sending_failure = exit_on_sending_failure
 
         def __check_func():
             self._check_sending_objs()
 
         self._check_send_thread = threading.Thread(target=__check_func)
         self._check_send_thread.start()
         logger.info('Start check sending thread.')
@@ -94,13 +94,13 @@
             if isinstance(obj_ref, bool):
                 break
             try:
                 res = ray.get(obj_ref)
             except Exception as e:
                 logger.warn(f'Failed to send {obj_ref} with error: {e}')
                 res = False
-            if not res and self._exit_when_failure_sending:
+            if not res and self._exit_on_sending_failure:
                 logger.warn('Signal self to exit.')
                 _signal_exit()
                 break
 
         logger.info('Check sending thread was exited.')
```

## fed/config.py

```diff
@@ -1,24 +1,23 @@
-
-
 """This module should be cached locally due to all configurations
    are mutable.
 """
 
 import fed._private.compatible_utils as compatible_utils
 import fed._private.constants as fed_constants
 import cloudpickle
 import json
 
 from typing import Dict, List, Optional
-from dataclasses import dataclass
+from dataclasses import dataclass, fields
 
 
 class ClusterConfig:
     """A local cache of cluster configuration items."""
+
     def __init__(self, raw_bytes: bytes) -> None:
         self._data = cloudpickle.loads(raw_bytes)
 
     @property
     def cluster_addresses(self):
         return self._data[fed_constants.KEY_OF_CLUSTER_ADDRESSES]
 
@@ -35,18 +34,16 @@
     def __init__(self, raw_bytes: bytes) -> None:
         if raw_bytes is None:
             self._data = {}
         else:
             self._data = cloudpickle.loads(raw_bytes)
 
     @property
-    def cross_silo_message_config(self):
-        return self._data.get(
-            fed_constants.KEY_OF_CROSS_SILO_MESSAGE_CONFIG,
-            CrossSiloMessageConfig())
+    def cross_silo_comm_config_dict(self) -> Dict:
+        return self._data.get(fed_constants.KEY_OF_CROSS_SILO_COMM_CONFIG_DICT, {})
 
 
 # A module level cache for the cluster configurations.
 _cluster_config = None
 
 _job_config = None
 
@@ -99,50 +96,48 @@
             cross-silo messages.
             If None, the default value of 500 MB is specified.
         timeout_in_ms: The timeout in mili-seconds of a cross-silo RPC call.
             It's 60000 by default.
         http_header: The HTTP header, e.g. metadata in grpc, sent with the RPC request.
             This won't override basic tcp headers, such as `user-agent`, but concat
             them together.
+        max_concurrency: the max_concurrency of the sender/receiver proxy actor.
     """
+
     proxy_max_restarts: int = None
     timeout_in_ms: int = 60000
     messages_max_size_in_bytes: int = None
     exit_on_sending_failure: Optional[bool] = False
     serializing_allowed_list: Optional[Dict[str, str]] = None
     send_resource_label: Optional[Dict[str, str]] = None
     recv_resource_label: Optional[Dict[str, str]] = None
     http_header: Optional[Dict[str, str]] = None
-    # (Optional) The address this party is going to listen on.
-    # If not provided, the `address` will be used.
-    listening_address: str = None
+    max_concurrency: Optional[int] = None
 
     def __json__(self):
         return json.dumps(self.__dict__)
 
     @classmethod
     def from_json(cls, json_str):
         data = json.loads(json_str)
         return cls(**data)
 
     @classmethod
-    def from_dict(cls, data: Dict):
+    def from_dict(cls, data: Dict) -> 'CrossSiloMessageConfig':
         """Initialize CrossSiloMessageConfig from a dictionary.
 
         Args:
             data (Dict): Dictionary with keys as member variable names.
 
         Returns:
             CrossSiloMessageConfig: An instance of CrossSiloMessageConfig.
         """
         # Get the attributes of the class
-
         data = data or {}
-        all_annotations = {**cls.__annotations__, **cls.__base__.__annotations__}
-        attrs = {attr for attr, _ in all_annotations.items()}
+        attrs = [field.name for field in fields(cls)]
         # Filter the dictionary to only include keys that are attributes of the class
         filtered_data = {key: value for key, value in data.items() if key in attrs}
         return cls(**filtered_data)
 
 
 @dataclass
 class GrpcCrossSiloMessageConfig(CrossSiloMessageConfig):
@@ -166,9 +161,10 @@
                 }
         grpc_channel_options: A list of tuples to store GRPC channel options,
             e.g. [
                     ('grpc.enable_retries', 1),
                     ('grpc.max_send_message_length', 50 * 1024 * 1024)
                 ]
     """
+
     grpc_channel_options: List = None
     grpc_retry_policy: Dict[str, str] = None
```

## fed/tree_util.py

```diff
@@ -121,15 +121,15 @@
     typ = type(pytree)
     bases = typ.__bases__
     if len(bases) != 1 or bases[0] != tuple:
         return False
     fields = getattr(typ, '_fields', None)
     if not isinstance(fields, tuple):
         return False
-    return all(type(entry) == str for entry in fields)
+    return all(isinstance(entry, str) for entry in fields)
 
 
 def _get_node_type(pytree: Any) -> Any:
     if _is_namedtuple_instance(pytree):
         return namedtuple
     return type(pytree)
```

## fed/utils.py

```diff
@@ -12,17 +12,17 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import logging
 import re
 import sys
 
-import fed
 import ray
 
+import fed
 from fed._private.compatible_utils import _compare_version_strings
 from fed.fed_object import FedObject
 
 logger = logging.getLogger(__name__)
 
 
 def get_package_version(package_name: str) -> str:
@@ -32,17 +32,19 @@
     You don't need to worry about the Python version.
     When using version 3.7 and below, it uses the built-in `pkg_resources`.
     When using Python 3.8 and above, it uses `importlib.metadata`.
     """
     curr_python_version = sys.version.split(" ")[0]
     if _compare_version_strings(curr_python_version, '3.7.99'):
         import importlib.metadata
+
         return importlib.metadata.version(package_name)
     else:
         import pkg_resources
+
         return pkg_resources.get_distribution(package_name).version
 
 
 def resolve_dependencies(current_party, current_fed_task_id, *args, **kwargs):
     from fed.proxy.barriers import recv
 
     flattened_args, tree = fed.tree_util.tree_flatten((args, kwargs))
@@ -164,21 +166,22 @@
 
 
 def dict2tuple(dic):
     """
     Convert a dictionary to a two-dimensional tuple, for example:
     {'key': 'value'} => (('key', 'value'), ).
     """
-    if (dic is None or isinstance(dic, tuple)):
+    if dic is None or isinstance(dic, tuple):
         return dic
-    elif (isinstance(dic, dict)):
+    elif isinstance(dic, dict):
         return tuple((k, v) for k, v in dic.items())
     else:
-        logger.warn(f"Unable to convert type {type(dic)} to tuple"
-                    f"skip converting {dic}.")
+        logger.warn(
+            f"Unable to convert type {type(dic)} to tuple" f"skip converting {dic}."
+        )
         return dic
 
 
 def validate_address(address: str) -> None:
     if address is None:
         raise ValueError("The address shouldn't be None.")
 
@@ -197,21 +200,26 @@
         return
 
     # Rule 3: https or http link
     link_pattern = r'^(https?://).*'
     if re.match(link_pattern, address):
         return
 
-    error_msg = ("The address format is invalid. It should be in one of the following formats:\n" # noqa
-                "- `local` for starting a new cluster, or `localhost` for connecting a local cluster.\n" # noqa
-                "- 'ip:port' format, where the IP needs to follow the IP address specifications and the port is a number.\n" # noqa
-                "- 'hostname:port' format, where the hostname is a string and the port is a number.\n" # noqa
-                "- An HTTPS or HTTP link starting with 'https://' or 'http://'.") # noqa
+    error_msg = (
+        "The address format is invalid. It should be in one of the following formats:\n"  # noqa
+        "- `local` for starting a new cluster, or `localhost` for connecting a local cluster.\n"  # noqa
+        "- 'ip:port' format, where the IP needs to follow the IP address specifications and the port is a number.\n"  # noqa
+        "- 'hostname:port' format, where the hostname is a string and the port is a number.\n"  # noqa
+        "- An HTTPS or HTTP link starting with 'https://' or 'http://'."
+    )  # noqa
     raise ValueError(error_msg)
 
 
 def validate_addresses(addresses: dict):
     """
     Validate whether the addresses is in correct forms.
     """
-    for _, address in addresses.items():
+    for address in addresses.values():
+        assert (
+            isinstance(address, str) and address
+        ), f'Address should be string but got {address}.'
         validate_address(address)
```

## fed/_private/constants.py

```diff
@@ -21,14 +21,14 @@
 
 KEY_OF_CLUSTER_ADDRESSES = "CLUSTER_ADDRESSES"
 
 KEY_OF_CURRENT_PARTY_NAME = "CURRENT_PARTY_NAME"
 
 KEY_OF_TLS_CONFIG = "TLS_CONFIG"
 
-KEY_OF_CROSS_SILO_MESSAGE_CONFIG = "CROSS_SILO_MESSAGE_CONFIG"
+KEY_OF_CROSS_SILO_COMM_CONFIG_DICT = "CROSS_SILO_COMM_CONFIG_DICT"
 
 RAYFED_LOG_FMT = "%(asctime)s %(levelname)s %(filename)s:%(lineno)s [%(party)s] --  %(message)s" # noqa
 
 RAYFED_DATE_FMT = "%Y-%m-%d %H:%M:%S"
 
 RAY_VERSION_2_0_0_STR = "2.0.0"
```

## fed/_private/serialization_utils.py

```diff
@@ -26,27 +26,30 @@
     *,
     fix_imports=True,
     encoding="ASCII",
     errors="strict",
     buffers=None,
 ):
     from sys import version_info
+
     assert version_info.major == 3
 
     if version_info.minor >= 8:
         import pickle as pickle
     else:
         import pickle5 as pickle
 
     class RestrictedUnpickler(pickle.Unpickler):
         def find_class(self, module, name):
             if _pickle_whitelist is None or (
                 module in _pickle_whitelist
-                and (_pickle_whitelist[module] is None or name in _pickle_whitelist[
-                    module])
+                and (
+                    _pickle_whitelist[module] is None
+                    or name in _pickle_whitelist[module]
+                )
             ):
                 return super().find_class(module, name)
 
             if module == "fed._private":  # TODO(qwang): Not sure if it works.
                 return super().find_class(module, name)
 
             # Forbid everything else.
@@ -59,16 +62,18 @@
         file, fix_imports=fix_imports, buffers=buffers, encoding=encoding, errors=errors
     ).load()
 
 
 def _apply_loads_function_with_whitelist():
     global _pickle_whitelist
 
-    _pickle_whitelist = fed_config.get_job_config() \
-        .cross_silo_message_config.serializing_allowed_list
+    cross_silo_comm_config = fed_config.CrossSiloMessageConfig.from_dict(
+        fed_config.get_job_config().cross_silo_comm_config_dict
+    )
+    _pickle_whitelist = cross_silo_comm_config.serializing_allowed_list
     if _pickle_whitelist is None:
         return
 
     if "*" in _pickle_whitelist:
         _pickle_whitelist = None
         return
```

## fed/proxy/barriers.py

```diff
@@ -8,30 +8,52 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+import copy
 import logging
 import time
-import copy
-from typing import Dict, Optional
+from typing import Dict
 
 import ray
 
 import fed.config as fed_config
-from fed.config import get_job_config
-from fed.proxy.base_proxy import SenderProxy, ReceiverProxy
-from fed.utils import setup_logger
 from fed._private import constants
 from fed._private.global_context import get_global_context
+from fed.proxy.base_proxy import ReceiverProxy, SenderProxy, SenderReceiverProxy
+from fed.utils import setup_logger
 
 logger = logging.getLogger(__name__)
 
+_SENDER_PROXY_ACTOR_NAME = 'SenderProxyActor'
+_RECEIVER_PROXY_ACTOR_NAME = 'ReceiverProxyActor'
+
+
+def sender_proxy_actor_name() -> str:
+    global _SENDER_PROXY_ACTOR_NAME
+    return _SENDER_PROXY_ACTOR_NAME
+
+
+def set_sender_proxy_actor_name(name: str):
+    global _SENDER_PROXY_ACTOR_NAME
+    _SENDER_PROXY_ACTOR_NAME = name
+
+
+def receiver_proxy_actor_name() -> str:
+    global _RECEIVER_PROXY_ACTOR_NAME
+    return _RECEIVER_PROXY_ACTOR_NAME
+
+
+def set_receiver_proxy_actor_name(name: str):
+    global _RECEIVER_PROXY_ACTOR_NAME
+    _RECEIVER_PROXY_ACTOR_NAME = name
+
 
 def key_exists_in_two_dim_dict(the_dict, key_a, key_b) -> bool:
     key_a, key_b = str(key_a), str(key_b)
     if key_a not in the_dict:
         return False
     return key_b in the_dict[key_a]
 
@@ -58,31 +80,32 @@
 class SenderProxyActor:
     def __init__(
         self,
         addresses: Dict,
         party: str,
         tls_config: Dict = None,
         logging_level: str = None,
-        proxy_cls=None
+        proxy_cls=None,
     ):
         setup_logger(
             logging_level=logging_level,
             logging_format=constants.RAYFED_LOG_FMT,
             date_format=constants.RAYFED_DATE_FMT,
             party_val=party,
         )
 
         self._stats = {"send_op_count": 0}
         self._addresses = addresses
         self._party = party
         self._tls_config = tls_config
         job_config = fed_config.get_job_config()
-        cross_silo_message_config = job_config.cross_silo_message_config
+        cross_silo_comm_config = job_config.cross_silo_comm_config_dict
         self._proxy_instance: SenderProxy = proxy_cls(
-            addresses, party, tls_config, cross_silo_message_config)
+            addresses, party, tls_config, cross_silo_comm_config
+        )
 
     async def is_ready(self):
         res = await self._proxy_instance.is_ready()
         return res
 
     async def send(
         self,
@@ -101,15 +124,16 @@
         )
         logger.debug(
             f'Sending {send_log_msg} with{"out" if not self._tls_config else ""}'
             ' credentials.'
         )
         try:
             response = await self._proxy_instance.send(
-                dest_party, data, upstream_seq_id, downstream_seq_id)
+                dest_party, data, upstream_seq_id, downstream_seq_id
+            )
         except Exception as e:
             logger.error(f'Failed to {send_log_msg}, error: {e}')
             return False
         logger.debug(f"Succeeded to send {send_log_msg}. Response is {response}")
         return True  # True indicates it's sent successfully.
 
     async def _get_stats(self):
@@ -139,29 +163,31 @@
             party_val=party,
         )
         self._stats = {"receive_op_count": 0}
         self._listening_address = listening_address
         self._party = party
         self._tls_config = tls_config
         job_config = fed_config.get_job_config()
-        cross_silo_message_config = job_config.cross_silo_message_config
+        cross_silo_comm_config = job_config.cross_silo_comm_config_dict
         self._proxy_instance: ReceiverProxy = proxy_cls(
-            listening_address, party, tls_config, cross_silo_message_config)
+            listening_address, party, tls_config, cross_silo_comm_config
+        )
 
     async def start(self):
         await self._proxy_instance.start()
 
     async def is_ready(self):
         res = await self._proxy_instance.is_ready()
         return res
 
     async def get_data(self, src_party, upstream_seq_id, curr_seq_id):
         self._stats["receive_op_count"] += 1
         data = await self._proxy_instance.get_data(
-            src_party, upstream_seq_id, curr_seq_id)
+            src_party, upstream_seq_id, curr_seq_id
+        )
         return data
 
     async def _get_stats(self):
         return self._stats
 
     async def _get_proxy_config(self):
         return await self._proxy_instance.get_proxy_config()
@@ -174,43 +200,41 @@
 
 def _start_receiver_proxy(
     addresses: str,
     party: str,
     logging_level: str,
     tls_config=None,
     proxy_cls=None,
-    proxy_config: Optional[fed_config.CrossSiloMessageConfig] = None
+    proxy_config: Dict = None,
+    ready_timeout_second: int = 60,
 ):
-
-    # Create RecevrProxyActor
-    # Not that this is now a threaded actor.
-    # NOTE(NKcqx): This is not just addr, but a party dict containing 'address'
-    party_addr = addresses[party]
-    listening_address = proxy_config.listening_address
-    if not listening_address:
-        listening_address = party_addr
-
     actor_options = copy.deepcopy(_DEFAULT_RECEIVER_PROXY_OPTIONS)
-    if proxy_config is not None and proxy_config.recv_resource_label is not None:
-        actor_options.update({"resources": proxy_config.recv_resource_label})
+    if proxy_config:
+        proxy_config = fed_config.CrossSiloMessageConfig.from_dict(proxy_config)
+        if proxy_config.recv_resource_label is not None:
+            actor_options.update({"resources": proxy_config.recv_resource_label})
+        if proxy_config.max_concurrency:
+            actor_options.update({"max_concurrency": proxy_config.max_concurrency})
 
     logger.debug(f"Starting ReceiverProxyActor with options: {actor_options}")
 
+    global _RECEIVER_PROXY_ACTOR_NAME
     receiver_proxy_actor = ReceiverProxyActor.options(
-        name=f"ReceiverProxyActor-{party}", **actor_options
+        name=_RECEIVER_PROXY_ACTOR_NAME, **actor_options
     ).remote(
-        listening_address=listening_address,
+        listening_address=addresses[party],
         party=party,
         tls_config=tls_config,
         logging_level=logging_level,
-        proxy_cls=proxy_cls
+        proxy_cls=proxy_cls,
     )
     receiver_proxy_actor.start.remote()
-    timeout = proxy_config.timeout_in_ms / 1000 if proxy_config is not None else 60
-    server_state = ray.get(receiver_proxy_actor.is_ready.remote(), timeout=timeout)
+    server_state = ray.get(
+        receiver_proxy_actor.is_ready.remote(), timeout=ready_timeout_second
+    )
     assert server_state[0], server_state[1]
     logger.info("Succeeded to create receiver proxy actor.")
 
 
 _SENDER_PROXY_ACTOR = None
 _DEFAULT_SENDER_PROXY_OPTIONS = {
     "max_concurrency": 1000,
@@ -219,64 +243,197 @@
 
 def _start_sender_proxy(
     addresses: Dict,
     party: str,
     logging_level: str,
     tls_config: Dict = None,
     proxy_cls=None,
-    proxy_config: Optional[fed_config.CrossSiloMessageConfig] = None
+    proxy_config: Dict = None,
+    ready_timeout_second: int = 60,
 ):
-    # Create SenderProxyActor
-    global _SENDER_PROXY_ACTOR
-
+    if proxy_config:
+        proxy_config = fed_config.GrpcCrossSiloMessageConfig.from_dict(proxy_config)
     actor_options = copy.deepcopy(_DEFAULT_SENDER_PROXY_OPTIONS)
-    if proxy_config and proxy_config.proxy_max_restarts:
-        actor_options.update({
-            "max_task_retries": proxy_config.proxy_max_restarts,
-            "max_restarts": 1,
-            })
-    if proxy_config and proxy_config.send_resource_label:
-        actor_options.update({"resources": proxy_config.send_resource_label})
+    if proxy_config:
+        if proxy_config.proxy_max_restarts:
+            actor_options.update(
+                {
+                    "max_task_retries": proxy_config.proxy_max_restarts,
+                    "max_restarts": 1,
+                }
+            )
+        if proxy_config.send_resource_label:
+            actor_options.update({"resources": proxy_config.send_resource_label})
+        if proxy_config.max_concurrency:
+            actor_options.update({"max_concurrency": proxy_config.max_concurrency})
 
     logger.debug(f"Starting SenderProxyActor with options: {actor_options}")
+    global _SENDER_PROXY_ACTOR
+    global _SENDER_PROXY_ACTOR_NAME
     _SENDER_PROXY_ACTOR = SenderProxyActor.options(
-        name="SenderProxyActor", **actor_options)
+        name=_SENDER_PROXY_ACTOR_NAME, **actor_options
+    )
 
     _SENDER_PROXY_ACTOR = _SENDER_PROXY_ACTOR.remote(
         addresses=addresses,
         party=party,
         tls_config=tls_config,
         logging_level=logging_level,
-        proxy_cls=proxy_cls
+        proxy_cls=proxy_cls,
     )
-    timeout = get_job_config().cross_silo_message_config.timeout_in_ms / 1000
-    assert ray.get(_SENDER_PROXY_ACTOR.is_ready.remote(), timeout=timeout)
+    assert ray.get(_SENDER_PROXY_ACTOR.is_ready.remote(), timeout=ready_timeout_second)
     logger.info("SenderProxyActor has successfully created.")
 
 
+_SENDER_RECEIVER_PROXY_ACTOR = None
+_DEFAULT_SENDER_RECEIVER_PROXY_OPTIONS = {
+    "max_concurrency": 1,
+}
+
+
+@ray.remote
+class SenderReceiverProxyActor:
+    def __init__(
+        self,
+        addresses: Dict,
+        party: str,
+        tls_config: Dict = None,
+        logging_level: str = None,
+        proxy_cls: SenderReceiverProxy = None,
+    ):
+        setup_logger(
+            logging_level=logging_level,
+            logging_format=constants.RAYFED_LOG_FMT,
+            date_format=constants.RAYFED_DATE_FMT,
+            party_val=party,
+        )
+
+        self._stats = {'send_op_count': 0, 'receive_op_count': 0}
+        self._addresses = addresses
+        self._party = party
+        self._tls_config = tls_config
+        job_config = fed_config.get_job_config()
+        cross_silo_comm_config = job_config.cross_silo_comm_config_dict
+        self._proxy_instance = proxy_cls(
+            addresses, party, tls_config, cross_silo_comm_config
+        )
+
+    def is_ready(self):
+        return self._proxy_instance.is_ready()
+
+    def start(self):
+        self._proxy_instance.start()
+
+    def get_data(self, src_party, upstream_seq_id, curr_seq_id):
+        self._stats["receive_op_count"] += 1
+        data = self._proxy_instance.get_data(src_party, upstream_seq_id, curr_seq_id)
+        return data
+
+    def send(
+        self,
+        dest_party,
+        data,
+        upstream_seq_id,
+        downstream_seq_id,
+    ):
+        self._stats["send_op_count"] += 1
+        assert (
+            dest_party in self._addresses
+        ), f'Failed to find {dest_party} in cluster {self._addresses}.'
+        send_log_msg = (
+            f'send data to seq_id {downstream_seq_id} of {dest_party} '
+            f'from {upstream_seq_id}'
+        )
+        logger.debug(
+            f'Sending {send_log_msg} with{"out" if not self._tls_config else ""}'
+            ' credentials.'
+        )
+        try:
+            response = self._proxy_instance.send(
+                dest_party, data, upstream_seq_id, downstream_seq_id
+            )
+        except Exception as e:
+            logger.error(f'Failed to {send_log_msg}, error: {e}')
+            return False
+        logger.debug(f"Succeeded to send {send_log_msg}. Response is {response}")
+        return True  # True indicates it's sent successfully.
+
+    def _get_stats(self):
+        return self._stats
+
+    def _get_proxy_config(self, dest_party=None):
+        return self._proxy_instance.get_proxy_config(dest_party)
+
+
+def _start_sender_receiver_proxy(
+    addresses: str,
+    party: str,
+    logging_level: str,
+    tls_config=None,
+    proxy_cls=None,
+    proxy_config: Dict = None,
+    ready_timeout_second: int = 60,
+):
+    global _DEFAULT_SENDER_RECEIVER_PROXY_OPTIONS
+    actor_options = copy.deepcopy(_DEFAULT_SENDER_RECEIVER_PROXY_OPTIONS)
+    if proxy_config:
+        proxy_config = fed_config.CrossSiloMessageConfig.from_dict(proxy_config)
+        if proxy_config.proxy_max_restarts:
+            actor_options.update(
+                {
+                    "max_task_retries": proxy_config.proxy_max_restarts,
+                    "max_restarts": 1,
+                }
+            )
+        if proxy_config.max_concurrency:
+            actor_options.update({"max_concurrency": proxy_config.max_concurrency})
+
+    logger.debug(f"Starting ReceiverProxyActor with options: {actor_options}")
+
+    global _SENDER_RECEIVER_PROXY_ACTOR
+    global _RECEIVER_PROXY_ACTOR_NAME
+    _SENDER_RECEIVER_PROXY_ACTOR = SenderReceiverProxyActor.options(
+        name=_RECEIVER_PROXY_ACTOR_NAME, **actor_options
+    ).remote(
+        addresses=addresses,
+        party=party,
+        tls_config=tls_config,
+        logging_level=logging_level,
+        proxy_cls=proxy_cls,
+    )
+    _SENDER_RECEIVER_PROXY_ACTOR.start.remote()
+    server_state = ray.get(
+        _SENDER_RECEIVER_PROXY_ACTOR.is_ready.remote(), timeout=ready_timeout_second
+    )
+    assert server_state[0], server_state[1]
+    logger.info("Succeeded to create receiver proxy actor.")
+
+
 def send(
     dest_party,
     data,
     upstream_seq_id,
     downstream_seq_id,
 ):
-    sender_proxy = ray.get_actor("SenderProxyActor")
+    global _SENDER_PROXY_ACTOR_NAME
+    sender_proxy = ray.get_actor(_SENDER_PROXY_ACTOR_NAME)
     res = sender_proxy.send.remote(
         dest_party=dest_party,
         data=data,
         upstream_seq_id=upstream_seq_id,
         downstream_seq_id=downstream_seq_id,
     )
     get_global_context().get_cleanup_manager().push_to_sending(res)
     return res
 
 
 def recv(party: str, src_party: str, upstream_seq_id, curr_seq_id):
     assert party, 'Party can not be None.'
-    receiver_proxy = ray.get_actor(f"ReceiverProxyActor-{party}")
+    global _RECEIVER_PROXY_ACTOR_NAME
+    receiver_proxy = ray.get_actor(_RECEIVER_PROXY_ACTOR_NAME)
     return receiver_proxy.get_data.remote(src_party, upstream_seq_id, curr_seq_id)
 
 
 def ping_others(addresses: Dict[str, Dict], self_party: str, max_retries=3600):
     """Ping other parties until all are ready or timeout."""
     others = [party for party in addresses if not party == self_party]
     tried = 0
@@ -289,16 +446,15 @@
         _party_ping_obj = {}  # {$party_name: $ObjectRef}
         # Batch ping all the other parties
         for other in others:
             _party_ping_obj[other] = send(other, b'data', 'ping', 'ping')
         _, _unready = ray.wait(list(_party_ping_obj.values()), timeout=1)
 
         # Keep the unready party for the next ping.
-        others = [
-            other for other in others if _party_ping_obj[other] in _unready
-        ]
+        others = [other for other in others if _party_ping_obj[other] in _unready]
         if others:
             time.sleep(2)
     if others:
-        raise RuntimeError(f"Failed to wait for parties: {others} to start, "
-                           "abort `fed.init`.")
+        raise RuntimeError(
+            f"Failed to wait for parties: {others} to start, " "abort `fed.init`."
+        )
     return True
```

## fed/proxy/base_proxy.py

```diff
@@ -20,61 +20,83 @@
 
 class SenderProxy(abc.ABC):
     def __init__(
         self,
         addresses: Dict,
         party: str,
         tls_config: Dict,
-        proxy_config: CrossSiloMessageConfig = None
+        proxy_config: CrossSiloMessageConfig = None,
     ) -> None:
         self._addresses = addresses
         self._party = party
         self._tls_config = tls_config
         self._proxy_config = proxy_config
 
     @abc.abstractmethod
-    async def send(
-        self,
-        dest_party,
-        data,
-        upstream_seq_id,
-        downstream_seq_id
-    ):
+    async def send(self, dest_party, data, upstream_seq_id, downstream_seq_id):
         pass
 
     async def is_ready(self):
         return True
 
     async def get_proxy_config(self, dest_party=None):
         return self._proxy_config
 
 
 class ReceiverProxy(abc.ABC):
     def __init__(
-            self,
-            listen_addr: str,
-            party: str,
-            tls_config: Dict,
-            proxy_config: CrossSiloMessageConfig = None
+        self,
+        listen_addr: str,
+        party: str,
+        tls_config: Dict,
+        proxy_config: CrossSiloMessageConfig = None,
     ) -> None:
         self._listen_addr = listen_addr
         self._party = party
         self._tls_config = tls_config
         self._proxy_config = proxy_config
 
     @abc.abstractmethod
     def start(self):
         pass
 
     @abc.abstractmethod
-    async def get_data(
-            self,
-            src_party,
-            upstream_seq_id,
-            curr_seq_id):
+    async def get_data(self, src_party, upstream_seq_id, curr_seq_id):
         pass
 
     async def is_ready(self):
         return True
 
     async def get_proxy_config(self):
         return self._proxy_config
+
+
+class SenderReceiverProxy(abc.ABC):
+    def __init__(
+        self,
+        addresses: Dict,
+        self_party: str,
+        tls_config: Dict,
+        proxy_config: CrossSiloMessageConfig = None,
+    ) -> None:
+        self._addresses = addresses
+        self._party = self_party
+        self._tls_config = tls_config
+        self._proxy_config = proxy_config
+
+    @abc.abstractmethod
+    def start(self):
+        pass
+
+    def is_ready(self):
+        return True
+
+    @abc.abstractmethod
+    def get_data(self, src_party, upstream_seq_id, curr_seq_id):
+        pass
+
+    @abc.abstractmethod
+    def send(self, dest_party, data, upstream_seq_id, downstream_seq_id):
+        pass
+
+    def get_proxy_config(self):
+        return self._proxy_config
```

## Comparing `fed/_private/api.py` & `fed/proxy/grpc/__init__.py`

 * *Files identical despite different names*

## Comparing `rayfed_nightly-0.1.0b20230722.dev0.dist-info/LICENSE` & `rayfed_nightly-0.1.0b20230805.dev0.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `rayfed_nightly-0.1.0b20230722.dev0.dist-info/METADATA` & `rayfed_nightly-0.1.0b20230805.dev0.dist-info/METADATA`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: rayfed-nightly
-Version: 0.1.0b20230722.dev0
+Version: 0.1.0b20230805.dev0
 Summary: A multiple parties joint, distributed execution engine based on Ray,to help build your own federated learning frameworks in minutes.
 Home-page: https://github.com/ray-project/rayfed
 Author: RayFed Team
 Author-email: rayfed-dev@googlegroups.com
 License: Apache 2.0
 Platform: UNKNOWN
 Description-Content-Type: text/markdown
```

## Comparing `rayfed_nightly-0.1.0b20230722.dev0.dist-info/RECORD` & `rayfed_nightly-0.1.0b20230805.dev0.dist-info/RECORD`

 * *Files 14% similar despite different names*

```diff
@@ -1,28 +1,26 @@
 fed/__init__.py,sha256=WN5lu4fAPUVxzlL4T6Z_ygmP3d9-DAVLWFRkd92SVmc,859
-fed/api.py,sha256=2jwG6egBJVRFiWr4iq-IZ9Qk-DJttz2Aj9amtrbqV3E,13013
-fed/cleanup.py,sha256=LeADQUgJ8n94PJdBqkbM9LbMnA-aZfMPe7bk0MMWZIU,3676
-fed/config.py,sha256=HUaU_qmToYRLLSVXfba0nnSCMb55bL9wKiSL7YBa84k,6368
+fed/api.py,sha256=yk9NXWixcFmRDjd0o4X-MQkRojQSfego7UwIofxR5Vw,13964
+fed/cleanup.py,sha256=he50D41g9pxbay3IvCMIZpqEOYyScbHYL6uxnPp5Hxo,3668
+fed/config.py,sha256=4KQHMKJVKZWE-pGX6IBhQ1ocEpq50DtOeWRCy0sLgDA,6254
 fed/fed_object.py,sha256=l2BRa9WZ3jqXSDjwFPURcAIK8XkH8A5UYA_OcY4TOD0,2877
-fed/tree_util.py,sha256=25LiGASBcreE34OsGsoeRWH4t6BuBk3QVcKjCe38jTI,7908
-fed/utils.py,sha256=tEzcc6LmH0gpg1aZD6zJZVSBeVYQ1rVxmG4h4niTxgA,7505
+fed/tree_util.py,sha256=J29bCmCF_ChHUdXYBXDtItV6dmiHmhCskCXrmtSn1Ag,7912
+fed/utils.py,sha256=X_brj4FeyF4ePfZ-u4u7dJJftvqFFrgZ0vOH48w_yeg,7614
 fed/_private/__init__.py,sha256=MDXwlZ-y7_o-tfqh8DVkjlO-6MlWqwLdbSIZUJfXtF0,581
-fed/_private/api.py,sha256=MDXwlZ-y7_o-tfqh8DVkjlO-6MlWqwLdbSIZUJfXtF0,581
 fed/_private/compatible_utils.py,sha256=4JdcL1qGlLwQx6DMBAs5lphPxOIJyz1YqYbS1Hq8Ruk,5269
-fed/_private/constants.py,sha256=91H8u6UL_qvvUII-UlKbipuapOH_xVAGVcRv8qBWMnw,1074
+fed/_private/constants.py,sha256=32zdgXupZ13qbexm0TXwyL3WGVQeamRH3SC7CCd-uFw,1078
 fed/_private/fed_actor.py,sha256=juzgqrBsycl1kKaND-HwBeG0YOBcvk5rGcOQutA8RJ0,3851
 fed/_private/fed_call_holder.py,sha256=RDD3fet8CD2clUNfSX6ByXViaYlkcLQNDxaAADU-Ld4,3947
 fed/_private/global_context.py,sha256=JtRCuBHju_8OcIbtfNvhzPR9bDxeD_X8bRe-8SogNtY,1262
-fed/_private/serialization_utils.py,sha256=TbvlXDj8_r_uzkUkESs4Z8qOqKzUQ-Li7h0Xile-WkM,2419
+fed/_private/serialization_utils.py,sha256=_yJdd97df1BCUU2PNe9R1L04UlbgjBGw4N8GmTTB0rs,2559
 fed/grpc/__init__.py,sha256=MDXwlZ-y7_o-tfqh8DVkjlO-6MlWqwLdbSIZUJfXtF0,581
-fed/grpc/fed_pb2_grpc_in_protobuf3.py,sha256=Do9HCVH4CdclNRsX8LASBCrDUtcji8QP8voHl485Qw8,2968
-fed/grpc/fed_pb2_grpc_in_protobuf4.py,sha256=MyuFC-u-QMNNAyHsXRc3TvEq9U4DQ9EjBIGdcPzSQvk,2989
-fed/grpc/fed_pb2_in_protobuf3.py,sha256=wJRyg1d7dK6SHfw7lzQPlEHWkINq8JqiK0qbqXWhRro,2095
-fed/grpc/fed_pb2_in_protobuf4.py,sha256=0JbbKEqCKx31MiIMVU3UOJ93ZgTbAr6g51z1EgIRNG8,2113
 fed/proxy/__init__.py,sha256=zjmMrqPxcVeXa7sMvRccSdnmV8_A73tETlvUJtAd0lA,613
-fed/proxy/barriers.py,sha256=mt-XVee-ezoxbTBbnkZPYp4s4w0Y13yg342s8tTb-gQ,9796
-fed/proxy/base_proxy.py,sha256=hCNmp66-gTvE-Xsfy1TThZfEiuPhSRFM5NMYLQNh1uc,2025
-rayfed_nightly-0.1.0b20230722.dev0.dist-info/LICENSE,sha256=QwcOLU5TJoTeUhuIXzhdCEEDDvorGiC6-3YTOl4TecE,11356
-rayfed_nightly-0.1.0b20230722.dev0.dist-info/METADATA,sha256=c1wvMgCNiqNI2GBXXRVxpnnTF73FgGpMpgd8Wy71WGk,7327
-rayfed_nightly-0.1.0b20230722.dev0.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
-rayfed_nightly-0.1.0b20230722.dev0.dist-info/top_level.txt,sha256=5gd1qhbpzLAi36ONV8p_s65_-iAiOSwFljYh-ONEwts,4
-rayfed_nightly-0.1.0b20230722.dev0.dist-info/RECORD,,
+fed/proxy/barriers.py,sha256=HoGtl1B8h7nEjeQp_8u0-8TaW1wjKAlFUHRUkooL6Iw,14523
+fed/proxy/base_proxy.py,sha256=gtukHs1GZEXhfa9oir_uqPigxL31FBPIUkV2raW4g_8,2661
+fed/proxy/grpc/__init__.py,sha256=MDXwlZ-y7_o-tfqh8DVkjlO-6MlWqwLdbSIZUJfXtF0,581
+fed/proxy/grpc/grpc_options.py,sha256=8zdKKqcpvWQG3nTNzvgk0RLLbumocg7exX-ZfbQMv2c,2569
+fed/proxy/grpc/grpc_proxy.py,sha256=OHj8F7igHnPvAfPq8qPRGfu04TMLzCnTthFEAmd8tmM,12361
+rayfed_nightly-0.1.0b20230805.dev0.dist-info/LICENSE,sha256=QwcOLU5TJoTeUhuIXzhdCEEDDvorGiC6-3YTOl4TecE,11356
+rayfed_nightly-0.1.0b20230805.dev0.dist-info/METADATA,sha256=yAPUqNWsoiLLdocLN6FD7lVf34J8ekjNcHh15on733o,7327
+rayfed_nightly-0.1.0b20230805.dev0.dist-info/WHEEL,sha256=AtBG6SXL3KF_v0NxLf0ehyVOh0cold-JbJYXNGorC6Q,92
+rayfed_nightly-0.1.0b20230805.dev0.dist-info/top_level.txt,sha256=5gd1qhbpzLAi36ONV8p_s65_-iAiOSwFljYh-ONEwts,4
+rayfed_nightly-0.1.0b20230805.dev0.dist-info/RECORD,,
```

